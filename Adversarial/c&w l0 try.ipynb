{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from absl import app, flags\n",
    "from easydict import EasyDict\n",
    "from tensorflow.keras import Model\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dense, Flatten, Conv2D, MaxPool2D\n",
    "\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from cleverhans.future.tf2.attacks import projected_gradient_descent, fast_gradient_method\n",
    "\n",
    "import foolbox as fb\n",
    "import eagerpy as ep\n",
    "from foolbox import TensorFlowModel, accuracy, samples\n",
    "import foolbox.attacks as fa\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, activation='relu'):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #print(input_shape)\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True)\n",
    "        self.mask = self.add_weight(shape=(self.w.shape),\n",
    "                                    initializer='ones',\n",
    "                                    trainable=False)\n",
    "        self.pruned_w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='ones',\n",
    "                                 trainable=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #self.mask_2 = tf.multiply(self.mask, self.mask_2)\n",
    "        self.pruned_w = tf.multiply(self.w, self.mask)\n",
    "        #print('layer inputy', inputs.shape)\n",
    "        x = tf.matmul(inputs, self.pruned_w)\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return tf.keras.activations.relu(x)\n",
    "        if self.activation == 'softmax':\n",
    "            return tf.keras.activations.softmax(x)\n",
    "        raise ValueError('Activation function not implemented')\n",
    "\n",
    "\n",
    "class LeNet300_100(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet300_100, self).__init__()\n",
    "        self.dense1 = CustomLayer(300)\n",
    "        self.dense2 = CustomLayer(100)\n",
    "        self.dense3 = CustomLayer(10, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #print('NN call',inputs.shape)\n",
    "        x = tf.keras.layers.Flatten()(inputs)\n",
    "        x = self.dense1(x)\n",
    "        \n",
    "        x = self.dense2(x)\n",
    "        #print(x.shape)\n",
    "        return self.dense3(x)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ld_mnist():\n",
    "    \"\"\"Load training and test data.\"\"\"\n",
    "\n",
    "    def convert_types(image, label):\n",
    "        image = tf.cast(image, tf.float32)\n",
    "        image /= 255\n",
    "        return image, label\n",
    "\n",
    "    dataset, info = tfds.load('mnist', \n",
    "                              data_dir='gs://tfds-data/datasets', \n",
    "                              with_info=True,\n",
    "                              as_supervised=True)\n",
    "    mnist_train, mnist_test = dataset['train'], dataset['test']\n",
    "    mnist_train = mnist_train.map(convert_types).shuffle(10000).batch(128)\n",
    "    mnist_test = mnist_test.map(convert_types).batch(128)\n",
    "    return EasyDict(train=mnist_train, test=mnist_test)\n",
    "data = ld_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def initialize_model():\n",
    "    model = LeNet300_100()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "    return model\n",
    "\n",
    "def train_model(model):\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=data.train,\n",
    "              #batch_size=64,\n",
    "              epochs=5000,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5000\n",
      "    469/Unknown - 12s 26ms/step - loss: 1.6007 - accuracy: 0.7955Epoch 1/5000\n",
      "469/469 [==============================] - 13s 27ms/step - loss: 1.6006 - accuracy: 0.7959 - val_loss: 1.5303 - val_accuracy: 0.9339\n",
      "Epoch 2/5000\n",
      "460/469 [============================>.] - ETA: 0s - loss: 1.5224 - accuracy: 0.9389Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.5222 - accuracy: 0.9390 - val_loss: 1.5137 - val_accuracy: 0.9497\n",
      "Epoch 3/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.5075 - accuracy: 0.9548Epoch 1/5000\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 1.5074 - accuracy: 0.9548 - val_loss: 1.5024 - val_accuracy: 0.9605\n",
      "Epoch 4/5000\n",
      "468/469 [============================>.] - ETA: 0s - loss: 1.4977 - accuracy: 0.9644Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.4977 - accuracy: 0.9644 - val_loss: 1.4974 - val_accuracy: 0.9653\n",
      "Epoch 5/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4922 - accuracy: 0.9700Epoch 1/5000\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 1.4921 - accuracy: 0.9701 - val_loss: 1.4946 - val_accuracy: 0.9678\n",
      "Epoch 6/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4871 - accuracy: 0.9758Epoch 1/5000\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 1.4872 - accuracy: 0.9758 - val_loss: 1.4956 - val_accuracy: 0.9667\n",
      "Epoch 7/5000\n",
      "461/469 [============================>.] - ETA: 0s - loss: 1.4839 - accuracy: 0.9781Epoch 1/5000\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 1.4839 - accuracy: 0.9781 - val_loss: 1.4912 - val_accuracy: 0.9704\n",
      "Epoch 8/5000\n",
      "467/469 [============================>.] - ETA: 0s - loss: 1.4810 - accuracy: 0.9818Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.4810 - accuracy: 0.9818 - val_loss: 1.4894 - val_accuracy: 0.9722\n",
      "Epoch 9/5000\n",
      "466/469 [============================>.] - ETA: 0s - loss: 1.4792 - accuracy: 0.9827Epoch 1/5000\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 1.4792 - accuracy: 0.9827 - val_loss: 1.4877 - val_accuracy: 0.9737\n",
      "Epoch 10/5000\n",
      "467/469 [============================>.] - ETA: 0s - loss: 1.4774 - accuracy: 0.9840Epoch 1/5000\n",
      "469/469 [==============================] - 11s 24ms/step - loss: 1.4774 - accuracy: 0.9840 - val_loss: 1.4868 - val_accuracy: 0.9751\n",
      "Epoch 11/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4767 - accuracy: 0.9847Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.4767 - accuracy: 0.9847 - val_loss: 1.4854 - val_accuracy: 0.9758\n",
      "Epoch 12/5000\n",
      "462/469 [============================>.] - ETA: 0s - loss: 1.4760 - accuracy: 0.9859Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.4760 - accuracy: 0.9859 - val_loss: 1.4869 - val_accuracy: 0.9741\n",
      "Epoch 13/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4743 - accuracy: 0.9878Epoch 1/5000\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 1.4744 - accuracy: 0.9878 - val_loss: 1.4847 - val_accuracy: 0.9771\n",
      "Epoch 14/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4734 - accuracy: 0.9879Epoch 1/5000\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 1.4735 - accuracy: 0.9879 - val_loss: 1.4879 - val_accuracy: 0.9740\n",
      "Epoch 15/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4725 - accuracy: 0.9899Epoch 1/5000\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 1.4725 - accuracy: 0.9899 - val_loss: 1.4824 - val_accuracy: 0.9785\n",
      "Epoch 16/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4722 - accuracy: 0.9898Epoch 1/5000\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 1.4722 - accuracy: 0.9898 - val_loss: 1.4837 - val_accuracy: 0.9774\n",
      "Epoch 17/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4716 - accuracy: 0.9898Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.4716 - accuracy: 0.9898 - val_loss: 1.4867 - val_accuracy: 0.9750\n",
      "Epoch 18/5000\n",
      "462/469 [============================>.] - ETA: 0s - loss: 1.4711 - accuracy: 0.9907Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.4711 - accuracy: 0.9907 - val_loss: 1.4818 - val_accuracy: 0.9797\n",
      "Epoch 19/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4711 - accuracy: 0.9904Epoch 1/5000\n",
      "469/469 [==============================] - 10s 22ms/step - loss: 1.4711 - accuracy: 0.9904 - val_loss: 1.4804 - val_accuracy: 0.9812\n",
      "Epoch 20/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4707 - accuracy: 0.9906Epoch 1/5000\n",
      "469/469 [==============================] - 11s 22ms/step - loss: 1.4708 - accuracy: 0.9906 - val_loss: 1.4825 - val_accuracy: 0.9788\n",
      "Epoch 21/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4701 - accuracy: 0.9908Epoch 1/5000\n",
      "469/469 [==============================] - 11s 23ms/step - loss: 1.4701 - accuracy: 0.9908 - val_loss: 1.4819 - val_accuracy: 0.9796\n",
      "Epoch 22/5000\n",
      "463/469 [============================>.] - ETA: 0s - loss: 1.4697 - accuracy: 0.9928Epoch 1/5000\n",
      "469/469 [==============================] - 10s 21ms/step - loss: 1.4697 - accuracy: 0.9927 - val_loss: 1.4811 - val_accuracy: 0.9802\n"
     ]
    }
   ],
   "source": [
    "model = initialize_model()\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'tensorflow.python.framework.ops.EagerTensor' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-50-694914fd8da0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mattack\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mCarliniL0\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_iterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minitial_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlargest_const\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-49-cf36550e20b5>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, model, targeted, learning_rate, max_iterations, abort_early, initial_const, largest_const, reduce_const, const_factor, independent_channels)\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0;31m#self.grad = self.gradient_descent(sess, model)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgradient_descent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-49-cf36550e20b5>\u001b[0m in \u001b[0;36mgradient_descent\u001b[0;34m(self, model)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0;31m#optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m             \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLEARNING_RATE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m             \u001b[0mtrain\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodifier\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[0;31m#end_vars = tf.global_variables()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name)\u001b[0m\n\u001b[1;32m    314\u001b[0m     \"\"\"\n\u001b[1;32m    315\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 316\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss)\n\u001b[0m\u001b[1;32m    317\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    318\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/site-packages/tensorflow_core/python/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    348\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m         \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 350\u001b[0;31m       \u001b[0mloss_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    351\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    352\u001b[0m       \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'tensorflow.python.framework.ops.EagerTensor' object is not callable"
     ]
    }
   ],
   "source": [
    "model.num_channels = 1\n",
    "model.image_size = 28\n",
    "model.num_labels = 10\n",
    "\n",
    "attack= CarliniL0(model, max_iterations=1000, initial_const=10, largest_const=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "MAX_ITERATIONS = 1000   # number of iterations to perform gradient descent\n",
    "ABORT_EARLY = True      # abort gradient descent upon first valid solution\n",
    "LEARNING_RATE = 1e-2    # larger values converge faster to less accurate results\n",
    "INITIAL_CONST = 1e-3    # the first value of c to start at\n",
    "LARGEST_CONST = 2e6     # the largest value of c to go up to before giving up\n",
    "REDUCE_CONST = False    # try to lower c each iteration; faster to set to false\n",
    "TARGETED = True         # should we target one specific class? or just be wrong?\n",
    "CONST_FACTOR = 2.0      # f>1, rate at which we increase constant, smaller better\n",
    "\n",
    "class CarliniL0:\n",
    "    def __init__(self, model,\n",
    "                 targeted = TARGETED, learning_rate = LEARNING_RATE,\n",
    "                 max_iterations = MAX_ITERATIONS, abort_early = ABORT_EARLY,\n",
    "                 initial_const = INITIAL_CONST, largest_const = LARGEST_CONST,\n",
    "                 reduce_const = REDUCE_CONST, const_factor = CONST_FACTOR,\n",
    "                 independent_channels = False):\n",
    "        \"\"\"\n",
    "        The L_0 optimized attack. \n",
    "        Returns adversarial examples for the supplied model.\n",
    "        targeted: True if we should perform a targetted attack, False otherwise.\n",
    "        learning_rate: The learning rate for the attack algorithm. Smaller values\n",
    "          produce better results but are slower to converge.\n",
    "        max_iterations: The maximum number of iterations. Larger values are more\n",
    "          accurate; setting too small will require a large learning rate and will\n",
    "          produce poor results.\n",
    "        abort_early: If true, allows early aborts if gradient descent gets stuck.\n",
    "        initial_const: The initial tradeoff-constant to use to tune the relative\n",
    "          importance of distance and confidence. Should be set to a very small\n",
    "          value (but positive).\n",
    "        largest_const: The largest constant to use until we report failure. Should\n",
    "          be set to a very large value.\n",
    "        const_factor: The rate at which we should increase the constant, when the\n",
    "          previous constant failed. Should be greater than one, smaller is better.\n",
    "        independent_channels: set to false optimizes for number of pixels changed,\n",
    "          set to true (not recommended) returns number of channels changed.\n",
    "        \"\"\"\n",
    "\n",
    "        self.model = model\n",
    "        #self.sess = sess\n",
    "\n",
    "        self.TARGETED = targeted\n",
    "        self.LEARNING_RATE = learning_rate\n",
    "        self.MAX_ITERATIONS = max_iterations\n",
    "        self.ABORT_EARLY = abort_early\n",
    "        self.INITIAL_CONST = initial_const\n",
    "        self.LARGEST_CONST = largest_const\n",
    "        self.REDUCE_CONST = reduce_const\n",
    "        self.const_factor = const_factor\n",
    "        self.independent_channels = independent_channels\n",
    "\n",
    "        self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = False\n",
    "\n",
    "        #self.grad = self.gradient_descent(sess, model)\n",
    "        self.grad = self.gradient_descent(model)\n",
    "\n",
    "    def gradient_descent(self, model):\n",
    "        def compare(x,y):\n",
    "            if self.TARGETED:\n",
    "                return x == y\n",
    "            else:\n",
    "                return x != y\n",
    "        shape = (1,model.image_size,model.image_size,model.num_channels)\n",
    "        \n",
    "        # the variable to optimize over\n",
    "        modifier = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "\n",
    "        # the variables we're going to hold, use for efficiency\n",
    "        canchange = tf.Variable(np.zeros(shape),dtype=np.float32)\n",
    "        simg = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "        original = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "        timg = tf.Variable(np.zeros(shape,dtype=np.float32))\n",
    "        tlab = tf.Variable(np.zeros((1,model.num_labels),dtype=np.float32))\n",
    "        const = tf.Variable([], dtype=np.float32)\n",
    "\n",
    "        '''# and the assignment to set the variables\n",
    "        assign_modifier = tf.compat.v1.placeholder(np.float32,shape)\n",
    "        assign_canchange = tf.compat.v1.placeholder(np.float32,shape)\n",
    "        assign_simg = tf.compat.v1.placeholder(np.float32,shape)\n",
    "        assign_original = tf.compat.v1.placeholder(np.float32,shape)\n",
    "        assign_timg = tf.compat.v1.placeholder(np.float32,shape)\n",
    "        assign_tlab = tf.compat.v1.placeholder(np.float32,(1,self.model.num_labels))\n",
    "\n",
    "        # these are the variables to initialize when we run\n",
    "        set_modifier = tf.compat.v1.assign(modifier, assign_modifier)\n",
    "        setup = []\n",
    "        setup.append(tf.compat.v1.assign(canchange, assign_canchange))\n",
    "        setup.append(tf.compat.v1.assign(timg, assign_timg))\n",
    "        setup.append(tf.compat.v1.assign(original, assign_original))\n",
    "        setup.append(tf.compat.v1.assign(simg, assign_simg))\n",
    "        setup.append(tf.compat.v1.assign(tlab, assign_tlab))'''\n",
    "        \n",
    "        newimg = (tf.tanh(modifier + simg)/2)*canchange+(1-canchange)*original\n",
    "        \n",
    "        output = model.predict(newimg)\n",
    "        \n",
    "        real = tf.reduce_sum((tlab)*output,1)\n",
    "        other = tf.reduce_max((1-tlab)*output - (tlab*10000),1)\n",
    "        \n",
    "        with tf.GradientTape() as tape:\n",
    "            if self.TARGETED:\n",
    "                # if targetted, optimize for making the other class most likely\n",
    "                loss1 = tf.maximum(0.0, other-real+.01)\n",
    "            else:\n",
    "                # if untargeted, optimize for making this class least likely.\n",
    "                loss1 = tf.maximum(0.0, real-other+.01)\n",
    "\n",
    "            # sum up the losses\n",
    "            loss2 = tf.reduce_sum(tf.square(newimg-tf.tanh(timg)/2))\n",
    "            loss = const*loss1+loss2\n",
    "\n",
    "            outgrad = tape.gradient(loss,[modifier])\n",
    "            #tf.gradients(loss, [modifier])[0]\n",
    "        \n",
    "        # setup the adam optimizer and keep track of variables we're creating\n",
    "        #start_vars = set(x.name for x in tf.global_variables())\n",
    "        #optimizer = tf.train.AdamOptimizer(self.LEARNING_RATE)\n",
    "            optimizer = tf.optimizers.Adam(self.LEARNING_RATE)\n",
    "            train = optimizer.minimize(loss, var_list=modifier)\n",
    "\n",
    "        #end_vars = tf.global_variables()\n",
    "        #new_vars = [x for x in end_vars if x.name not in start_vars]\n",
    "        #init = tf.variables_initializer(var_list=[modifier,canchange,simg,\n",
    "                                                  #original,timg,tlab]+new_vars)\n",
    "\n",
    "        \n",
    "        def doit(oimgs, labs, starts, valid, CONST):\n",
    "            # convert to tanh-space\n",
    "            imgs = np.arctanh(np.array(oimgs)*1.999999)\n",
    "            starts = np.arctanh(np.array(starts)*1.999999)\n",
    "\n",
    "            # initialize the variables\n",
    "            # todo what is sess.run doing?\n",
    "            sess.run(init)\n",
    "            sess.run(setup, {assign_timg: imgs, \n",
    "                                    assign_tlab:labs, \n",
    "                                    assign_simg: starts, \n",
    "                                    assign_original: oimgs,\n",
    "                                    assign_canchange: valid})\n",
    "\n",
    "            while CONST < self.LARGEST_CONST:\n",
    "                # try solving for each value of the constant\n",
    "                print('try const', CONST)\n",
    "                for step in range(self.MAX_ITERATIONS):\n",
    "                    feed_dict={const: CONST}\n",
    "\n",
    "                    # remember the old value\n",
    "                    oldmodifier = self.sess.run(modifier)\n",
    "\n",
    "                    if step%(self.MAX_ITERATIONS//10) == 0:\n",
    "                        print(step,*sess.run((loss1,loss2),feed_dict=feed_dict))\n",
    "\n",
    "                    # perform the update step\n",
    "                    _, works, scores = sess.run([train, loss1, output], feed_dict=feed_dict)\n",
    "\n",
    "                    if np.all(scores>=-.0001) and np.all(scores <= 1.0001):\n",
    "                        if np.allclose(np.sum(scores,axis=1), 1.0, atol=1e-3):\n",
    "                            if not self.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK:\n",
    "                                raise Exception(\"The output of model.predict should return the pre-softmax layer. It looks like you are returning the probability vector (post-softmax). If you are sure you want to do that, set attack.I_KNOW_WHAT_I_AM_DOING_AND_WANT_TO_OVERRIDE_THE_PRESOFTMAX_CHECK = True\")\n",
    "                    \n",
    "                    if works < .0001 and self.ABORT_EARLY:\n",
    "                        # it worked previously, restore the old value and finish\n",
    "                        self.sess.run(set_modifier, {assign_modifier: oldmodifier})\n",
    "                        grads, scores, nimg = sess.run((outgrad, output,newimg),\n",
    "                                                       feed_dict=feed_dict)\n",
    "\n",
    "                        l2s=np.square(nimg-np.tanh(imgs)/2).sum(axis=(1,2,3))\n",
    "                        return grads, scores, nimg, CONST\n",
    "\n",
    "                # we didn't succeed, increase constant and try again\n",
    "                CONST *= self.const_factor\n",
    "        return doit\n",
    "        \n",
    "    def attack(self, imgs, targets):\n",
    "        \"\"\"\n",
    "        Perform the L_0 attack on the given images for the given targets.\n",
    "        If self.targeted is true, then the targets represents the target labels.\n",
    "        If self.targeted is false, then targets are the original class labels.\n",
    "        \"\"\"\n",
    "        r = []\n",
    "        for i,(img,target) in enumerate(zip(imgs, targets)):\n",
    "            print(\"Attack iteration\",i)\n",
    "            r.extend(self.attack_single(img, target))\n",
    "        return np.array(r)\n",
    "\n",
    "    def attack_single(self, img, target):\n",
    "        \"\"\"\n",
    "        Run the attack on a single image and label\n",
    "        \"\"\"\n",
    "\n",
    "        # the pixels we can change\n",
    "        valid = np.ones((1,self.model.image_size,self.model.image_size,self.model.num_channels))\n",
    "\n",
    "        # the previous image\n",
    "        prev = np.copy(img).reshape((1,self.model.image_size,self.model.image_size,\n",
    "                                     self.model.num_channels))\n",
    "\n",
    "        # initially set the solution to None, if we can't find an adversarial\n",
    "        # example then we will return None as the solution.\n",
    "        last_solution = None\n",
    "        const = self.INITIAL_CONST\n",
    "\n",
    "        equal_count = None\n",
    "    \n",
    "        while True:\n",
    "            # try to solve given this valid map\n",
    "            res = self.grad([np.copy(img)], [target], np.copy(prev), \n",
    "                       valid, const)\n",
    "            if res == None:\n",
    "                # the attack failed, we return this as our final answer\n",
    "                print(\"Final answer\",equal_count)\n",
    "                return last_solution\n",
    "    \n",
    "            # the attack succeeded, now we pick new pixels to set to 0\n",
    "            restarted = False\n",
    "            gradientnorm, scores, nimg, const = res\n",
    "            if self.REDUCE_CONST: const /= 2\n",
    "    \n",
    "            equal_count = self.model.image_size**2-np.sum(np.all(np.abs(img-nimg[0])<.0001,axis=2))\n",
    "            print(\"Forced equal:\",np.sum(1-valid),\n",
    "                  \"Equal count:\",equal_count)\n",
    "            if np.sum(valid) == 0:\n",
    "                # if no pixels changed, return \n",
    "                return [img]\n",
    "    \n",
    "            if self.independent_channels:\n",
    "                # we are allowed to change each channel independently\n",
    "                valid = valid.flatten()\n",
    "                totalchange = abs(nimg[0]-img)*np.abs(gradientnorm[0])\n",
    "            else:\n",
    "                # we care only about which pixels change, not channels independently\n",
    "                # compute total change as sum of change for each channel\n",
    "                valid = valid.reshape((self.model.image_size**2,self.model.num_channels))\n",
    "                totalchange = abs(np.sum(nimg[0]-img,axis=2))*np.sum(np.abs(gradientnorm[0]),axis=2)\n",
    "            totalchange = totalchange.flatten()\n",
    "\n",
    "            # set some of the pixels to 0 depending on their total change\n",
    "            did = 0\n",
    "            for e in np.argsort(totalchange):\n",
    "                if np.all(valid[e]):\n",
    "                    did += 1\n",
    "                    valid[e] = 0\n",
    "\n",
    "                    if totalchange[e] > .01:\n",
    "                        # if this pixel changed a lot, skip\n",
    "                        break\n",
    "                    if did >= .3*equal_count**.5:\n",
    "                        # if we changed too many pixels, skip\n",
    "                        break\n",
    "\n",
    "            valid = np.reshape(valid,(1,self.model.image_size,self.model.image_size,-1))\n",
    "            print(\"Now forced equal:\",np.sum(1-valid))\n",
    "    \n",
    "            last_solution = prev = nimg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
