{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras import Model, layers\n",
    "from tensorflow.keras.layers import AveragePooling2D, Dense, Flatten, Conv2D, MaxPool2D\n",
    "from absl import app, flags\n",
    "from easydict import EasyDict\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import foolbox as fb\n",
    "#rom foolbox import TensorFlowModel, accuracy, samples\n",
    "#mport foolbox.attacks as fa\n",
    "from cleverhans.future.tf2.attacks import projected_gradient_descent, fast_gradient_method, carlini_wagner_l2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "fmodel = fb.models.TensorFlowModel(model, bounds=(0,1))\n",
    "attack = fb.attacks.L2CarliniWagnerAttack(binary_search_steps = 10,\n",
    "        steps = 10000,\n",
    "        stepsize = 0.03,\n",
    "        confidence = 0,\n",
    "        initial_const = .1,\n",
    "        abort_early = True)\n",
    "adv_foolbox, clipped_adversarials, success = attack(\n",
    "    fmodel,\n",
    "    x_to_attack,\n",
    "    y_to_attack,\n",
    "    epsilons=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_inputs = tf.reshape(x_to_attack, [10,28,28,1]);\n",
    "\n",
    "adv_cleverhans = carlini_wagner_l2(model,clean_inputs,y=None,\n",
    "               batch_size=10,\n",
    "               clip_min=0.,\n",
    "               clip_max=1.,\n",
    "               binary_search_steps=10,\n",
    "               max_iterations=10000,\n",
    "               abort_early=False,\n",
    "               confidence=0.,\n",
    "               initial_const=.1,\n",
    "               learning_rate=0.03)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:31<00:00, 63.52it/s]\n",
      "100%|██████████| 2000/2000 [00:32<00:00, 62.40it/s]\n",
      "100%|██████████| 2000/2000 [00:32<00:00, 62.01it/s]\n",
      "100%|██████████| 2000/2000 [00:30<00:00, 65.06it/s]\n",
      "100%|██████████| 2000/2000 [00:30<00:00, 65.84it/s]\n",
      "100%|██████████| 2000/2000 [00:29<00:00, 67.60it/s]\n",
      "100%|██████████| 2000/2000 [00:28<00:00, 69.54it/s]\n",
      "100%|██████████| 2000/2000 [00:29<00:00, 68.51it/s]\n",
      "100%|██████████| 2000/2000 [00:29<00:00, 67.17it/s]\n",
      "100%|██████████| 2000/2000 [00:32<00:00, 61.93it/s]\n"
     ]
    }
   ],
   "source": [
    "adv_eigene = cw_2_eigene_implementierung(model,x_to_attack, steps=2000, const=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Distance für Foolbox"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.27698365,\n",
       " 4.467263,\n",
       " 3.0980809,\n",
       " 1.4680731,\n",
       " 0.52765006,\n",
       " 2.3055704,\n",
       " 1.6391438,\n",
       " 4.9969068,\n",
       " 3.2843807,\n",
       " 1.7282153]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_fb = [tf.norm(adv_foolbox[i]-x_to_attack[i]).numpy() for i in range(len(x_to_attack))]\n",
    "l2_fb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Distance für Cleverhans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.41441822,\n",
       " 3.494786,\n",
       " 3.8353348,\n",
       " 2.3682518,\n",
       " 0.80121773,\n",
       " 2.360535,\n",
       " 1.9128388,\n",
       " 5.1769786,\n",
       " 1.7254946,\n",
       " 1.8560805]"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_ch = [tf.norm(adv_cleverhans[i].flatten()-x_to_attack[i]).numpy() for i in range(len(x_to_attack))]\n",
    "l2_ch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### L2 Distance für eigene Implementierung"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.36972156,\n",
       " 1.0551662,\n",
       " 0.88235486,\n",
       " 0.42711136,\n",
       " 0.39737734,\n",
       " 0.96320105,\n",
       " 0.4400873,\n",
       " 1.1267165,\n",
       " 0.53550375,\n",
       " 0.8555029]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l2_ei = [tf.norm(adv_eigene[i]-x_to_attack[i]).numpy() for i in range(len(x_to_attack))]\n",
    "l2_ei"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unsupported operand type(s) for -: 'list' and 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-144-cd1f706f18f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2_ch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'b'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2_ei\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'g'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m.2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ml2_fb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwidth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malign\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'center'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautoscale\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unsupported operand type(s) for -: 'list' and 'float'"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "ax = plt.subplot(111)\n",
    "w = 0.3\n",
    "ax.bar(range(10),l2_ch,width=w, color='b', align='center')\n",
    "ax.bar(range(10),l2_ei, width=w, color='g', align='center')\n",
    "ax.bar(range(10)-.2,l2_fb, width=w, color='r', align='center')\n",
    "\n",
    "ax.autoscale(tight=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[5, 0, 4, 1, 9, 2, 1, 3, 1, 4]\n",
      "[5, 0, 4, 1, 9, 2, 1, 3, 1, 4]\n",
      "[3, 2, 7, 8, 4, 8, 8, 7, 4, 7]\n",
      "[3, 2, 1, 8, 4, 8, 8, 7, 4, 7]\n",
      "[3, 2, 1, 8, 8, 8, 8, 7, 4, 7]\n"
     ]
    }
   ],
   "source": [
    "res = fmodel(x_to_attack[:10])\n",
    "adv_fb_res = fmodel(adv_foolbox)\n",
    "adv_ch_res = fmodel(adv_cleverhans)\n",
    "adv_eigen_res = fmodel(np.array(adv_eigene))\n",
    "\n",
    "print(y_to_attack.numpy().tolist())\n",
    "print([np.argmax(x) for x in res])\n",
    "print([np.argmax(x) for x in adv_fb_res])\n",
    "print([np.argmax(x) for x in adv_ch_res])\n",
    "print([np.argmax(x) for x in adv_eigen_res])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CNN-5 Definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'conv_1': (5, 5, 1, 6),\n",
    "    # 5x5 conv, 6 inputs, 16 outputs\n",
    "    'conv_2': (5, 5, 6, 16),\n",
    "    #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "    'conv_3': (1, 1, 16, 120),\n",
    "    # fully connected, 5*5*16 inputs, 120 outputs\n",
    "    'dense_1': (5*5*16, 120),\n",
    "    # fully connected, 120 inputs, 84 outputs\n",
    "    'dense_2': (120, 84),\n",
    "    # 84 inputs, 10 outputs (class prediction)\n",
    "    'dense_3': (84, 10),\n",
    "}\n",
    "bias_shapes = {\n",
    "    #output depth\n",
    "    'conv_1': (6),\n",
    "    'conv_2': (16),\n",
    "    'dense_1': (120),\n",
    "    'dense_2': (84),\n",
    "    'dense_3': (10),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, weights, mask, biases, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = weights\n",
    "        self.m = mask\n",
    "        self.b = biases\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='valid'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "#        return tf.keras.layers.AveragePooling2D(pool_size=(self.k, self.k), strides=None, padding=self.p, data_format='channels_first')(inputs)\n",
    "        return tf.nn.avg_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p,)# data_format='NCHW')\n",
    "    \n",
    "\n",
    "        \n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "\n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'tanh'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'tanh':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        if self.a == None:\n",
    "            return x\n",
    "        \n",
    "        \n",
    "        \n",
    "class CustomConvModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvModel, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], True, 1, 'SAME')#'VALID')\n",
    "        self.maxpool1 = CustomPoolLayer(k=2, padding='SAME')\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], True, 1, 'VALID')\n",
    "        self.maxpool2 = CustomPoolLayer(k=2, padding='VALID')\n",
    "\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], True, 'tanh')\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], True, 'tanh')\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, None)\n",
    "        self.pre_softmax = None\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, shape=[-1,28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x =  self.dense3(x)\n",
    "        self.pre_softmax = x\n",
    "        return tf.nn.softmax(x)\n",
    "    \n",
    "    \n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, to_convergence=True):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "\n",
    "    model = CustomConvModel()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x_to_attack = tf.convert_to_tensor(x_train[:10].reshape(10,28*28))\n",
    "y_to_attack = tf.convert_to_tensor([y_train[:10]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5928 - accuracy: 0.8909 - val_loss: 1.5168 - val_accuracy: 0.9490\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5051 - accuracy: 0.9595 - val_loss: 1.4944 - val_accuracy: 0.9686\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4917 - accuracy: 0.9718 - val_loss: 1.4897 - val_accuracy: 0.9729\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4859 - accuracy: 0.9770 - val_loss: 1.4828 - val_accuracy: 0.9800\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4821 - accuracy: 0.9803 - val_loss: 1.4837 - val_accuracy: 0.9782\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4794 - accuracy: 0.9830 - val_loss: 1.4802 - val_accuracy: 0.9810\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4769 - accuracy: 0.9852 - val_loss: 1.4813 - val_accuracy: 0.9804\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4757 - accuracy: 0.9862 - val_loss: 1.4806 - val_accuracy: 0.9811\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4744 - accuracy: 0.9876 - val_loss: 1.4781 - val_accuracy: 0.9833\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4726 - accuracy: 0.9893 - val_loss: 1.4765 - val_accuracy: 0.9848\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4721 - accuracy: 0.9898 - val_loss: 1.4766 - val_accuracy: 0.9849\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4713 - accuracy: 0.9904 - val_loss: 1.4771 - val_accuracy: 0.9840\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4710 - accuracy: 0.9906 - val_loss: 1.4764 - val_accuracy: 0.9847\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4705 - accuracy: 0.9911 - val_loss: 1.4774 - val_accuracy: 0.9837\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4702 - accuracy: 0.9915 - val_loss: 1.4755 - val_accuracy: 0.9860\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4691 - accuracy: 0.9924 - val_loss: 1.4769 - val_accuracy: 0.9845\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4690 - accuracy: 0.9926 - val_loss: 1.4756 - val_accuracy: 0.9857\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4687 - accuracy: 0.9930 - val_loss: 1.4752 - val_accuracy: 0.9861\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4684 - accuracy: 0.9932 - val_loss: 1.4757 - val_accuracy: 0.9860\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4687 - accuracy: 0.9929 - val_loss: 1.4761 - val_accuracy: 0.9853\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4689 - accuracy: 0.9926 - val_loss: 1.4735 - val_accuracy: 0.9878\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4672 - accuracy: 0.9943 - val_loss: 1.4775 - val_accuracy: 0.9837\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4674 - accuracy: 0.9941 - val_loss: 1.4741 - val_accuracy: 0.9869\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4671 - accuracy: 0.9944 - val_loss: 1.4756 - val_accuracy: 0.9859\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CustomConvModel at 0x7f277e60e0d0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = initialize_base_model(999,'')\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eigene Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cw_2_eigene_implementierung(model, xs, const=1, steps=1000, learning_rate=.03, ):\n",
    "    xa=[]\n",
    "    \n",
    "    for x in xs:\n",
    "        w = tf.Variable(initial_value=tf.zeros(x.shape))\n",
    "        \n",
    "        show_adv = []\n",
    "        optimizer = tf.keras.optimizers.Adam(.03)\n",
    "        target = find_second_most_probable_class(get_logits(model,x))\n",
    "        adv_image = .5 * (tf.tanh(w) + 1)\n",
    "        best_adv_image = adv_image\n",
    "        best_l2_dist = tf.norm(adv_image - x).numpy()\n",
    "        for i in tqdm(range(steps)):\n",
    "            with tf.GradientTape() as tape:\n",
    "\n",
    "                adv_image = .5 * (tf.tanh(w) + 1)\n",
    "                preds = model(adv_image)\n",
    "                #const= CONST\n",
    "                loss = loss_function(model= model, x=x, adv_image=adv_image, const=const, target=target)\n",
    "                l2_distance = tf.norm(adv_image - x).numpy()\n",
    "                if l2_distance < best_l2_dist and np.argmax(preds) == target:\n",
    "                    best_adv_image = adv_image\n",
    "                    best_l2_dist = l2_distance\n",
    "                grads = tape.gradient(loss, w)\n",
    "                optimizer.apply_gradients([(grads, w)])\n",
    "                #print('l2 dist:', l2_distance)\n",
    "                #print('target class: ',target)\n",
    "                #print('pred of adv ex: ',np.argmax(model(adv_image)))\n",
    "                #plt.figure()\n",
    "                #plt.imshow(tf.reshape(adv_image, (28,28)))\n",
    "                #plt.show()\n",
    "                \n",
    "        xa.append(best_adv_image)\n",
    "    return xa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(model, x, adv_image, const, target):\n",
    "    l1 = tf.square(tf.norm(adv_image - x))\n",
    "    logit_of_best_other = get_logit_of_best_except_target(model, adv_image, target)\n",
    "    logit_of_target = get_logit_of_target(model, adv_image,target)\n",
    "    l2 = const * tf.math.maximum(\n",
    "        logit_of_best_other\n",
    "        - logit_of_target\n",
    "        , 0 )\n",
    "    return l1 + l2\n",
    "\n",
    "\n",
    "def find_second_most_probable_class(logits):\n",
    "    logits = logits.numpy().flatten()\n",
    "    return np.argpartition(logits,len(logits)-2)[len(logits)-2]\n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "def get_logit_of_best_except_target(model, adv_x, target):\n",
    "    all_logits = get_logits(model, adv_x)\n",
    "    #print('all_logits', all_logits)\n",
    "    most_probable_class = tf.math.argmax(all_logits)\n",
    "    second_most_probable_class = find_second_most_probable_class(all_logits)\n",
    "    if target == most_probable_class: \n",
    "        return all_logits[second_most_probable_class ]\n",
    "    if target != most_probable_class: \n",
    "        return all_logits[most_probable_class ]\n",
    "\n",
    "def get_logit_of_target(model, adv_x, target):\n",
    "    all_logits = get_logits(model, adv_x)\n",
    "    return all_logits[target]\n",
    "\n",
    "def get_logits(model, x):\n",
    "    model(x)\n",
    "    return model.pre_softmax[0]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Don't use all available GPU Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * x of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
