{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'cnn'\n",
    "EXPERIMENT_NAME = 'cnn-global-magnitude-unstruct'\n",
    "ITERATIONS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import foolbox as fb\n",
    "import random\n",
    "import json\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there\n",
      "dis do or no?\n",
      "hello\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0540 - accuracy: 0.4061 - val_loss: 2.0478 - val_accuracy: 0.4118\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0537 - accuracy: 0.4067 - val_loss: 2.0481 - val_accuracy: 0.4120\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0536 - accuracy: 0.4069 - val_loss: 2.0490 - val_accuracy: 0.4101\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0536 - accuracy: 0.4068 - val_loss: 2.0491 - val_accuracy: 0.4110\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0535 - accuracy: 0.4072 - val_loss: 2.0482 - val_accuracy: 0.4113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:12, 12.62s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0537 - accuracy: 0.4065 - val_loss: 2.0485 - val_accuracy: 0.4111\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0538 - accuracy: 0.4066 - val_loss: 2.0485 - val_accuracy: 0.4118\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0537 - accuracy: 0.4072 - val_loss: 2.0475 - val_accuracy: 0.4115\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0534 - accuracy: 0.4070 - val_loss: 2.0473 - val_accuracy: 0.4120\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0533 - accuracy: 0.4072 - val_loss: 2.0480 - val_accuracy: 0.4119\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0540 - accuracy: 0.4060 - val_loss: 2.0474 - val_accuracy: 0.4121\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0532 - accuracy: 0.4072 - val_loss: 2.0479 - val_accuracy: 0.4112\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0533 - accuracy: 0.4069 - val_loss: 2.0483 - val_accuracy: 0.4122\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0532 - accuracy: 0.4070 - val_loss: 2.0493 - val_accuracy: 0.4105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:39, 19.95s/it]\u001b[A\n",
      " 33%|███▎      | 1/3 [00:43<01:26, 43.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there\n",
      "dis do or no?\n",
      "hello\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0532 - accuracy: 0.4073 - val_loss: 2.0471 - val_accuracy: 0.4122\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0531 - accuracy: 0.4072 - val_loss: 2.0480 - val_accuracy: 0.4106\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0531 - accuracy: 0.4071 - val_loss: 2.0472 - val_accuracy: 0.4125\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0530 - accuracy: 0.4075 - val_loss: 2.0487 - val_accuracy: 0.4112\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0529 - accuracy: 0.4074 - val_loss: 2.0472 - val_accuracy: 0.4119\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0530 - accuracy: 0.4069 - val_loss: 2.0477 - val_accuracy: 0.4106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:15, 15.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0534 - accuracy: 0.4065 - val_loss: 2.0473 - val_accuracy: 0.4122\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0536 - accuracy: 0.4066 - val_loss: 2.0474 - val_accuracy: 0.4125\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0529 - accuracy: 0.4076 - val_loss: 2.0490 - val_accuracy: 0.4105\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0530 - accuracy: 0.4069 - val_loss: 2.0485 - val_accuracy: 0.4108\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0526 - accuracy: 0.4074 - val_loss: 2.0477 - val_accuracy: 0.4126\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0529 - accuracy: 0.4072 - val_loss: 2.0469 - val_accuracy: 0.4127\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0534 - accuracy: 0.4068 - val_loss: 2.0473 - val_accuracy: 0.4120\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0526 - accuracy: 0.4076 - val_loss: 2.0471 - val_accuracy: 0.4121\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0527 - accuracy: 0.4072 - val_loss: 2.0472 - val_accuracy: 0.4112\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [00:43, 21.53s/it]\u001b[A\n",
      " 67%|██████▋   | 2/3 [01:30<00:44, 44.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello there\n",
      "dis do or no?\n",
      "hello\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0525 - accuracy: 0.4074 - val_loss: 2.0470 - val_accuracy: 0.4114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0529 - accuracy: 0.4070 - val_loss: 2.0465 - val_accuracy: 0.4129\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0525 - accuracy: 0.4080 - val_loss: 2.0475 - val_accuracy: 0.4113\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0530 - accuracy: 0.4067 - val_loss: 2.0465 - val_accuracy: 0.4129\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0524 - accuracy: 0.4077 - val_loss: 2.0477 - val_accuracy: 0.4116\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0522 - accuracy: 0.4078 - val_loss: 2.0467 - val_accuracy: 0.4129\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0522 - accuracy: 0.4083 - val_loss: 2.0479 - val_accuracy: 0.4106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [00:18, 18.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]]\n",
      "Epoch 1/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0524 - accuracy: 0.4077 - val_loss: 2.0470 - val_accuracy: 0.4116\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0530 - accuracy: 0.4068 - val_loss: 2.0478 - val_accuracy: 0.4111\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0523 - accuracy: 0.4079 - val_loss: 2.0468 - val_accuracy: 0.4126\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0525 - accuracy: 0.4079 - val_loss: 2.0485 - val_accuracy: 0.4100\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0524 - accuracy: 0.4079 - val_loss: 2.0559 - val_accuracy: 0.4059\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0535 - accuracy: 0.4067 - val_loss: 2.0485 - val_accuracy: 0.4106\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0529 - accuracy: 0.4072 - val_loss: 2.0470 - val_accuracy: 0.4117\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0523 - accuracy: 0.4082 - val_loss: 2.0489 - val_accuracy: 0.4099\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0528 - accuracy: 0.4072 - val_loss: 2.0469 - val_accuracy: 0.4134\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0534 - accuracy: 0.4063 - val_loss: 2.0473 - val_accuracy: 0.4122\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0517 - accuracy: 0.4080 - val_loss: 2.0474 - val_accuracy: 0.4122\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0523 - accuracy: 0.4075 - val_loss: 2.0467 - val_accuracy: 0.4133\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0521 - accuracy: 0.4079 - val_loss: 2.0458 - val_accuracy: 0.4143\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0521 - accuracy: 0.4078 - val_loss: 2.0458 - val_accuracy: 0.4138\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0515 - accuracy: 0.4090 - val_loss: 2.0477 - val_accuracy: 0.4115\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0521 - accuracy: 0.4081 - val_loss: 2.0460 - val_accuracy: 0.4138\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.0521 - accuracy: 0.4078 - val_loss: 2.0489 - val_accuracy: 0.4108\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [01:07, 33.88s/it]\u001b[A\n",
      "100%|██████████| 3/3 [02:41<00:00, 53.75s/it]\n"
     ]
    }
   ],
   "source": [
    "run_experiment(structure='unstructured', method='magnitude', scope='global', iterations=ITERATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(structure='unstructured', method='random', scope='global', iterations=10):\n",
    "    \n",
    "    experiment_name = f'{ARCHITECTURE}-{method}-{scope}-{structure}'\n",
    "    pgd_success_rates = []\n",
    "    cw_success_rates = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    compression_rates = [1, 64]\n",
    "    pruning_ratios = [1-1/x for x in compression_rates]\n",
    "\n",
    "    for j in tqdm(range(iterations)):\n",
    "        accuracies = []\n",
    "        pgd_success_rate = []\n",
    "        cw_success_rate = []\n",
    "        try: \n",
    "            del model\n",
    "        except:\n",
    "            ;\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = initialize_base_model(j, experiment_name=experiment_name, save_weights=True)\n",
    "        for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "            model.load_weights(f'./saved-weights/{experiment_name}-{j}')\n",
    "            print(model.get_weights()[14])\n",
    "            for i in range(index + 1):\n",
    "                if i != index:\n",
    "                    if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_random_global_unstruct(pruning_ratio)\n",
    "                    if  method=='random' and scope=='global' and structure=='structured':\n",
    "                        model.prune_random_global_struct(pruning_ratio)\n",
    "                    if  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_random_local_unstruct(pruning_ratio)\n",
    "                    if  method=='random' and scope=='local' and structure=='structured':\n",
    "                        model.prune_random_local_struct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_magnitude_global_unstruct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                        model.prune_magnitude_global_struct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_magnitude_local_unstruct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                        model.prune_magnitude_local_struct(pruning_ratio)\n",
    "\n",
    "\n",
    "                    # must recompile otherwise pruned weights will get updated\n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                                  metrics=['accuracy'],\n",
    "                                  experimental_run_tf_function=False\n",
    "                                 )\n",
    "                    model = train_model(model, to_convergence=False)\n",
    "                if i == index:\n",
    "                    if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_random_global_unstruct(pruning_ratio)\n",
    "                    if  method=='random' and scope=='global' and structure=='structured':\n",
    "                        model.prune_random_global_struct(pruning_ratio)\n",
    "                    if  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_random_local_unstruct(pruning_ratio)\n",
    "                    if  method=='random' and scope=='local' and structure=='structured':\n",
    "                        model.prune_random_local_struct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_magnitude_global_unstruct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                        model.prune_magnitude_global_struct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_magnitude_local_unstruct(pruning_ratio)\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                        model.prune_magnitude_local_struct(pruning_ratio)\n",
    "                        \n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                                  metrics=['accuracy'],\n",
    "                                  experimental_run_tf_function=False\n",
    "                                 )\n",
    "                    model = train_model(model, to_convergence=True)\n",
    "                    accuracies.append(model.evaluate(x_test, y_test, verbose=0))\n",
    "                    pgd_success_rate.append(pgd_attack(model))\n",
    "                    #cw_success_rate.append(cw2_attack(model))\n",
    "\n",
    "        all_accuracies.append(accuracies)\n",
    "        pgd_success_rates.append(pgd_success_rate)\n",
    "        cw_success_rates.append(cw_success_rate)\n",
    "    #write to csv and json\n",
    "\n",
    "    pd.DataFrame(all_accuracies).to_csv(f'saved-results/{experiment_name}-accuracies.csv',index=False)\n",
    "    with open(f'saved-results/{experiment_name}-accuracies.json', 'w') as f:\n",
    "        json.dump(all_accuracies, f)\n",
    "\n",
    "    pd.DataFrame(pgd_success_rates).to_csv(f'saved-results/{experiment_name}-pgd-success.csv',index=False)\n",
    "    with open(f'saved-results/{experiment_name}-pgd-success.json', 'w') as f:\n",
    "        json.dump(pgd_success_rates, f)\n",
    "    pd.DataFrame(cw_success_rates).to_csv(f'saved-results/{experiment_name}-cw0-success.csv',index=False)\n",
    "    with open(f'saved-results/{experiment_name}-cw0-success', 'w') as f:\n",
    "        json.dump(cw_success_rates, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune, Train Attack Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_success_rates = []\n",
    "cw_success_rates = []\n",
    "all_accuracies = []\n",
    "\n",
    "compression_rates = [1, 2, 4, 8, 16, 32, 64]\n",
    "pruning_ratios = [1-1/x for x in compression_rates]\n",
    "    \n",
    "for j in tqdm(range(ITERATIONS)):\n",
    "    accuracies = []\n",
    "    pgd_success_rate = []\n",
    "    cw_success_rate = []\n",
    "    model = initialize_base_model(j, save_weights=True)\n",
    "    for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "        model.load_weights(f'./saved-weights/{EXPERIMENT_NAME}-{j}')\n",
    "        for i in range(index + 1):\n",
    "            if i != index:\n",
    "                model.prune_globally(pruning_ratios[i])\n",
    "                # must recompile otherwise pruned weights will get updated\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                              experimental_run_tf_function=False\n",
    "                             )\n",
    "                model = train_model(model, to_convergence=False)\n",
    "            if i == index:\n",
    "                print('final pruning and eval')\n",
    "                model.prune_globally(pruning_ratios[i])\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                              experimental_run_tf_function=False\n",
    "                             )\n",
    "                model = train_model(model, to_convergence=True)\n",
    "                accuracies.append(model.evaluate(x_test, y_test, verbose=0))\n",
    "                pgd_success_rate.append(pgd_attack(model))\n",
    "                #cw_success_rate.append(cw2_attack(model))\n",
    "                \n",
    "    all_accuracies.append(accuracies)\n",
    "    pgd_success_rates.append(pgd_success_rate)\n",
    "    cw_success_rates.append(cw_success_rate)\n",
    "#write to csv and json\n",
    "\n",
    "pd.DataFrame(all_accuracies).to_csv(f'saved-results/{EXPERIMENT_NAME}-accuracies.csv',index=False)\n",
    "with open(f'saved-results/{EXPERIMENT_NAME}-accuracies.json', 'w') as f:\n",
    "    json.dump(all_accuracies, f)\n",
    "    \n",
    "pd.DataFrame(pgd_success_rates).to_csv(f'saved-results/{EXPERIMENT_NAME}-pgd-success.csv',index=False)\n",
    "with open(f'saved-results/{EXPERIMENT_NAME}-pgd-success.json', 'w') as f:\n",
    "    json.dump(pgd_success_rates, f)\n",
    "pd.DataFrame(cw_success_rates).to_csv(f'saved-results/{EXPERIMENT_NAME}-cw0-success.csv',index=False)\n",
    "with open(f'saved-results/{EXPERIMENT_NAME}-cw0-success', 'w') as f:\n",
    "    json.dump(cw_success_rates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx,x in enumerate(all_accuracies):\n",
    "    for idy,y in enumerate(x):\n",
    "        all_accuracies[idx][idy] = [float(y[0]), float(y[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_success_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reinitialize_params():\n",
    "    print('dis do or no?')\n",
    "    weights = {\n",
    "        # 5x5 conv, 1 input, 6 outputs\n",
    "        'weights_conv_1': tf.Variable(tf.random.normal([5, 5, 1, 6])),\n",
    "        # 5x5 conv, 6 inputs, 16 outputs\n",
    "        'weights_conv_2': tf.Variable(tf.random.normal([5, 5, 6, 16])),\n",
    "        #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "        'weights_conv_3': tf.Variable(tf.random.normal([1, 1, 16, 120])),\n",
    "        # fully connected, 5*5*16 inputs, 120 outputs\n",
    "        'weights_dense_1': tf.Variable(tf.random.normal([5*5*16, 120])),\n",
    "        # fully connected, 120 inputs, 84 outputs\n",
    "        'weights_dense_2': tf.Variable(tf.random.normal([120, 84])),\n",
    "        # 84 inputs, 10 outputs (class prediction)\n",
    "        'weights_dense_3': tf.Variable(tf.random.normal([84, 10])),\n",
    "    }\n",
    "    #print(weights['weights_dense_3'])\n",
    "\n",
    "    masks = {\n",
    "        # 5x5 conv, 1 input, 6 outputs\n",
    "        'mask_conv_1': tf.Variable(tf.ones([5, 5, 1, 6]), trainable=False),\n",
    "        # 5x5 conv, 6 inputs, 16 outputs\n",
    "        'mask_conv_2': tf.Variable(tf.ones([5, 5, 6, 16]), trainable=False),\n",
    "        #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "        'mask_conv_3': tf.Variable(tf.ones([1, 1, 16, 120]), trainable=False),\n",
    "        # fully connected, 5*5*16 inputs, 120 outputs\n",
    "        'mask_dense_1': tf.Variable(tf.ones([5*5*16, 120]), trainable=False),\n",
    "        # fully connected, 120 inputs, 84 outputs\n",
    "        'mask_dense_2': tf.Variable(tf.ones([120, 84]), trainable=False),\n",
    "        # 84 inputs, 10 outputs (class prediction)\n",
    "        'mask_dense_3': tf.Variable(tf.ones([84, 10]), trainable=False),\n",
    "    }\n",
    "\n",
    "    biases = {\n",
    "        #output depth\n",
    "        'bias_conv_1': tf.Variable(tf.random.normal([6])),\n",
    "        'bias_conv_2': tf.Variable(tf.random.normal([16])),\n",
    "        'bias_dense_1': tf.Variable(tf.random.normal([120])),\n",
    "        'bias_dense_2': tf.Variable(tf.random.normal([84])),\n",
    "        'bias_dense_3': tf.Variable(tf.random.normal([10])),\n",
    "    }\n",
    "    return True\n",
    "\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "    print('hello there')\n",
    "    reinitialize_params()\n",
    "    print('hello')\n",
    "    model = CustomConvModel()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    print(model.get_weights()[14])\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, to_convergence=True):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=5,\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "def prune_weights(model, pruning_ratio):\n",
    "    weights = model.get_weights()\n",
    "    weights_to_prune = model.get_weights()\n",
    "    for index, weight in enumerate(weights):\n",
    "        \n",
    "        if (index == 9) or (index == 12) :\n",
    "            #print(weight.shape)\n",
    "            #print(index)\n",
    "            flat_weights = weight.flatten()\n",
    "            flat_weights_to_prune = weights_to_prune[index+2].flatten()\n",
    "            #print (flat_weights_to_prune.shape, flat_weights.shape)\n",
    "            flat_weights_df = pd.DataFrame(flat_weights)\n",
    "            flat_weights_to_prune_df = pd.DataFrame(flat_weights_to_prune)\n",
    "            no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "            #print(len(flat_weights))\n",
    "            #print('no of weights',no_of_weights_to_prune)\n",
    "            #print('weights to prune shape', flat_weights_to_prune.shape)\n",
    "            indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "            for idx_to_delete in indices_to_delete:\n",
    "                flat_weights_to_prune[idx_to_delete] = 0\n",
    "            dims = weights_to_prune[index+2].shape\n",
    "            weights_reshaped = flat_weights_to_prune.reshape(dims)\n",
    "            weights_to_prune[index+2] = weights_reshaped\n",
    "    #print(weights_to_prune)\n",
    "    return weights_to_prune\n",
    "\n",
    "def pgd_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x,\n",
    "        y,\n",
    "        epsilons=[25/255]\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/500\n",
    "\n",
    "def cw2_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.L2CarliniWagnerAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x,\n",
    "        y,\n",
    "        #epsilons=[.5]\n",
    "        epsilons=None\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/len(y)\n",
    "\n",
    "def prune_conv_layers(pruning_ratio):\n",
    "    layer_to_prune = [0, 3]\n",
    "    pruned_weights = model.get_weights()\n",
    "    \n",
    "    for layer in layer_to_prune:\n",
    "        converted_weights = convert_from_hwio_to_iohw(model.get_weights()[layer])\n",
    "        converted_mask = convert_from_hwio_to_iohw(model.get_weights()[layer + 2]).numpy()\n",
    "        for input_index, input_layer in enumerate(converted_weights):\n",
    "\n",
    "            for kernel_index, kernel in enumerate(input_layer):\n",
    "                dims = kernel.shape\n",
    "                flat_weights = kernel.numpy().flatten()\n",
    "                flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "                #print(no_of_weights_to_prune)\n",
    "                indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "\n",
    "                converted_mask[input_index][kernel_index] = flat_masks.reshape(dims)\n",
    "        back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "        pruned_weights[layer+2] = back_converted_mask\n",
    "    \n",
    "    return pruned_weights\n",
    "\n",
    "def convert_from_hwio_to_iohw(weights_nchw):\n",
    "    return tf.transpose(weights_nchw, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "def convert_from_iohw_to_hwio(weights_nhwc):\n",
    "    return tf.transpose(weights_nhwc, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "def get_average_accuracies(all_accuracies):\n",
    "    acc_per_pruning_rate=[]\n",
    "    for i in range(len(all_accuracies)):\n",
    "        for j in range(len(all_accuracies[i])):\n",
    "\n",
    "            try:\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "            except:\n",
    "                acc_per_pruning_rate.append([])\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "    avg_acc_per_pruning_rate = [sum(x)/len(x) for x in acc_per_pruning_rate]; avg_acc_per_pruning_rate\n",
    "    return avg_acc_per_pruning_rate\n",
    "\n",
    "def get_average_success_rates(all_success_rates):\n",
    "    success_per_pruning_rate=[]\n",
    "    for i in range(len(all_success_rates)):\n",
    "        for j in range(len(all_success_rates[i])):\n",
    "\n",
    "            try:\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "            except:\n",
    "                success_per_pruning_rate.append([])\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "    avg_success_per_pruning_rate = [sum(x)/len(x) for x in success_per_pruning_rate];avg_success_per_pruning_rate\n",
    "    return avg_success_per_pruning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x = tf.convert_to_tensor(x_train[:500].reshape(500,28*28))\n",
    "y = tf.convert_to_tensor([y_train[:500]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'weights_conv_1': tf.Variable(tf.random.normal([5, 5, 1, 6])),\n",
    "    # 5x5 conv, 6 inputs, 16 outputs\n",
    "    'weights_conv_2': tf.Variable(tf.random.normal([5, 5, 6, 16])),\n",
    "    #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "    'weights_conv_3': tf.Variable(tf.random.normal([1, 1, 16, 120])),\n",
    "    # fully connected, 5*5*16 inputs, 120 outputs\n",
    "    'weights_dense_1': tf.Variable(tf.random.normal([5*5*16, 120])),\n",
    "    # fully connected, 120 inputs, 84 outputs\n",
    "    'weights_dense_2': tf.Variable(tf.random.normal([120, 84])),\n",
    "    # 84 inputs, 10 outputs (class prediction)\n",
    "    'weights_dense_3': tf.Variable(tf.random.normal([84, 10])),\n",
    "}\n",
    "\n",
    "masks = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'mask_conv_1': tf.Variable(tf.ones([5, 5, 1, 6]), trainable=False),\n",
    "    # 5x5 conv, 6 inputs, 16 outputs\n",
    "    'mask_conv_2': tf.Variable(tf.ones([5, 5, 6, 16]), trainable=False),\n",
    "    #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "    'mask_conv_3': tf.Variable(tf.ones([1, 1, 16, 120]), trainable=False),\n",
    "    # fully connected, 5*5*16 inputs, 120 outputs\n",
    "    'mask_dense_1': tf.Variable(tf.ones([5*5*16, 120]), trainable=False),\n",
    "    # fully connected, 120 inputs, 84 outputs\n",
    "    'mask_dense_2': tf.Variable(tf.ones([120, 84]), trainable=False),\n",
    "    # 84 inputs, 10 outputs (class prediction)\n",
    "    'mask_dense_3': tf.Variable(tf.ones([84, 10]), trainable=False),\n",
    "}\n",
    "\n",
    "biases = {\n",
    "    #output depth\n",
    "    'bias_conv_1': tf.Variable(tf.random.normal([6])),\n",
    "    'bias_conv_2': tf.Variable(tf.random.normal([16])),\n",
    "    'bias_dense_1': tf.Variable(tf.random.normal([120])),\n",
    "    'bias_dense_2': tf.Variable(tf.random.normal([84])),\n",
    "    'bias_dense_3': tf.Variable(tf.random.normal([10])),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, weights, mask, biases, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = weights\n",
    "        self.m = mask\n",
    "        self.b = biases\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='valid'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "#        return tf.keras.layers.AveragePooling2D(pool_size=(self.k, self.k), strides=None, padding=self.p, data_format='channels_first')(inputs)\n",
    "        return tf.nn.avg_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p,)# data_format='NCHW')\n",
    "    \n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, weights, mask, bias, activation = 'tanh'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.w = weights\n",
    "        self.b = bias\n",
    "        self.a = activation\n",
    "        self.m = mask\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'tanh':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        \n",
    "class CustomConvModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvModel, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(weights['weights_conv_1'], masks['mask_conv_1'], biases['bias_conv_1'], 1, 'SAME')#'VALID')\n",
    "        self.maxpool1 = CustomPoolLayer(k=2, padding='SAME')\n",
    "        self.conv2 = CustomConvLayer(weights['weights_conv_2'], masks['mask_conv_2'], biases['bias_conv_2'], 1, 'VALID')\n",
    "        self.maxpool2 = CustomPoolLayer(k=2, padding='VALID')\n",
    "        #self.conv3 = CustomConvLayer(weights['weights_conv_3'], masks['mask_conv_3'], biases['bias_dense_1'], 1, 'VALID')\n",
    "        self.dense1 = CustomDenseLayer(weights['weights_dense_1'], masks['mask_dense_1'], biases['bias_dense_1'], 'tanh')\n",
    "        self.dense2 = CustomDenseLayer(weights['weights_dense_2'], masks['mask_dense_2'], biases['bias_dense_2'], 'tanh')\n",
    "        self.dense3 = CustomDenseLayer(weights['weights_dense_3'], masks['mask_dense_3'], biases['bias_dense_3'], 'softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, shape=[-1,28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x =  self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            weights = model.get_weights()\n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        \n",
    "                        no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                        # find unpruned weights\n",
    "                        non_zero_weights = np.nonzero(flat_masks)[0]\n",
    "                        # calculate the amount of weights to be pruned this round\n",
    "                        no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                        # shuffle all non-zero weights\n",
    "                        random.shuffle(non_zero_weights)\n",
    "                        # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                        indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                        \n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = model.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "                    no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self, ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "        #flat out all weights:\n",
    "        conv_layer_to_prune = [0, 3]\n",
    "        dense_layer_to_prune = [6, 9, 12]\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            flat_weights = np.append(flat_weights, weights[x])\n",
    "            flat_mask = np.append(flat_mask, weights[x+2])\n",
    "            \n",
    "        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[x + 2] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            weights = model.get_weights()\n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        #flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                        #flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                        #print(no_of_weights_to_prune)\n",
    "                        #indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "\n",
    "\n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = model.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "\n",
    "                    no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                    indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self,ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "            \n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CustomConvModel()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "              \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x=x_train[:500],\n",
    "          y=y_train[:500],\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )\n",
    "#model.save('./saved-models/mini-pipeline-CNN-baseline-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.load_model('./saved-models/mini-pipeline-CNN-baseline-model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pgd_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x,\n",
    "        y,\n",
    "        epsilons=[25/255]\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pgd_attack(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
