{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative pruning pipeline\n",
    "Model: Multi Layer Perceptron\n",
    "\n",
    "Pruning Mehtods: global and local iterative magnitude pruning\n",
    "\n",
    "*Pruning functions as class methods*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "EXPERIMENT_NAME = 'mlp-global-magnitude-unstruct'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import foolbox as fb\n",
    "\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune, Train Attack Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loc_pruning_pgd_success_rates = []\n",
    "loc_pruning_cw_success_rates = []\n",
    "glob_pruning_pgd_success_rates = []\n",
    "glob_pruning_cw_success_rates = []\n",
    "loc_pruning_all_accuracies = []\n",
    "glob_pruning_all_accuracies = []\n",
    "for j in tqdm(range(3)):\n",
    "    model_for_glob_pruning = initialize_base_model(j, save_weights=True)\n",
    "    model_for_loc_pruning = initialize_base_model(j)\n",
    "    loc_accuracies = []\n",
    "    loc_pgd_success_rate = []\n",
    "    loc_cw_success_rate = []\n",
    "    glob_accuracies = []\n",
    "    glob_pgd_success_rate = []\n",
    "    glob_cw_success_rate = []\n",
    "    compression_rates = [1, 2, 4, 8, 16, 32, 64]\n",
    "    pruning_ratios = [1-1/x for x in compression_rates]\n",
    "    for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "        model_for_glob_pruning.load_weights(f'./saved-weights/{EXPERIMENT_NAME}-{j}')\n",
    "        model_for_loc_pruning.load_weights(f'./saved-weights/base-model-weights-{j}')\n",
    "        #iteratively prune and train (only to convergence if the final stage of pruning is reached)\n",
    "        for i in range(index + 1):\n",
    "            if i != index:\n",
    "                #glocbal pruning\n",
    "                model_for_glob_pruning.prune_globally(pruning_ratio)\n",
    "                model_for_glob_pruning.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                             )\n",
    "                \n",
    "                #local pruning\n",
    "                model_for_loc_pruning.prune_globally(pruning_ratio)\n",
    "                model_for_loc_pruning.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                             )\n",
    "                #fine-tune\n",
    "                model_for_glob_pruning = train_model(model_for_glob_pruning, to_convergence=False)\n",
    "                model_for_loc_pruning = train_model(model_for_loc_pruning, to_convergence=False)\n",
    "            if i == index:\n",
    "                print('final pruning and eval')\n",
    "                model_for_glob_pruning.prune_globally(pruning_ratio)\n",
    "                model_for_glob_pruning.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                             )\n",
    "                model_for_loc_pruning.prune_globally(pruning_ratio)\n",
    "                model_for_loc_pruning.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                             )\n",
    "                model_for_glob_pruning = train_model(model_for_glob_pruning, to_convergence=True)\n",
    "                model_for_loc_pruning = train_model(model_for_loc_pruning, to_convergence=True)\n",
    "                loc_accuracies.append(model_for_loc_pruning.evaluate(x_test, y_test, verbose=0))\n",
    "                loc_pgd_success_rate.append(pgd_attack(model_for_loc_pruning))\n",
    "                #loc_cw_success_rate.append(cw2_attack(model_for_loc_pruning))\n",
    "                glob_accuracies.append(model_for_glob_pruning.evaluate(x_test, y_test, verbose=0))\n",
    "                glob_pgd_success_rate.append(pgd_attack(model_for_glob_pruning))\n",
    "                #glob_cw_success_rate.append(cw2_attack(model_for_glob_pruning))\n",
    "                \n",
    "    loc_pruning_all_accuracies.append(loc_accuracies)\n",
    "    loc_pruning_pgd_success_rates.append(loc_pgd_success_rate)\n",
    "    loc_pruning_cw_success_rates.append(loc_cw_success_rate)\n",
    "    glob_pruning_all_accuracies.append(glob_accuracies)\n",
    "    glob_pruning_pgd_success_rates.append(glob_pgd_success_rate)\n",
    "    glob_pruning_cw_success_rates.append(glob_cw_success_rate)\n",
    "\n",
    "    \n",
    "\n",
    "#write to csv and json\n",
    "pd.DataFrame(loc_pruning_all_accuracies).to_csv('saved-results/mlp-loc-accuracies.csv',index=False)\n",
    "with open('saved-results/mlp-loc-accuracies.json', 'w') as f:\n",
    "    json.dump(loc_pruning_all_accuracies, f)\n",
    "    \n",
    "pd.DataFrame(loc_pruning_pgd_success_rates).to_csv('saved-results/mlp-loc-pgd-success.csv',index=False)\n",
    "with open('saved-results/mlp-loc-pgd-success.json', 'w') as f:\n",
    "    json.dump(loc_pruning_pgd_success_rates, f)\n",
    "    \n",
    "pd.DataFrame(loc_pruning_cw_success_rates).to_csv('saved-results/mlp-loc-cw2-success.csv',index=False)\n",
    "with open('saved-results/mlp-loc-cw2-success.json', 'w') as f:\n",
    "    json.dump(loc_pruning_cw_success_rates, f)\n",
    "    \n",
    "pd.DataFrame(glob_pruning_all_accuracies).to_csv('saved-results/mlp-glob-accuracies.csv',index=False)\n",
    "with open('saved-results/mlp-glob-accuracies.json', 'w') as f:\n",
    "    json.dump(glob_pruning_all_accuracies, f)\n",
    "    \n",
    "pd.DataFrame(glob_pruning_pgd_success_rates).to_csv('saved-results/mlp-glob-pgd-success.csv',index=False)\n",
    "with open('saved-results/mlp-glob-pgd-success.json', 'w') as f:\n",
    "    json.dump(glob_pruning_pgd_success_rates, f)\n",
    "    \n",
    "pd.DataFrame(glob_pruning_cw_success_rates).to_csv('saved-results/mlp-glob-cw2-success.csv',index=False)\n",
    "with open('saved-results/mlp-glob-cw2-success.json', 'w') as f:\n",
    "    json.dump(glob_pruning_cw_success_rates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "#write to csv and json\n",
    "pd.DataFrame(loc_pruning_all_accuracies).to_csv('saved-results/mlp-loc-accuracies.csv',index=False)\n",
    "with open('saved-results/mlp-loc-accuracies.json', 'w') as f:\n",
    "    json.dump(loc_pruning_all_accuracies, f)\n",
    "    \n",
    "pd.DataFrame(loc_pruning_pgd_success_rates).to_csv('saved-results/mlp-loc-pgd-success.csv',index=False)\n",
    "with open('saved-results/mlp-loc-pgd-success.json', 'w') as f:\n",
    "    json.dump(loc_pruning_pgd_success_rates, f)\n",
    "    \n",
    "pd.DataFrame(loc_pruning_cw_success_rates).to_csv('saved-results/mlp-loc-cw2-success.csv',index=False)\n",
    "with open('saved-results/mlp-loc-cw2-success.json', 'w') as f:\n",
    "    json.dump(loc_pruning_cw_success_rates, f)\n",
    "    \n",
    "pd.DataFrame(glob_pruning_all_accuracies).to_csv('saved-results/mlp-glob-accuracies.csv',index=False)\n",
    "with open('saved-results/mlp-glob-accuracies.json', 'w') as f:\n",
    "    json.dump(glob_pruning_all_accuracies, f)\n",
    "    \n",
    "pd.DataFrame(glob_pruning_pgd_success_rates).to_csv('saved-results/mlp-glob-pgd-success.csv',index=False)\n",
    "with open('saved-results/mlp-glob-pgd-success.json', 'w') as f:\n",
    "    json.dump(glob_pruning_pgd_success_rates, f)\n",
    "    \n",
    "pd.DataFrame(glob_pruning_cw_success_rates).to_csv('saved-results/mlp-glob-cw2-success.csv',index=False)\n",
    "with open('saved-results/mlp-glob-cw2-success.json', 'w') as f:\n",
    "    json.dump(glob_pruning_cw_success_rates, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_success_rates(all_success_rates):\n",
    "    success_per_pruning_rate=[]\n",
    "    for i in range(len(all_success_rates)):\n",
    "        for j in range(len(all_success_rates[i])):\n",
    "\n",
    "            try:\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "            except:\n",
    "                success_per_pruning_rate.append([])\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "    avg_success_per_pruning_rate = [sum(x)/len(x) for x in success_per_pruning_rate]\n",
    "    return avg_success_per_pruning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_average_accuracies(all_accuracies):\n",
    "    acc_per_pruning_rate=[]\n",
    "    for i in range(len(all_accuracies)):\n",
    "        for j in range(len(all_accuracies[i])):\n",
    "\n",
    "            try:\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "            except:\n",
    "                acc_per_pruning_rate.append([])\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "    avg_acc_per_pruning_rate = [sum(x)/len(x) for x in acc_per_pruning_rate]\n",
    "    return avg_acc_per_pruning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, to_convergence=True):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=2,\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def prune_weights(model, pruning_ratio):\n",
    "    weights = model.get_weights()\n",
    "    weights_to_prune = model.get_weights()\n",
    "    for index, weight in enumerate(weights):\n",
    "        if (index == 0) or (index == 2) or (index == 4):\n",
    "            flat_weights = weight.flatten()\n",
    "            flat_weights_to_prune = weights_to_prune[index].flatten()\n",
    "            mask = weights_to_prune[index+1].flatten()\n",
    "            #print (flat_weights_to_prune.shape, flat_weights.shape)\n",
    "            flat_weights_df = pd.DataFrame(flat_weights)\n",
    "            #mask_df = pd.DataFrame(mask)\n",
    "            no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "            #print(no_of_weights_to_prune)\n",
    "            indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "            for idx_to_delete in indices_to_delete:\n",
    "                mask[idx_to_delete] = 0\n",
    "                flat_weights_to_prune[idx_to_delete] = 0\n",
    "            dims = weights_to_prune[index+1].shape\n",
    "            mask_reshaped = mask.reshape(dims)\n",
    "            weights_reshaped = flat_weights_to_prune.reshape(dims)\n",
    "            weights_to_prune[index+1] = mask_reshaped\n",
    "            weights_to_prune[index] = weights_reshaped\n",
    "    \n",
    "    return weights_to_prune\n",
    "\n",
    "\n",
    "\n",
    "def pgd_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x,\n",
    "        y,\n",
    "        epsilons=[15/255]\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/len(y)\n",
    "\n",
    "def cw2_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.L2CarliniWagnerAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x,\n",
    "        y,\n",
    "        epsilons=[.5]\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/len(y)\n",
    "\n",
    "def initialize_base_model(index, save_weights=False):\n",
    "    model = LeNet300_100()\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/base-model-weights-{index}')\n",
    "    return model\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x = tf.convert_to_tensor(x_train[:500].reshape(500,28*28))\n",
    "y = tf.convert_to_tensor([y_train[:500]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, units=32, activation='relu'):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #print(input_shape)\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                name='unpruned_weights')\n",
    "        self.mask = self.add_weight(shape=(self.w.shape),\n",
    "                                    initializer='ones',\n",
    "                                    trainable=False,\n",
    "                                   name='pruning_mask')\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #self.mask_2 = tf.multiply(self.mask, self.mask_2)\n",
    "        x = tf.multiply(self.w, self.mask)\n",
    "        #print(self.pruned_w.eval())\n",
    "        x = tf.matmul(inputs, x)\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return tf.keras.activations.relu(x)\n",
    "        if self.activation == 'softmax':\n",
    "            return tf.keras.activations.softmax(x)\n",
    "        raise ValueError('Activation function not implemented')\n",
    "\n",
    "class LeNet300_100(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet300_100, self).__init__()\n",
    "        self.dense1 = CustomLayer(300)\n",
    "        self.dense2 = CustomLayer(100)\n",
    "        self.dense3 = CustomLayer(10, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = self.dense1(inputs)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "    \n",
    "    def prune_globally(self,ratio):\n",
    "                \n",
    "        shape1 = self.dense1.w.shape\n",
    "        shape2 = self.dense2.w.shape\n",
    "        shape3 = self.dense3.w.shape\n",
    "\n",
    "        flat_weights = np.append(self.dense1.w.numpy().flatten() ,self.dense2.w.numpy().flatten())\n",
    "        flat_weights = np.append(flat_weights ,self.dense3.w.numpy().flatten())\n",
    "        flat_mask = np.append(self.dense1.mask.numpy().flatten(), self.dense2.mask.numpy().flatten())\n",
    "        flat_mask = np.append(flat_mask, self.dense3.mask.numpy().flatten())\n",
    "        \n",
    "        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "        indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "            \n",
    "        w1 = flat_weights[:shape1[0]*shape1[1]].reshape(shape1)\n",
    "        w2 = flat_weights[shape1[0]*shape1[1]:shape1[0]*shape1[1]+shape2[0]*shape2[1]].reshape(shape2)\n",
    "        w3 = flat_weights[-shape3[0]*shape3[1]:].reshape(shape3)\n",
    "        m1 = flat_mask[:shape1[0]*shape1[1]].reshape(shape1)\n",
    "        m2 = flat_mask[shape1[0]*shape1[1]:shape1[0]*shape1[1]+shape2[0]*shape2[1]].reshape(shape2)\n",
    "        m3 = flat_mask[-shape3[0]*shape3[1]:].reshape(shape3)\n",
    "        self.set_weights([w1,m1,w2,m2,w3,m3])\n",
    "        #print(weights)\n",
    "        return\n",
    "    \n",
    "    def prune_locally(self, ratio):\n",
    "        layers = self.get_weights()\n",
    "        for index, weights in enumerate(layers):\n",
    "            if (index == 0) or (index == 2) or (index == 4):\n",
    "                shape = weights.shape\n",
    "                flat_weights = weights.flatten()\n",
    "                mask = layers[index+1].flatten()\n",
    "                \n",
    "                no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    mask[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                \n",
    "                mask_reshaped = mask.reshape(shape)\n",
    "                weights_reshaped = flat_weights.reshape(shape)\n",
    "                layers[index+1] = mask_reshaped\n",
    "                layers[index] = weights_reshaped\n",
    "        self.set_weights(layers)\n",
    "        return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet300_100()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "             )\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )\n",
    "\n",
    "model.save('./saved-models/mini-pipeline-mlp-baseline-model')\n",
    "model.save_weights('./saved-models/weights')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  0,  3,  0,  5,  0,  7,  0,  9,  0, 11,  0, 13,  0, 15,  0,\n",
       "       17,  0, 19,  0, 21,  0, 23,  0, 25,  0, 27,  0, 29,  0, 31,  0, 33,\n",
       "        0, 35,  0, 37,  0, 39,  0, 41,  0, 43,  0, 45,  0, 47,  0, 49,  0,\n",
       "       51,  0, 53,  0, 55,  0, 57,  0, 59,  0, 61,  0, 63,  0, 65,  0, 67,\n",
       "        0, 69,  0, 71,  0, 73,  0, 75,  0, 77,  0, 79,  0, 81,  0, 83,  0,\n",
       "       85,  0, 87,  0, 89,  0, 91,  0, 93,  0, 95,  0, 97,  0, 99])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weights = np.array(list(range(100)));zz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "no_of_weighs_to_prune = rate * len(weights)\n",
    "\n",
    "non_zero_weights = np.nonzero(zz)[0]\n",
    "no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(weights) - len(non_zero_weights)) )\n",
    "\n",
    "random.shuffle(non_zero_weights)\n",
    "indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
