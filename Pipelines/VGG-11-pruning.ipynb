{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.0'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import helperfiles.helpers as helpers\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state.\n",
    "\n",
    "tf.__version__\n",
    "#tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train, ds_test, _ ,_ = helpers.load_data('imagenette')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74/74 [==============================] - 29s 393ms/step - loss: 2.1148 - accuracy: 0.3408\n"
     ]
    }
   ],
   "source": [
    "model = helpers.initialize_base_model('VGG', ds_train, 1 ,experiment_name='test', lr=1e-3, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "helpers.compile_model('VGG', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "74/74 [==============================] - 34s 454ms/step - loss: 2.1376 - accuracy: 0.3176 - val_loss: 2.3165 - val_accuracy: 0.1391 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "74/74 [==============================] - 28s 373ms/step - loss: 2.0788 - accuracy: 0.3743 - val_loss: 2.2792 - val_accuracy: 0.1753 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "53/74 [====================>.........] - ETA: 7s - loss: 2.0510 - accuracy: 0.4067"
     ]
    }
   ],
   "source": [
    "hist = helpers.train_model('VGG', ds_train, ds_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n",
      "74/74 [==============================] - 33s 453ms/step - loss: 2.2135 - accuracy: 0.2376 - val_loss: 2.3610 - val_accuracy: 0.0938 - lr: 0.0010\n",
      "Epoch 2/150\n",
      "74/74 [==============================] - 30s 408ms/step - loss: 2.1699 - accuracy: 0.2833 - val_loss: 2.3616 - val_accuracy: 0.0994 - lr: 0.0010\n",
      "Epoch 3/150\n",
      "74/74 [==============================] - 27s 371ms/step - loss: 2.1412 - accuracy: 0.3126 - val_loss: 2.3232 - val_accuracy: 0.1332 - lr: 0.0010\n",
      "Epoch 4/150\n",
      "74/74 [==============================] - 28s 373ms/step - loss: 2.1237 - accuracy: 0.3297 - val_loss: 2.1615 - val_accuracy: 0.2894 - lr: 0.0010\n",
      "Epoch 5/150\n",
      "74/74 [==============================] - 28s 374ms/step - loss: 2.0754 - accuracy: 0.3813 - val_loss: 2.2028 - val_accuracy: 0.2433 - lr: 0.0010\n",
      "Epoch 6/150\n",
      "74/74 [==============================] - 28s 375ms/step - loss: 2.0686 - accuracy: 0.3871 - val_loss: 2.2264 - val_accuracy: 0.2214 - lr: 0.0010\n",
      "Epoch 7/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 2.0344 - accuracy: 0.4206 - val_loss: 2.1886 - val_accuracy: 0.2647 - lr: 0.0010\n",
      "Epoch 8/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 2.0393 - accuracy: 0.4170 - val_loss: 2.1520 - val_accuracy: 0.3062 - lr: 0.0010\n",
      "Epoch 9/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 2.0129 - accuracy: 0.4453 - val_loss: 2.0500 - val_accuracy: 0.4102 - lr: 0.0010\n",
      "Epoch 10/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 2.0003 - accuracy: 0.4560 - val_loss: 2.0355 - val_accuracy: 0.4224 - lr: 0.0010\n",
      "Epoch 11/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 2.0006 - accuracy: 0.4561 - val_loss: 2.0281 - val_accuracy: 0.4265 - lr: 0.0010\n",
      "Epoch 12/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.9877 - accuracy: 0.4708 - val_loss: 2.1077 - val_accuracy: 0.3414 - lr: 0.0010\n",
      "Epoch 13/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.9755 - accuracy: 0.4820 - val_loss: 2.0392 - val_accuracy: 0.4145 - lr: 0.0010\n",
      "Epoch 14/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.9676 - accuracy: 0.4899 - val_loss: 1.9960 - val_accuracy: 0.4639 - lr: 0.0010\n",
      "Epoch 15/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.9747 - accuracy: 0.4805 - val_loss: 1.9530 - val_accuracy: 0.5047 - lr: 0.0010\n",
      "Epoch 16/150\n",
      "74/74 [==============================] - 28s 381ms/step - loss: 1.9633 - accuracy: 0.4932 - val_loss: 1.9222 - val_accuracy: 0.5371 - lr: 0.0010\n",
      "Epoch 17/150\n",
      "74/74 [==============================] - 29s 385ms/step - loss: 1.9634 - accuracy: 0.4935 - val_loss: 2.0657 - val_accuracy: 0.3896 - lr: 0.0010\n",
      "Epoch 18/150\n",
      "74/74 [==============================] - 28s 379ms/step - loss: 1.9481 - accuracy: 0.5105 - val_loss: 2.0045 - val_accuracy: 0.4555 - lr: 0.0010\n",
      "Epoch 19/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.9345 - accuracy: 0.5248 - val_loss: 1.9009 - val_accuracy: 0.5595 - lr: 0.0010\n",
      "Epoch 20/150\n",
      "74/74 [==============================] - 28s 383ms/step - loss: 1.9290 - accuracy: 0.5275 - val_loss: 2.0731 - val_accuracy: 0.3855 - lr: 0.0010\n",
      "Epoch 21/150\n",
      "74/74 [==============================] - 28s 382ms/step - loss: 1.9181 - accuracy: 0.5396 - val_loss: 1.9510 - val_accuracy: 0.5068 - lr: 0.0010\n",
      "Epoch 22/150\n",
      "74/74 [==============================] - 28s 385ms/step - loss: 1.9217 - accuracy: 0.5365 - val_loss: 1.9508 - val_accuracy: 0.5098 - lr: 0.0010\n",
      "Epoch 23/150\n",
      "74/74 [==============================] - 28s 381ms/step - loss: 1.9041 - accuracy: 0.5517 - val_loss: 1.9483 - val_accuracy: 0.5085 - lr: 0.0010\n",
      "Epoch 24/150\n",
      "74/74 [==============================] - 28s 375ms/step - loss: 1.9147 - accuracy: 0.5423 - val_loss: 2.0319 - val_accuracy: 0.4232 - lr: 0.0010\n",
      "Epoch 25/150\n",
      "74/74 [==============================] - 28s 377ms/step - loss: 1.9164 - accuracy: 0.5430 - val_loss: 2.1178 - val_accuracy: 0.3383 - lr: 0.0010\n",
      "Epoch 26/150\n",
      "74/74 [==============================] - 28s 377ms/step - loss: 1.9162 - accuracy: 0.5423 - val_loss: 1.9742 - val_accuracy: 0.4815 - lr: 0.0010\n",
      "Epoch 27/150\n",
      "74/74 [==============================] - 28s 377ms/step - loss: 1.8978 - accuracy: 0.5614 - val_loss: 1.9330 - val_accuracy: 0.5274 - lr: 0.0010\n",
      "Epoch 28/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8898 - accuracy: 0.5689 - val_loss: 1.9011 - val_accuracy: 0.5590 - lr: 0.0010\n",
      "Epoch 29/150\n",
      "74/74 [==============================] - 28s 377ms/step - loss: 1.8819 - accuracy: 0.5774 - val_loss: 1.8971 - val_accuracy: 0.5636 - lr: 0.0010\n",
      "Epoch 30/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8827 - accuracy: 0.5754 - val_loss: 1.9876 - val_accuracy: 0.4683 - lr: 0.0010\n",
      "Epoch 31/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8867 - accuracy: 0.5745 - val_loss: 1.8920 - val_accuracy: 0.5671 - lr: 0.0010\n",
      "Epoch 32/150\n",
      "74/74 [==============================] - 28s 377ms/step - loss: 1.8698 - accuracy: 0.5908 - val_loss: 1.9438 - val_accuracy: 0.5164 - lr: 0.0010\n",
      "Epoch 33/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8872 - accuracy: 0.5707 - val_loss: 1.9949 - val_accuracy: 0.4619 - lr: 0.0010\n",
      "Epoch 34/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8650 - accuracy: 0.5944 - val_loss: 1.8715 - val_accuracy: 0.5860 - lr: 0.0010\n",
      "Epoch 35/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8606 - accuracy: 0.6013 - val_loss: 1.8540 - val_accuracy: 0.6056 - lr: 0.0010\n",
      "Epoch 36/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8512 - accuracy: 0.6088 - val_loss: 1.8671 - val_accuracy: 0.5959 - lr: 0.0010\n",
      "Epoch 37/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8576 - accuracy: 0.6025 - val_loss: 1.8785 - val_accuracy: 0.5817 - lr: 0.0010\n",
      "Epoch 38/150\n",
      "74/74 [==============================] - 28s 377ms/step - loss: 1.8630 - accuracy: 0.5976 - val_loss: 2.1558 - val_accuracy: 0.3014 - lr: 0.0010\n",
      "Epoch 39/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8540 - accuracy: 0.6035 - val_loss: 1.8471 - val_accuracy: 0.6122 - lr: 0.0010\n",
      "Epoch 40/150\n",
      "74/74 [==============================] - 28s 377ms/step - loss: 1.8478 - accuracy: 0.6119 - val_loss: 1.9351 - val_accuracy: 0.5236 - lr: 0.0010\n",
      "Epoch 41/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8584 - accuracy: 0.6014 - val_loss: 1.9194 - val_accuracy: 0.5373 - lr: 0.0010\n",
      "Epoch 42/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8542 - accuracy: 0.6051 - val_loss: 1.9158 - val_accuracy: 0.5406 - lr: 0.0010\n",
      "Epoch 43/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8413 - accuracy: 0.6176 - val_loss: 1.9632 - val_accuracy: 0.4932 - lr: 0.0010\n",
      "Epoch 44/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8361 - accuracy: 0.6222 - val_loss: 1.8360 - val_accuracy: 0.6252 - lr: 0.0010\n",
      "Epoch 45/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8362 - accuracy: 0.6222 - val_loss: 1.8779 - val_accuracy: 0.5832 - lr: 0.0010\n",
      "Epoch 46/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8381 - accuracy: 0.6206 - val_loss: 1.8880 - val_accuracy: 0.5722 - lr: 0.0010\n",
      "Epoch 47/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8213 - accuracy: 0.6381 - val_loss: 1.8325 - val_accuracy: 0.6283 - lr: 0.0010\n",
      "Epoch 48/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8150 - accuracy: 0.6451 - val_loss: 1.8146 - val_accuracy: 0.6446 - lr: 0.0010\n",
      "Epoch 49/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8333 - accuracy: 0.6257 - val_loss: 1.8735 - val_accuracy: 0.5827 - lr: 0.0010\n",
      "Epoch 50/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8211 - accuracy: 0.6389 - val_loss: 1.8620 - val_accuracy: 0.5952 - lr: 0.0010\n",
      "Epoch 51/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.8165 - accuracy: 0.6427 - val_loss: 1.7933 - val_accuracy: 0.6650 - lr: 0.0010\n",
      "Epoch 52/150\n",
      "74/74 [==============================] - 28s 376ms/step - loss: 1.7991 - accuracy: 0.6625 - val_loss: 1.7831 - val_accuracy: 0.6759 - lr: 0.0010\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.8003 - accuracy: 0.6590 - val_loss: 1.8027 - val_accuracy: 0.6540 - lr: 0.0010\n",
      "Epoch 54/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7926 - accuracy: 0.6672 - val_loss: 1.8400 - val_accuracy: 0.6201 - lr: 0.0010\n",
      "Epoch 55/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.8047 - accuracy: 0.6543 - val_loss: 1.8320 - val_accuracy: 0.6321 - lr: 0.0010\n",
      "Epoch 56/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7901 - accuracy: 0.6698 - val_loss: 1.8122 - val_accuracy: 0.6494 - lr: 0.0010\n",
      "Epoch 57/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7846 - accuracy: 0.6743 - val_loss: 1.8485 - val_accuracy: 0.6117 - lr: 0.0010\n",
      "Epoch 58/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7894 - accuracy: 0.6721 - val_loss: 1.8100 - val_accuracy: 0.6494 - lr: 0.0010\n",
      "Epoch 59/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7984 - accuracy: 0.6635 - val_loss: 1.8209 - val_accuracy: 0.6405 - lr: 0.0010\n",
      "Epoch 60/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7885 - accuracy: 0.6711 - val_loss: 1.7881 - val_accuracy: 0.6693 - lr: 0.0010\n",
      "Epoch 61/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7975 - accuracy: 0.6619 - val_loss: 1.8072 - val_accuracy: 0.6527 - lr: 0.0010\n",
      "Epoch 62/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7938 - accuracy: 0.6644 - val_loss: 1.7999 - val_accuracy: 0.6611 - lr: 0.0010\n",
      "Epoch 63/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7897 - accuracy: 0.6708 - val_loss: 1.8140 - val_accuracy: 0.6425 - lr: 0.0010\n",
      "Epoch 64/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7871 - accuracy: 0.6740 - val_loss: 1.8107 - val_accuracy: 0.6489 - lr: 0.0010\n",
      "Epoch 65/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7624 - accuracy: 0.6968 - val_loss: 1.7498 - val_accuracy: 0.7121 - lr: 3.0000e-04\n",
      "Epoch 66/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7415 - accuracy: 0.7186 - val_loss: 1.7362 - val_accuracy: 0.7251 - lr: 3.0000e-04\n",
      "Epoch 67/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7375 - accuracy: 0.7219 - val_loss: 1.7391 - val_accuracy: 0.7215 - lr: 3.0000e-04\n",
      "Epoch 68/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7350 - accuracy: 0.7267 - val_loss: 1.7308 - val_accuracy: 0.7307 - lr: 3.0000e-04\n",
      "Epoch 69/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7283 - accuracy: 0.7325 - val_loss: 1.7273 - val_accuracy: 0.7350 - lr: 3.0000e-04\n",
      "Epoch 70/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7191 - accuracy: 0.7418 - val_loss: 1.7391 - val_accuracy: 0.7197 - lr: 3.0000e-04\n",
      "Epoch 71/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7247 - accuracy: 0.7356 - val_loss: 1.7217 - val_accuracy: 0.7381 - lr: 3.0000e-04\n",
      "Epoch 72/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7164 - accuracy: 0.7449 - val_loss: 1.7214 - val_accuracy: 0.7396 - lr: 3.0000e-04\n",
      "Epoch 73/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7143 - accuracy: 0.7468 - val_loss: 1.7367 - val_accuracy: 0.7223 - lr: 3.0000e-04\n",
      "Epoch 74/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7133 - accuracy: 0.7489 - val_loss: 1.7186 - val_accuracy: 0.7422 - lr: 3.0000e-04\n",
      "Epoch 75/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7176 - accuracy: 0.7442 - val_loss: 1.7285 - val_accuracy: 0.7350 - lr: 3.0000e-04\n",
      "Epoch 76/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7096 - accuracy: 0.7521 - val_loss: 1.7236 - val_accuracy: 0.7373 - lr: 3.0000e-04\n",
      "Epoch 77/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7039 - accuracy: 0.7581 - val_loss: 1.7257 - val_accuracy: 0.7345 - lr: 3.0000e-04\n",
      "Epoch 78/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7135 - accuracy: 0.7465 - val_loss: 1.7317 - val_accuracy: 0.7307 - lr: 3.0000e-04\n",
      "Epoch 79/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7084 - accuracy: 0.7519 - val_loss: 1.7181 - val_accuracy: 0.7434 - lr: 3.0000e-04\n",
      "Epoch 80/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.7032 - accuracy: 0.7596 - val_loss: 1.7139 - val_accuracy: 0.7457 - lr: 3.0000e-04\n",
      "Epoch 81/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7012 - accuracy: 0.7611 - val_loss: 1.7298 - val_accuracy: 0.7294 - lr: 3.0000e-04\n",
      "Epoch 82/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7062 - accuracy: 0.7563 - val_loss: 1.7143 - val_accuracy: 0.7475 - lr: 3.0000e-04\n",
      "Epoch 83/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6991 - accuracy: 0.7617 - val_loss: 1.7111 - val_accuracy: 0.7485 - lr: 3.0000e-04\n",
      "Epoch 84/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7017 - accuracy: 0.7582 - val_loss: 1.7144 - val_accuracy: 0.7452 - lr: 3.0000e-04\n",
      "Epoch 85/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.7005 - accuracy: 0.7624 - val_loss: 1.7297 - val_accuracy: 0.7320 - lr: 3.0000e-04\n",
      "Epoch 86/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6984 - accuracy: 0.7642 - val_loss: 1.7092 - val_accuracy: 0.7526 - lr: 3.0000e-04\n",
      "Epoch 87/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6986 - accuracy: 0.7632 - val_loss: 1.7173 - val_accuracy: 0.7429 - lr: 3.0000e-04\n",
      "Epoch 88/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6952 - accuracy: 0.7643 - val_loss: 1.7119 - val_accuracy: 0.7462 - lr: 3.0000e-04\n",
      "Epoch 89/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6992 - accuracy: 0.7619 - val_loss: 1.7256 - val_accuracy: 0.7371 - lr: 3.0000e-04\n",
      "Epoch 90/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6985 - accuracy: 0.7620 - val_loss: 1.7094 - val_accuracy: 0.7506 - lr: 3.0000e-04\n",
      "Epoch 91/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6946 - accuracy: 0.7671 - val_loss: 1.7203 - val_accuracy: 0.7406 - lr: 3.0000e-04\n",
      "Epoch 92/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6898 - accuracy: 0.7716 - val_loss: 1.7048 - val_accuracy: 0.7577 - lr: 3.0000e-04\n",
      "Epoch 93/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6909 - accuracy: 0.7705 - val_loss: 1.7211 - val_accuracy: 0.7399 - lr: 3.0000e-04\n",
      "Epoch 94/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6925 - accuracy: 0.7696 - val_loss: 1.7183 - val_accuracy: 0.7445 - lr: 3.0000e-04\n",
      "Epoch 95/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6857 - accuracy: 0.7758 - val_loss: 1.7048 - val_accuracy: 0.7554 - lr: 3.0000e-04\n",
      "Epoch 96/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6880 - accuracy: 0.7738 - val_loss: 1.7112 - val_accuracy: 0.7508 - lr: 3.0000e-04\n",
      "Epoch 97/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6874 - accuracy: 0.7735 - val_loss: 1.7155 - val_accuracy: 0.7455 - lr: 3.0000e-04\n",
      "Epoch 98/150\n",
      "74/74 [==============================] - 27s 369ms/step - loss: 1.6883 - accuracy: 0.7742 - val_loss: 1.7036 - val_accuracy: 0.7572 - lr: 3.0000e-04\n",
      "Epoch 99/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6885 - accuracy: 0.7726 - val_loss: 1.7373 - val_accuracy: 0.7225 - lr: 3.0000e-04\n",
      "Epoch 100/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6832 - accuracy: 0.7781 - val_loss: 1.7064 - val_accuracy: 0.7554 - lr: 3.0000e-04\n",
      "Epoch 101/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6760 - accuracy: 0.7853 - val_loss: 1.7034 - val_accuracy: 0.7600 - lr: 3.0000e-04\n",
      "Epoch 102/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6787 - accuracy: 0.7842 - val_loss: 1.6983 - val_accuracy: 0.7623 - lr: 3.0000e-04\n",
      "Epoch 103/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6758 - accuracy: 0.7859 - val_loss: 1.7012 - val_accuracy: 0.7587 - lr: 3.0000e-04\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 104/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6741 - accuracy: 0.7868 - val_loss: 1.6970 - val_accuracy: 0.7628 - lr: 3.0000e-04\n",
      "Epoch 105/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6829 - accuracy: 0.7788 - val_loss: 1.7049 - val_accuracy: 0.7562 - lr: 3.0000e-04\n",
      "Epoch 106/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6824 - accuracy: 0.7793 - val_loss: 1.7023 - val_accuracy: 0.7587 - lr: 3.0000e-04\n",
      "Epoch 107/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6749 - accuracy: 0.7870 - val_loss: 1.6956 - val_accuracy: 0.7664 - lr: 3.0000e-04\n",
      "Epoch 108/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6765 - accuracy: 0.7853 - val_loss: 1.6985 - val_accuracy: 0.7648 - lr: 3.0000e-04\n",
      "Epoch 109/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6796 - accuracy: 0.7807 - val_loss: 1.7081 - val_accuracy: 0.7516 - lr: 3.0000e-04\n",
      "Epoch 110/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6742 - accuracy: 0.7892 - val_loss: 1.7059 - val_accuracy: 0.7569 - lr: 3.0000e-04\n",
      "Epoch 111/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6785 - accuracy: 0.7835 - val_loss: 1.7078 - val_accuracy: 0.7518 - lr: 3.0000e-04\n",
      "Epoch 112/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6798 - accuracy: 0.7810 - val_loss: 1.7013 - val_accuracy: 0.7580 - lr: 3.0000e-04\n",
      "Epoch 113/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6735 - accuracy: 0.7883 - val_loss: 1.6895 - val_accuracy: 0.7717 - lr: 3.0000e-04\n",
      "Epoch 114/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6678 - accuracy: 0.7945 - val_loss: 1.6904 - val_accuracy: 0.7707 - lr: 3.0000e-04\n",
      "Epoch 115/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6703 - accuracy: 0.7921 - val_loss: 1.7229 - val_accuracy: 0.7371 - lr: 3.0000e-04\n",
      "Epoch 116/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6750 - accuracy: 0.7883 - val_loss: 1.6868 - val_accuracy: 0.7735 - lr: 3.0000e-04\n",
      "Epoch 117/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6726 - accuracy: 0.7901 - val_loss: 1.7014 - val_accuracy: 0.7590 - lr: 3.0000e-04\n",
      "Epoch 118/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6657 - accuracy: 0.7968 - val_loss: 1.6902 - val_accuracy: 0.7697 - lr: 3.0000e-04\n",
      "Epoch 119/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6692 - accuracy: 0.7941 - val_loss: 1.6972 - val_accuracy: 0.7661 - lr: 3.0000e-04\n",
      "Epoch 120/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6670 - accuracy: 0.7958 - val_loss: 1.6980 - val_accuracy: 0.7628 - lr: 3.0000e-04\n",
      "Epoch 121/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6699 - accuracy: 0.7902 - val_loss: 1.7016 - val_accuracy: 0.7623 - lr: 3.0000e-04\n",
      "Epoch 122/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6696 - accuracy: 0.7917 - val_loss: 1.7019 - val_accuracy: 0.7595 - lr: 3.0000e-04\n",
      "Epoch 123/150\n",
      "74/74 [==============================] - 27s 370ms/step - loss: 1.6668 - accuracy: 0.7933 - val_loss: 1.6961 - val_accuracy: 0.7664 - lr: 3.0000e-04\n",
      "Epoch 124/150\n",
      "74/74 [==============================] - 28s 382ms/step - loss: 1.6675 - accuracy: 0.7941 - val_loss: 1.6947 - val_accuracy: 0.7631 - lr: 3.0000e-04\n",
      "Epoch 125/150\n",
      "74/74 [==============================] - 29s 387ms/step - loss: 1.6596 - accuracy: 0.8030 - val_loss: 1.6976 - val_accuracy: 0.7628 - lr: 3.0000e-04\n",
      "Epoch 126/150\n"
     ]
    }
   ],
   "source": [
    "hist = helpers.train_model('VGG', ds_train, ds_test, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 57\n",
      "1 / 57\n",
      "2 / 57\n",
      "3 / 57\n",
      "4 / 57\n",
      "5 / 57\n",
      "6 / 57\n",
      "7 / 57\n",
      "8 / 57\n",
      "9 / 57\n",
      "10 / 57\n",
      "11 / 57\n",
      "12 / 57\n",
      "13 / 57\n",
      "14 / 57\n",
      "15 / 57\n",
      "16 / 57\n",
      "17 / 57\n",
      "18 / 57\n",
      "19 / 57\n",
      "20 / 57\n",
      "21 / 57\n",
      "22 / 57\n",
      "23 / 57\n",
      "24 / 57\n",
      "25 / 57\n",
      "26 / 57\n",
      "27 / 57\n",
      "28 / 57\n",
      "29 / 57\n",
      "30 / 57\n",
      "31 / 57\n",
      "32 / 57\n",
      "33 / 57\n",
      "34 / 57\n",
      "35 / 57\n",
      "36 / 57\n",
      "37 / 57\n",
      "38 / 57\n",
      "39 / 57\n",
      "40 / 57\n",
      "41 / 57\n",
      "42 / 57\n",
      "43 / 57\n",
      "44 / 57\n",
      "45 / 57\n",
      "46 / 57\n",
      "47 / 57\n",
      "48 / 57\n",
      "49 / 57\n",
      "50 / 57\n",
      "51 / 57\n",
      "52 / 57\n",
      "53 / 57\n",
      "54 / 57\n",
      "55 / 57\n",
      "56 / 57\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.find_layers_and_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([48, 51, 54])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([50, 53, 56])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([0, 6, 12, 18, 24, 30, 36, 42])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ListWrapper([1, 7, 13, 19, 25, 31, 37, 43])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.conv_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-d61b12e5c400>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mhelpers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_zeros_ratio\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/dev/network-pruning/Pipelines/helperfiles/helpers.py\u001b[0m in \u001b[0;36mget_zeros_ratio\u001b[0;34m(model, layers_to_examine)\u001b[0m\n\u001b[1;32m    345\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlayers_to_examine\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    346\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 347\u001b[0;31m         \u001b[0mall_weights\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweights\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    348\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcount_nonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_weights\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "helpers.get_zeros_ratio(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prune_magnitude_global_unstruct(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "helpers.get_zeros_ratio(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import helperfiles.vgg11 as vgg11\n",
    "model = vgg11.VGG11()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_model(architecture, model, lr=1e-3):\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "        optimizer=tf.keras.optimizers.Adam(lr),\n",
    "        metrics=['accuracy'],\n",
    "        experimental_run_tf_function=True\n",
    "    )\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compile_model('VGG', model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(),\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=True\n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Adam/learning_rate:0' shape=() dtype=float32, numpy=0.001>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.optimizer.learning_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3, 3, 3, 64)\n",
      "1 (3, 3, 3, 64)\n",
      "2 (64,)\n",
      "3 (64,)\n",
      "4 (64,)\n",
      "5 (64,)\n",
      "6 (3, 3, 64, 128)\n",
      "7 (3, 3, 64, 128)\n",
      "8 (128,)\n",
      "9 (128,)\n",
      "10 (128,)\n",
      "11 (128,)\n",
      "12 (3, 3, 128, 256)\n",
      "13 (3, 3, 128, 256)\n",
      "14 (256,)\n",
      "15 (256,)\n",
      "16 (256,)\n",
      "17 (256,)\n",
      "18 (3, 3, 256, 256)\n",
      "19 (3, 3, 256, 256)\n",
      "20 (256,)\n",
      "21 (256,)\n",
      "22 (256,)\n",
      "23 (256,)\n",
      "24 (3, 3, 256, 512)\n",
      "25 (3, 3, 256, 512)\n",
      "26 (512,)\n",
      "27 (512,)\n",
      "28 (512,)\n",
      "29 (512,)\n",
      "30 (3, 3, 512, 512)\n",
      "31 (3, 3, 512, 512)\n",
      "32 (512,)\n",
      "33 (512,)\n",
      "34 (512,)\n",
      "35 (512,)\n",
      "36 (3, 3, 512, 512)\n",
      "37 (3, 3, 512, 512)\n",
      "38 (512,)\n",
      "39 (512,)\n",
      "40 (512,)\n",
      "41 (512,)\n",
      "42 (3, 3, 512, 512)\n",
      "43 (3, 3, 512, 512)\n",
      "44 (25088, 4096)\n",
      "45 (4096,)\n",
      "46 (25088, 4096)\n",
      "47 (4096, 1024)\n",
      "48 (1024,)\n",
      "49 (4096, 1024)\n",
      "50 (1024, 10)\n",
      "51 (10,)\n",
      "52 (1024, 10)\n"
     ]
    }
   ],
   "source": [
    "ww = model.get_weights()\n",
    "for i,w in enumerate(ww):\n",
    "    print(i, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 (3, 3, 3, 64)\n",
      "1 (3, 3, 3, 64)\n",
      "2 (64,)\n",
      "3 (64,)\n",
      "4 (64,)\n",
      "5 (64,)\n",
      "6 (3, 3, 64, 128)\n",
      "7 (3, 3, 64, 128)\n",
      "8 (128,)\n",
      "9 (128,)\n",
      "10 (128,)\n",
      "11 (128,)\n",
      "12 (3, 3, 128, 256)\n",
      "13 (3, 3, 128, 256)\n",
      "14 (256,)\n",
      "15 (256,)\n",
      "16 (256,)\n",
      "17 (256,)\n",
      "18 (3, 3, 256, 256)\n",
      "19 (3, 3, 256, 256)\n",
      "20 (256,)\n",
      "21 (256,)\n",
      "22 (256,)\n",
      "23 (256,)\n",
      "24 (3, 3, 256, 512)\n",
      "25 (3, 3, 256, 512)\n",
      "26 (512,)\n",
      "27 (512,)\n",
      "28 (512,)\n",
      "29 (512,)\n",
      "30 (3, 3, 512, 512)\n",
      "31 (3, 3, 512, 512)\n",
      "32 (512,)\n",
      "33 (512,)\n",
      "34 (512,)\n",
      "35 (512,)\n",
      "36 (3, 3, 512, 512)\n",
      "37 (3, 3, 512, 512)\n",
      "38 (512,)\n",
      "39 (512,)\n",
      "40 (512,)\n",
      "41 (512,)\n",
      "42 (3, 3, 512, 512)\n",
      "43 (3, 3, 512, 512)\n",
      "44 (25088,)\n",
      "45 (25088,)\n",
      "46 (25088,)\n",
      "47 (25088,)\n",
      "48 (25088, 4096)\n",
      "49 (25088, 4096)\n",
      "50 (4096,)\n",
      "51 (4096,)\n",
      "52 (4096,)\n",
      "53 (4096,)\n",
      "54 (4096, 1024)\n",
      "55 (4096, 1024)\n",
      "56 (1024,)\n",
      "57 (1024,)\n",
      "58 (1024,)\n",
      "59 (1024,)\n",
      "60 (1024, 10)\n",
      "61 (10,)\n",
      "62 (1024, 10)\n"
     ]
    }
   ],
   "source": [
    "ww = model.get_weights()\n",
    "for i,w in enumerate(ww):\n",
    "    print(i, w.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 / 63\n",
      "1 / 63\n",
      "2 / 63\n",
      "3 / 63\n",
      "4 / 63\n",
      "5 / 63\n",
      "6 / 63\n",
      "7 / 63\n",
      "8 / 63\n",
      "9 / 63\n",
      "10 / 63\n",
      "11 / 63\n",
      "12 / 63\n",
      "13 / 63\n",
      "14 / 63\n",
      "15 / 63\n",
      "16 / 63\n",
      "17 / 63\n",
      "18 / 63\n",
      "19 / 63\n",
      "20 / 63\n",
      "21 / 63\n",
      "22 / 63\n",
      "23 / 63\n",
      "24 / 63\n",
      "25 / 63\n",
      "26 / 63\n",
      "27 / 63\n",
      "28 / 63\n",
      "29 / 63\n",
      "30 / 63\n",
      "31 / 63\n",
      "32 / 63\n",
      "33 / 63\n",
      "34 / 63\n",
      "35 / 63\n",
      "36 / 63\n",
      "37 / 63\n",
      "38 / 63\n",
      "39 / 63\n",
      "40 / 63\n",
      "41 / 63\n",
      "42 / 63\n",
      "43 / 63\n",
      "44 / 63\n",
      "45 / 63\n",
      "46 / 63\n",
      "47 / 63\n",
      "48 / 63\n",
      "49 / 63\n",
      "50 / 63\n",
      "51 / 63\n",
      "52 / 63\n",
      "53 / 63\n",
      "54 / 63\n",
      "55 / 63\n",
      "56 / 63\n",
      "57 / 63\n",
      "58 / 63\n",
      "59 / 63\n",
      "60 / 63\n",
      "61 / 63\n",
      "62 / 63\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.dense_layers=[]\n",
    "model.dense_masks=[]\n",
    "model.conv_layers=[]\n",
    "model.conv_masks=[]\n",
    "model.find_layers_and_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Virtual devices cannot be modified after being initialized\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4.5)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = {\n",
    "\n",
    "    'conv_1': (3, 3, 3, 64),\n",
    "    'conv_2': (3, 3, 64, 128),\n",
    "    'conv_3': (3, 3, 128, 256),\n",
    "    'conv_4': (3, 3, 256, 256),\n",
    "    'conv_5': (3, 3, 256, 512),\n",
    "    'conv_6': (3, 3, 512, 512),\n",
    "    'conv_7': (3, 3, 512, 512),\n",
    "    'conv_8': (3, 3, 512, 512),\n",
    "    'dense_1': (7*7*512, 4096),\n",
    "    'dense_2': (4096, 1024),\n",
    "    'dense_3': (1024, 10),\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, stride, padding='SAME'):\n",
    "        \n",
    "        #super(CustomConvLayer, self).__init__()\n",
    "        #self.w = weights\n",
    "        #self.m = mask\n",
    "        #self.b = biases\n",
    "        #self.s = strides\n",
    "        #self.p = padding\n",
    "        #self.bn = layers.BatchNormalization()\n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape=shape[-1],\n",
    "            initializer='zeros',\n",
    "            trainable=True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.s = stride\n",
    "        self.p = padding\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        #x = self.bn(x)\n",
    "        return tf.nn.relu(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='SAME'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.max_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p)\n",
    "    \n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'relu'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'relu':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG11(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], True, 1,)\n",
    "        self.maxpool1 = CustomPoolLayer(k=2)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], True, 1,)\n",
    "        self.maxpool2 = CustomPoolLayer(k=2)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv3 = CustomConvLayer(shapes['conv_3'], True, 1,)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.conv4 = CustomConvLayer(shapes['conv_4'], True, 1,)\n",
    "        self.maxpool3 = CustomPoolLayer(k=2)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.conv5 = CustomConvLayer(shapes['conv_5'], True, 1,)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.conv6 = CustomConvLayer(shapes['conv_6'], True, 1,)\n",
    "        self.maxpool4 = CustomPoolLayer(k=2)\n",
    "        self.bn6 = layers.BatchNormalization()\n",
    "        self.conv7 = CustomConvLayer(shapes['conv_7'], True, 1,)\n",
    "        self.bn7 = layers.BatchNormalization()\n",
    "        self.conv8 = CustomConvLayer(shapes['conv_8'], True, 1,)\n",
    "        self.maxpool5 = CustomPoolLayer(k=2)\n",
    "        self.bn8 = layers.BatchNormalization()\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], True, 'relu')\n",
    "        self.bn9 = layers.BatchNormalization()\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], True, 'relu')\n",
    "        self.bn10 = layers.BatchNormalization()\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, 'softmax')\n",
    "        self.conv_layers = []\n",
    "        self.conv_masks = []\n",
    "        self.dense_layers = []\n",
    "        self.dense_masks = []\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        #x = tf.reshape(inputs, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.maxpool5(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.bn8(x)\n",
    "        x =  self.dense1(x)\n",
    "        x = self.bn9(x)\n",
    "        x =  self.dense2(x)\n",
    "        x = self.bn10(x)\n",
    "        x =  self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer in enumerate(self.conv_layers):\n",
    "                #shape = 3,3,64,128\n",
    "                converted_weights = helpers.convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[i]]).numpy()\n",
    "                #shape = 128,64, 3,3\n",
    "                layer_shape = layer.shape\n",
    "                flat_weights = layer.flatten()\n",
    "                flat_masks = converted_mask.flatten()\n",
    "                no_of_weighs_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                non_zero_weights = np.nonzero(flat_weights)[0]\n",
    "                no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                random.shuffle(non_zero_weights)\n",
    "                indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                converted_mask = flat_masks.reshape(layer_shape)\n",
    "                converted_weights = flat_weights.reshape(layer_shape)\n",
    "                back_converted_mask = helpers.convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = helpers.convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[self.conv_masks[i]] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "#            for index, weight in enumerate(weights):\n",
    "            for i, layer in enumerate(self.dense_layers):\n",
    "#                if index in dense_layer_to_prune:\n",
    "                    shape = weights[layer].shape\n",
    "                    flat_weights = weights[layer].flatten()\n",
    "                    flat_mask = weights[self.dense_masks[i]].flatten()\n",
    "                    no_of_weighs_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[self.dense_masks[i]] = mask_reshaped\n",
    "                    weights[layer] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        weights = prune_conv_layers_locally(self, ratio)\n",
    "        weights = prune_dense_layers_locally(self,ratio)\n",
    "        return True\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        all_masks = self.conv_masks + self.dense_masks\n",
    "        for i, x in enumerate(self.conv_layers + self.dense_layers):\n",
    "            flat_weights = np.append(flat_weights, weights[x].flatten())\n",
    "            flat_mask = np.append(flat_mask, weights[all_masks[i]].flatten())\n",
    "            \n",
    "        no_of_weights_to_prune = int(np.round(len(flat_weights)*ratio))\n",
    "        #print('total weights',len(flat_weights))\n",
    "        #print('weights to prune w/o round',int(len(flat_weights)*ratio))\n",
    "        #print('weights to prune with round',int(np.round(len(flat_weights)*ratio)))\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for i, x in enumerate(self.conv_layers + self.dense_layers):\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[all_masks[i]] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def prune_random_local_struct(self, ratio):\n",
    "        def prune_conv_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer in enumerate(self.conv_layers):\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = helpers.convert_from_hwio_to_iohw(weights[layer])\n",
    "                iohw_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[i]])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                #print(channels)\n",
    "                non_zero_channels = np.nonzero([np.sum(channel) for channel in channels])[0]\n",
    "                #print(non_zero_channels)\n",
    "                no_of_channels_to_prune_left = no_of_channels_to_prune - (len(channels) - len(non_zero_channels))\n",
    "                random.shuffle(non_zero_channels)\n",
    "                channels_to_prune = non_zero_channels[:no_of_channels_to_prune_left]\n",
    "                mask = tf.reshape(iohw_mask, \n",
    "                                  (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[layer] = helpers.convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[self.conv_masks[i]] = helpers.convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        def prune_dense_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(np.round(ratio * len(weights[layer_to_prune])))\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                random.shuffle(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[self.dense_masks[i]][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers(self, ratio)\n",
    "        prune_dense_layers(self, ratio)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    \n",
    "    def prune_magnitude_local_struct(self, ratio):\n",
    "        def prune_conv_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, x in enumerate(self.conv_layers):\n",
    "                vals = []\n",
    "                iohw_weights = helpers.convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[i]])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                \n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                for channel in channels:\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(channel)))\n",
    "                channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = helpers.convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[self.conv_masks[i]] = helpers.convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        def prune_dense_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                no_of_rows_to_prune = int(np.round(ratio * len(weights[layer_to_prune])))\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[self.dense_masks[i]][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        weights = prune_conv_layers(self, ratio)\n",
    "        weights = prune_dense_layers(self, ratio)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio):\n",
    "        def prune_conv_layers(self, ratio):\n",
    "            all_channels = []\n",
    "            all_masks = []\n",
    "            vals = []\n",
    "            for layer_to_prune in self.conv_layers:\n",
    "                # convert from e.g. (3,3,1,6) to (1,6,3,3)\n",
    "                iohw_weights = helpers.convert_from_hwio_to_iohw(weights[layer_to_prune])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = helpers.converted_shape[0]*converted_shape[1]\n",
    "                #convert from (1,6,3,3) to (6,3,3)\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                mask = np.ones((no_of_channels,converted_shape[2],converted_shape[3]))\n",
    "                #calculate average magnitude for each filter\n",
    "                vals = vals + [np.sum(np.abs(channel)) / np.prod(channel.shape) for channel in channels]\n",
    "                #vals = vals + [np.sum(np.abs(channel)) for channel in channels]\n",
    "                all_channels = list(all_channels) +  list(channels)\n",
    "                all_masks = list(all_masks) + list(mask)\n",
    "            #vals = [np.sum(np.abs(channel)) for channel in all_channels]\n",
    "            no_of_channels_to_prune = int(np.round(ratio * len(vals)))\n",
    "            #print('lenght of vals',len(vals))\n",
    "            #print('number of all channels',no_of_channels)\n",
    "            #print('channels',no_of_channels_to_prune)\n",
    "            channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "            \n",
    "            for channel_to_prune in channels_to_prune:\n",
    "                #print(all_channels[channel_to_prune].shape)\n",
    "                all_channels[channel_to_prune] = tf.zeros(all_channels[channel_to_prune].shape) \n",
    "                all_masks[channel_to_prune] = tf.zeros(all_channels[channel_to_prune].shape) \n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(self.conv_layers):\n",
    "                original_shape = helpers.convert_from_hwio_to_iohw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_channels[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                pruned_mask = tf.reshape(all_masks[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                weights[layer_to_prune] = helpers.convert_from_iohw_to_hwio(pruned_layer)\n",
    "                weights[self.conv_masks[i]] = helpers.convert_from_iohw_to_hwio(pruned_mask)\n",
    "                z = original_shape[0]*original_shape[1]\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers(self, ratio):\n",
    "            vals = []\n",
    "            lengths = []\n",
    "            for layer_to_prune in self.dense_layers:\n",
    "                #print('dense',layer_to_prune)\n",
    "                lengths.append(weights[layer_to_prune].shape[0])\n",
    "                vals = vals + [np.sum(np.abs(row)) / len(row) for row in weights[layer_to_prune]]\n",
    "                #vals = vals + [np.sum(np.abs(row)) for row in weights[layer_to_prune]]\n",
    "            no_of_rows_to_prune = int(np.round(ratio * len(vals)))\n",
    "            #print('rows', no_of_rows_to_prune)\n",
    "            rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    if row_to_prune in range(int(np.sum(lengths[:i])), int(np.sum(lengths[:i+1]))):\n",
    "                        weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)\n",
    "                        weights[self.dense_masks[i]][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)                \n",
    "            self.set_weights(weights)        \n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        weights = prune_conv_layers(self, ratio)\n",
    "        weights = prune_dense_layers(self, ratio)\n",
    "        #self.set_weights(weights)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "\n",
    "            weights = self.get_weights()\n",
    "\n",
    "            for layer_index, layer in enumerate(self.conv_layers):\n",
    "                #shape = 3,3,64,128\n",
    "                converted_weights = helpers.convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[i]]).numpy()\n",
    "                #shape = 128,64, 3,3\n",
    "                layer_shape = layer.shape\n",
    "                flat_weights = layer.flatten()\n",
    "                flat_masks = converted_mask.flatten()\n",
    "                no_of_weighs_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                converted_mask = flat_masks.reshape(layer_shape)\n",
    "                converted_weights = flat_weights.reshape(layer_shape)\n",
    "                back_converted_mask = helpers.convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = helpers.convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[mask] = back_converted_mask\n",
    "\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for index, layer in enumerate(self.dense_layers):\n",
    "                shape = weights[layer].shape\n",
    "                flat_weights = weights[layer].flatten()\n",
    "                flat_mask = weights[self.dense_masks[index]].flatten()\n",
    "\n",
    "                no_of_weights_to_prune = int(np.round(len(flat_weights)*ratio))\n",
    "                indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_mask[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                mask_reshaped = flat_mask.reshape(shape)\n",
    "                weights_reshaped = flat_weights.reshape(shape)\n",
    "                weights[self.dense_masks[index]] = mask_reshaped\n",
    "                weights[layer] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        weights = prune_conv_layers_locally(self,ratio)\n",
    "        weights = prune_dense_layers_locally(self,ratio)\n",
    "        return True\n",
    "    \n",
    "    def find_layers_and_masks(self):\n",
    "        if len(self.conv_layers) != 0:\n",
    "            return True\n",
    "        for i, w in enumerate(self.get_weights()):\n",
    "            print(i ,'/', len(self.get_weights()))\n",
    "            if len(w.shape) == 4 and w.shape[0] != 1: \n",
    "                if np.all([x == 0 or x == 1 for x in w.flatten()[:100]]) == False: \n",
    "                    self.conv_layers.append(i)\n",
    "                else:\n",
    "                    self.conv_masks.append(i)\n",
    "            if len(w.shape) == 2: \n",
    "                if np.all([x == 0 or x == 1 for x in w.flatten()[:100]]) == False: \n",
    "                    self.dense_layers.append(i)\n",
    "                else:\n",
    "                    self.dense_masks.append(i)\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prune_magnitude_local_unstruct(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.find_layers_and_masks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.dense_layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "from secrets import randbelow\n",
    "import foolbox as fb\n",
    "from datetime import datetime\n",
    "\n",
    "import helperfiles.helpers as helpers\n",
    "\n",
    "shapes = {\n",
    "\n",
    "    'conv_1': (3, 3, 3, 64),\n",
    "    'conv_2': (3, 3, 64, 128),\n",
    "    'conv_3': (3, 3, 128, 256),\n",
    "    'conv_4': (3, 3, 256, 256),\n",
    "    'conv_5': (3, 3, 256, 512),\n",
    "    'conv_6': (3, 3, 512, 512),\n",
    "    'conv_7': (3, 3, 512, 512),\n",
    "    'conv_8': (3, 3, 512, 512),\n",
    "    'dense_1': (7*7*512, 4096),\n",
    "    'dense_2': (4096, 1024),\n",
    "    'dense_3': (1024, 10),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, stride, padding='SAME'):\n",
    "        \n",
    "        #super(CustomConvLayer, self).__init__()\n",
    "        #self.w = weights\n",
    "        #self.m = mask\n",
    "        #self.b = biases\n",
    "        #self.s = strides\n",
    "        #self.p = padding\n",
    "        #self.bn = layers.BatchNormalization()\n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        if self.bias==True:\n",
    "            self.b = self.add_weight(\n",
    "                shape=shape[-1],\n",
    "                initializer='zeros',\n",
    "                trainable=True,\n",
    "                name='b'\n",
    "            )\n",
    "        self.s = stride\n",
    "        self.p = padding\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)\n",
    "        if self.bias == True:\n",
    "            x = tf.nn.bias_add(x, self.b)\n",
    "        #x = self.bn(x)\n",
    "        return tf.nn.relu(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='SAME'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.max_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p)\n",
    "    \n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'relu'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        if self.bias == True:\n",
    "            self.b = self.add_weight(\n",
    "                shape = (shape[-1]),\n",
    "                initializer = 'zeros',\n",
    "                trainable = True,\n",
    "                name='b'\n",
    "            )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        if self.bias == True:\n",
    "            x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'relu':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        \n",
    "class VGG11(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], False, 1,)\n",
    "        self.maxpool1 = CustomPoolLayer(k=2)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], False, 1,)\n",
    "        self.maxpool2 = CustomPoolLayer(k=2)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv3 = CustomConvLayer(shapes['conv_3'], False, 1,)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.conv4 = CustomConvLayer(shapes['conv_4'], False, 1,)\n",
    "        self.maxpool3 = CustomPoolLayer(k=2)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.conv5 = CustomConvLayer(shapes['conv_5'], False, 1,)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.conv6 = CustomConvLayer(shapes['conv_6'], False, 1,)\n",
    "        self.maxpool4 = CustomPoolLayer(k=2)\n",
    "        self.bn6 = layers.BatchNormalization()\n",
    "        self.conv7 = CustomConvLayer(shapes['conv_7'], False, 1,)\n",
    "        self.bn7 = layers.BatchNormalization()\n",
    "        self.conv8 = CustomConvLayer(shapes['conv_8'], False, 1,)\n",
    "        self.maxpool5 = CustomPoolLayer(k=2)\n",
    "        self.bn8 = layers.BatchNormalization()\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], False, 'relu')\n",
    "        self.bn9 = layers.BatchNormalization()\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], False, 'relu')\n",
    "        self.bn10 = layers.BatchNormalization()\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, 'softmax')\n",
    "        #self.conv_layers = [0, 6, 12, 18, 24, 30, 36, 42]\n",
    "        #self.conv_masks = [1, 7, 13, 19, 25, 31, 37, 43]\n",
    "        #self.dense_layers = [48, 54, 60]\n",
    "        #self.dense_masks = [59, 55, 61]\n",
    "        self.conv_layers = []\n",
    "        self.conv_masks = []\n",
    "        self.dense_layers = []\n",
    "        self.dense_masks = []\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        #x = tf.reshape(inputs, shape=[-1, 28, 28, 1])\n",
    "\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.maxpool5(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.bn8(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.bn9(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.bn10(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer in enumerate(self.conv_layers):\n",
    "                #shape = 3,3,64,128\n",
    "                converted_weights = helpers.convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[i]]).numpy()\n",
    "                #shape = 128,64, 3,3\n",
    "                layer_shape = weights[layer].shape\n",
    "                flat_weights = weights[layer].flatten()\n",
    "                flat_masks = converted_mask.flatten()\n",
    "                no_of_weighs_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                non_zero_weights = np.nonzero(flat_weights)[0]\n",
    "                no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                random.shuffle(non_zero_weights)\n",
    "                indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                converted_mask = flat_masks.reshape(layer_shape)\n",
    "                converted_weights = flat_weights.reshape(layer_shape)\n",
    "                back_converted_mask = helpers.convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = helpers.convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[self.conv_masks[i]] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "#            for index, weight in enumerate(weights):\n",
    "            for i, layer in enumerate(self.dense_layers):\n",
    "#                if index in dense_layer_to_prune:\n",
    "                    shape = weights[layer].shape\n",
    "                    flat_weights = weights[layer].flatten()\n",
    "                    flat_mask = weights[self.dense_masks[i]].flatten()\n",
    "                    no_of_weighs_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[self.dense_masks[i]] = mask_reshaped\n",
    "                    weights[layer] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        weights = prune_conv_layers_locally(self, ratio)\n",
    "        weights = prune_dense_layers_locally(self,ratio)\n",
    "        return True\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        all_masks = self.conv_masks + self.dense_masks\n",
    "        for i, x in enumerate(self.conv_layers + self.dense_layers):\n",
    "            flat_weights = np.append(flat_weights, weights[x].flatten())\n",
    "            flat_mask = np.append(flat_mask, weights[all_masks[i]].flatten())\n",
    "            \n",
    "        no_of_weights_to_prune = int(np.round(len(flat_weights)*ratio))\n",
    "        #print('total weights',len(flat_weights))\n",
    "        #print('weights to prune w/o round',int(len(flat_weights)*ratio))\n",
    "        #print('weights to prune with round',int(np.round(len(flat_weights)*ratio)))\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for i, x in enumerate(self.conv_layers + self.dense_layers):\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[all_masks[i]] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def prune_random_local_struct(self, ratio):\n",
    "        def prune_conv_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer in enumerate(self.conv_layers):\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = helpers.convert_from_hwio_to_iohw(weights[layer])\n",
    "                iohw_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[i]])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                #print(channels)\n",
    "                non_zero_channels = np.nonzero([np.sum(channel) for channel in channels])[0]\n",
    "                #print(non_zero_channels)\n",
    "                no_of_channels_to_prune_left = no_of_channels_to_prune - (len(channels) - len(non_zero_channels))\n",
    "                random.shuffle(non_zero_channels)\n",
    "                channels_to_prune = non_zero_channels[:no_of_channels_to_prune_left]\n",
    "                mask = tf.reshape(iohw_mask, \n",
    "                                  (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[layer] = helpers.convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[self.conv_masks[i]] = helpers.convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        def prune_dense_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(np.round(ratio * len(weights[layer_to_prune])))\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                random.shuffle(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[self.dense_masks[i]][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers(self, ratio)\n",
    "        prune_dense_layers(self, ratio)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    \n",
    "    def prune_magnitude_local_struct(self, ratio):\n",
    "        def prune_conv_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, x in enumerate(self.conv_layers):\n",
    "                vals = []\n",
    "                iohw_weights = helpers.convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[i]])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                \n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                for channel in channels:\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(channel)))\n",
    "                channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = helpers.convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[self.conv_masks[i]] = helpers.convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        def prune_dense_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                no_of_rows_to_prune = int(np.round(ratio * len(weights[layer_to_prune])))\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[self.dense_masks[i]][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        weights = prune_conv_layers(self, ratio)\n",
    "        weights = prune_dense_layers(self, ratio)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio):\n",
    "        def prune_conv_layers(self, ratio):\n",
    "            all_channels = []\n",
    "            all_masks = []\n",
    "            vals = []\n",
    "            for layer_to_prune in self.conv_layers:\n",
    "                # convert from e.g. (3,3,1,6) to (1,6,3,3)\n",
    "                iohw_weights = helpers.convert_from_hwio_to_iohw(weights[layer_to_prune])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = helpers.converted_shape[0]*converted_shape[1]\n",
    "                #convert from (1,6,3,3) to (6,3,3)\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                mask = np.ones((no_of_channels,converted_shape[2],converted_shape[3]))\n",
    "                #calculate average magnitude for each filter\n",
    "                vals = vals + [np.sum(np.abs(channel)) / np.prod(channel.shape) for channel in channels]\n",
    "                #vals = vals + [np.sum(np.abs(channel)) for channel in channels]\n",
    "                all_channels = list(all_channels) +  list(channels)\n",
    "                all_masks = list(all_masks) + list(mask)\n",
    "            #vals = [np.sum(np.abs(channel)) for channel in all_channels]\n",
    "            no_of_channels_to_prune = int(np.round(ratio * len(vals)))\n",
    "            #print('lenght of vals',len(vals))\n",
    "            #print('number of all channels',no_of_channels)\n",
    "            #print('channels',no_of_channels_to_prune)\n",
    "            channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "            \n",
    "            for channel_to_prune in channels_to_prune:\n",
    "                #print(all_channels[channel_to_prune].shape)\n",
    "                all_channels[channel_to_prune] = tf.zeros(all_channels[channel_to_prune].shape) \n",
    "                all_masks[channel_to_prune] = tf.zeros(all_channels[channel_to_prune].shape) \n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(self.conv_layers):\n",
    "                original_shape = helpers.convert_from_hwio_to_iohw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_channels[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                pruned_mask = tf.reshape(all_masks[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                weights[layer_to_prune] = helpers.convert_from_iohw_to_hwio(pruned_layer)\n",
    "                weights[self.conv_masks[i]] = helpers.convert_from_iohw_to_hwio(pruned_mask)\n",
    "                z = original_shape[0]*original_shape[1]\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers(self, ratio):\n",
    "            vals = []\n",
    "            lengths = []\n",
    "            for layer_to_prune in self.dense_layers:\n",
    "                #print('dense',layer_to_prune)\n",
    "                lengths.append(weights[layer_to_prune].shape[0])\n",
    "                vals = vals + [np.sum(np.abs(row)) / len(row) for row in weights[layer_to_prune]]\n",
    "                #vals = vals + [np.sum(np.abs(row)) for row in weights[layer_to_prune]]\n",
    "            no_of_rows_to_prune = int(np.round(ratio * len(vals)))\n",
    "            #print('rows', no_of_rows_to_prune)\n",
    "            rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    if row_to_prune in range(int(np.sum(lengths[:i])), int(np.sum(lengths[:i+1]))):\n",
    "                        weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)\n",
    "                        weights[self.dense_masks[i]][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)                \n",
    "            self.set_weights(weights)        \n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        weights = prune_conv_layers(self, ratio)\n",
    "        weights = prune_dense_layers(self, ratio)\n",
    "        #self.set_weights(weights)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "\n",
    "            weights = self.get_weights()\n",
    "\n",
    "            for layer_index, layer in enumerate(self.conv_layers):\n",
    "                #shape = 3,3,64,128\n",
    "                print(layer)\n",
    "                print('mask shape', weights[self.conv_masks[layer_index]].shape)\n",
    "                print('weights shape',weights[layer].shape)\n",
    "                converted_weights = helpers.convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = helpers.convert_from_hwio_to_iohw(weights[self.conv_masks[layer_index]]).numpy()\n",
    "                print('mask after conversion 1', converted_mask.shape)\n",
    "                print('weights after conversion',converted_weights.shape)\n",
    "                #shape = 128,64, 3,3\n",
    "                layer_shape = converted_weights.shape\n",
    "                flat_weights = converted_weights.flatten()\n",
    "                flat_masks = converted_mask.flatten()\n",
    "                no_of_weights_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                converted_mask = flat_masks.reshape(layer_shape)\n",
    "                converted_weights = flat_weights.reshape(layer_shape)\n",
    "                back_converted_mask = helpers.convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = helpers.convert_from_iohw_to_hwio(converted_weights)\n",
    "                print('mask after conversion 2', back_converted_mask.shape)\n",
    "                print('weights after conversion 2',back_converted_weights.shape)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[self.conv_masks[layer_index]] = back_converted_mask\n",
    "                #weights[self.conv_masks[layer_index]] = back_converted_mask\n",
    "                \n",
    "\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for index, layer in enumerate(self.dense_layers):\n",
    "                shape = weights[layer].shape\n",
    "                flat_weights = weights[layer].flatten()\n",
    "                flat_mask = weights[self.dense_masks[index]].flatten()\n",
    "\n",
    "                no_of_weights_to_prune = int(np.round(len(flat_weights)*ratio))\n",
    "                indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_mask[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                mask_reshaped = flat_mask.reshape(shape)\n",
    "                weights_reshaped = flat_weights.reshape(shape)\n",
    "                weights[self.dense_masks[index]] = mask_reshaped\n",
    "                weights[layer] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        weights = prune_conv_layers_locally(self,ratio)\n",
    "        weights = prune_dense_layers_locally(self,ratio)\n",
    "        return True\n",
    "    \n",
    "    def find_layers_and_masks(self):\n",
    "        if len(self.conv_layers) != 0:\n",
    "            return True\n",
    "        for i, w in enumerate(self.get_weights()):\n",
    "            print(i ,'/', len(self.get_weights()))\n",
    "            if len(w.shape) == 4 and w.shape[0] != 1: \n",
    "                if np.all([x == 0 or x == 1 for x in w.flatten()[:100]]) == False: \n",
    "                    self.conv_layers.append(i)\n",
    "                else:\n",
    "                    self.conv_masks.append(i)\n",
    "            if len(w.shape) == 2: \n",
    "                if np.all([x == 0 or x == 1 for x in w.flatten()[:100]]) == False: \n",
    "                    self.dense_layers.append(i)\n",
    "                else:\n",
    "                    self.dense_masks.append(i)\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
