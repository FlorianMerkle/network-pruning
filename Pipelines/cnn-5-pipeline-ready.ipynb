{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'cnn'\n",
    "#EXPERIMENT_NAME = 'cnn-global-magnitude-unstruct'\n",
    "ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import foolbox as fb\n",
    "import random\n",
    "import json\n",
    "\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 10)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5903 - accuracy: 0.8921 - val_loss: 1.5128 - val_accuracy: 0.9543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, goal ratio ist 0.0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5053 - accuracy: 0.9589 - val_loss: 1.4931 - val_accuracy: 0.9705\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4921 - accuracy: 0.9712 - val_loss: 1.4889 - val_accuracy: 0.9742\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4863 - accuracy: 0.9765 - val_loss: 1.4893 - val_accuracy: 0.9734\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4825 - accuracy: 0.9802 - val_loss: 1.4827 - val_accuracy: 0.9798\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4796 - accuracy: 0.9830 - val_loss: 1.4807 - val_accuracy: 0.9809\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4779 - accuracy: 0.9841 - val_loss: 1.4813 - val_accuracy: 0.9803\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4758 - accuracy: 0.9862 - val_loss: 1.4790 - val_accuracy: 0.9833\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4746 - accuracy: 0.9872 - val_loss: 1.4786 - val_accuracy: 0.9834\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4744 - accuracy: 0.9874 - val_loss: 1.4778 - val_accuracy: 0.9835\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4732 - accuracy: 0.9884 - val_loss: 1.4758 - val_accuracy: 0.9862\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4722 - accuracy: 0.9897 - val_loss: 1.4767 - val_accuracy: 0.9846\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4718 - accuracy: 0.9899 - val_loss: 1.4763 - val_accuracy: 0.9851\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4712 - accuracy: 0.9904 - val_loss: 1.4769 - val_accuracy: 0.9851\n",
      "WARNING:tensorflow:From /home/florian/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/models/tensorflow.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [03:56, 236.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, goal ratio ist 0.5\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5052 - accuracy: 0.9589 - val_loss: 1.4985 - val_accuracy: 0.9648\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4927 - accuracy: 0.9705 - val_loss: 1.4885 - val_accuracy: 0.9736\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4864 - accuracy: 0.9762 - val_loss: 1.4884 - val_accuracy: 0.9746\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4825 - accuracy: 0.9800 - val_loss: 1.4831 - val_accuracy: 0.9787\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4799 - accuracy: 0.9820 - val_loss: 1.4826 - val_accuracy: 0.9798\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4776 - accuracy: 0.9844 - val_loss: 1.4780 - val_accuracy: 0.9841\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4760 - accuracy: 0.9858 - val_loss: 1.4844 - val_accuracy: 0.9776\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4753 - accuracy: 0.9866 - val_loss: 1.4784 - val_accuracy: 0.9835\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4737 - accuracy: 0.9880 - val_loss: 1.4817 - val_accuracy: 0.9806\n",
      "current pruning ratio is0.5, goal ratio ist 0.5\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4910 - accuracy: 0.9808 - val_loss: 1.4818 - val_accuracy: 0.9834\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4780 - accuracy: 0.9866 - val_loss: 1.4785 - val_accuracy: 0.9855\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4745 - accuracy: 0.9891 - val_loss: 1.4779 - val_accuracy: 0.9850\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4725 - accuracy: 0.9902 - val_loss: 1.4763 - val_accuracy: 0.9865\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4717 - accuracy: 0.9905 - val_loss: 1.4760 - val_accuracy: 0.9863\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4706 - accuracy: 0.9917 - val_loss: 1.4763 - val_accuracy: 0.9854\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4694 - accuracy: 0.9929 - val_loss: 1.4766 - val_accuracy: 0.9847\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4685 - accuracy: 0.9936 - val_loss: 1.4751 - val_accuracy: 0.9868\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4685 - accuracy: 0.9935 - val_loss: 1.4740 - val_accuracy: 0.9873\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4676 - accuracy: 0.9941 - val_loss: 1.4737 - val_accuracy: 0.9878\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4679 - accuracy: 0.9940 - val_loss: 1.4744 - val_accuracy: 0.9868\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4671 - accuracy: 0.9947 - val_loss: 1.4744 - val_accuracy: 0.9875\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4673 - accuracy: 0.9943 - val_loss: 1.4740 - val_accuracy: 0.9875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [07:51, 235.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, goal ratio ist 0.75\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5051 - accuracy: 0.9595 - val_loss: 1.4958 - val_accuracy: 0.9677\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4919 - accuracy: 0.9715 - val_loss: 1.4887 - val_accuracy: 0.9740\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4857 - accuracy: 0.9769 - val_loss: 1.4873 - val_accuracy: 0.9754\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4825 - accuracy: 0.9798 - val_loss: 1.4843 - val_accuracy: 0.9772\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4800 - accuracy: 0.9823 - val_loss: 1.4819 - val_accuracy: 0.9805\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4780 - accuracy: 0.9841 - val_loss: 1.4822 - val_accuracy: 0.9801\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4760 - accuracy: 0.9862 - val_loss: 1.4781 - val_accuracy: 0.9841\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4746 - accuracy: 0.9871 - val_loss: 1.4779 - val_accuracy: 0.9838\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4734 - accuracy: 0.9885 - val_loss: 1.4757 - val_accuracy: 0.9860\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4723 - accuracy: 0.9894 - val_loss: 1.4775 - val_accuracy: 0.9843\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4718 - accuracy: 0.9898 - val_loss: 1.4759 - val_accuracy: 0.9858\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4715 - accuracy: 0.9903 - val_loss: 1.4789 - val_accuracy: 0.9821\n",
      "current pruning ratio is0.5, goal ratio ist 0.75\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4866 - accuracy: 0.9823 - val_loss: 1.4805 - val_accuracy: 0.9843\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4759 - accuracy: 0.9886 - val_loss: 1.4775 - val_accuracy: 0.9856\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9903 - val_loss: 1.4763 - val_accuracy: 0.9871\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9913 - val_loss: 1.4753 - val_accuracy: 0.9875\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9925 - val_loss: 1.4742 - val_accuracy: 0.9884\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4698 - accuracy: 0.9927 - val_loss: 1.4769 - val_accuracy: 0.9850\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4686 - accuracy: 0.9937 - val_loss: 1.4738 - val_accuracy: 0.9879\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4687 - accuracy: 0.9932 - val_loss: 1.4744 - val_accuracy: 0.9874\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4677 - accuracy: 0.9943 - val_loss: 1.4748 - val_accuracy: 0.9868\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4672 - accuracy: 0.9947 - val_loss: 1.4738 - val_accuracy: 0.9874\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4671 - accuracy: 0.9949 - val_loss: 1.4746 - val_accuracy: 0.9871\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4667 - accuracy: 0.9952 - val_loss: 1.4730 - val_accuracy: 0.9889\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4662 - accuracy: 0.9956 - val_loss: 1.4727 - val_accuracy: 0.9890\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4660 - accuracy: 0.9958 - val_loss: 1.4733 - val_accuracy: 0.9884\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4659 - accuracy: 0.9958 - val_loss: 1.4749 - val_accuracy: 0.9869\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4656 - accuracy: 0.9961 - val_loss: 1.4725 - val_accuracy: 0.9892\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4653 - accuracy: 0.9962 - val_loss: 1.4726 - val_accuracy: 0.9890\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4655 - accuracy: 0.9960 - val_loss: 1.4728 - val_accuracy: 0.9886\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4655 - accuracy: 0.9962 - val_loss: 1.4710 - val_accuracy: 0.9904\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4650 - accuracy: 0.9966 - val_loss: 1.4726 - val_accuracy: 0.9884\n",
      "Epoch 21/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4649 - accuracy: 0.9966 - val_loss: 1.4715 - val_accuracy: 0.9899\n",
      "Epoch 22/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4652 - accuracy: 0.9964 - val_loss: 1.4718 - val_accuracy: 0.9898\n",
      "current pruning ratio is0.75, goal ratio ist 0.75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5111 - accuracy: 0.9650 - val_loss: 1.4859 - val_accuracy: 0.9807\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4815 - accuracy: 0.9851 - val_loss: 1.4800 - val_accuracy: 0.9851\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4767 - accuracy: 0.9882 - val_loss: 1.4783 - val_accuracy: 0.9856\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4739 - accuracy: 0.9905 - val_loss: 1.4768 - val_accuracy: 0.9876\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4720 - accuracy: 0.9919 - val_loss: 1.4767 - val_accuracy: 0.9861\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4709 - accuracy: 0.9926 - val_loss: 1.4747 - val_accuracy: 0.9885\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4697 - accuracy: 0.9935 - val_loss: 1.4750 - val_accuracy: 0.9873\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4691 - accuracy: 0.9937 - val_loss: 1.4757 - val_accuracy: 0.9869\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4686 - accuracy: 0.9939 - val_loss: 1.4744 - val_accuracy: 0.9877\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4681 - accuracy: 0.9946 - val_loss: 1.4739 - val_accuracy: 0.9882\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4673 - accuracy: 0.9951 - val_loss: 1.4743 - val_accuracy: 0.9878\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4671 - accuracy: 0.9953 - val_loss: 1.4732 - val_accuracy: 0.9890\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4666 - accuracy: 0.9955 - val_loss: 1.4734 - val_accuracy: 0.9885\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4661 - accuracy: 0.9960 - val_loss: 1.4726 - val_accuracy: 0.9893\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4660 - accuracy: 0.9959 - val_loss: 1.4733 - val_accuracy: 0.9883\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4659 - accuracy: 0.9962 - val_loss: 1.4739 - val_accuracy: 0.9878\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4655 - accuracy: 0.9965 - val_loss: 1.4743 - val_accuracy: 0.9877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [12:48, 254.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, goal ratio ist 0.875\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5049 - accuracy: 0.9596 - val_loss: 1.4956 - val_accuracy: 0.9679\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4922 - accuracy: 0.9712 - val_loss: 1.4886 - val_accuracy: 0.9749\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4856 - accuracy: 0.9769 - val_loss: 1.4844 - val_accuracy: 0.9778\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4823 - accuracy: 0.9800 - val_loss: 1.4835 - val_accuracy: 0.9783\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4798 - accuracy: 0.9827 - val_loss: 1.4828 - val_accuracy: 0.9788\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4784 - accuracy: 0.9835 - val_loss: 1.4792 - val_accuracy: 0.9826\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4760 - accuracy: 0.9858 - val_loss: 1.4796 - val_accuracy: 0.9826\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4750 - accuracy: 0.9870 - val_loss: 1.4784 - val_accuracy: 0.9838\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4743 - accuracy: 0.9874 - val_loss: 1.4782 - val_accuracy: 0.9828\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4728 - accuracy: 0.9889 - val_loss: 1.4779 - val_accuracy: 0.9837\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4724 - accuracy: 0.9892 - val_loss: 1.4775 - val_accuracy: 0.9834\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4715 - accuracy: 0.9901 - val_loss: 1.4780 - val_accuracy: 0.9832\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4708 - accuracy: 0.9906 - val_loss: 1.4808 - val_accuracy: 0.9803\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4706 - accuracy: 0.9911 - val_loss: 1.4753 - val_accuracy: 0.9864\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4699 - accuracy: 0.9917 - val_loss: 1.4766 - val_accuracy: 0.9849\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4701 - accuracy: 0.9915 - val_loss: 1.4768 - val_accuracy: 0.9847\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4695 - accuracy: 0.9921 - val_loss: 1.4745 - val_accuracy: 0.9867\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4686 - accuracy: 0.9930 - val_loss: 1.4759 - val_accuracy: 0.9857\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4688 - accuracy: 0.9927 - val_loss: 1.4748 - val_accuracy: 0.9867\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4681 - accuracy: 0.9934 - val_loss: 1.4750 - val_accuracy: 0.9862\n",
      "current pruning ratio is0.5, goal ratio ist 0.875\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4838 - accuracy: 0.9841 - val_loss: 1.4786 - val_accuracy: 0.9858\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4736 - accuracy: 0.9904 - val_loss: 1.4771 - val_accuracy: 0.9859\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4712 - accuracy: 0.9920 - val_loss: 1.4761 - val_accuracy: 0.9873\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4696 - accuracy: 0.9934 - val_loss: 1.4753 - val_accuracy: 0.9875\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4688 - accuracy: 0.9936 - val_loss: 1.4746 - val_accuracy: 0.9880\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4679 - accuracy: 0.9942 - val_loss: 1.4736 - val_accuracy: 0.9886\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4673 - accuracy: 0.9949 - val_loss: 1.4734 - val_accuracy: 0.9888\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4670 - accuracy: 0.9949 - val_loss: 1.4754 - val_accuracy: 0.9865\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4667 - accuracy: 0.9953 - val_loss: 1.4745 - val_accuracy: 0.9872\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4663 - accuracy: 0.9955 - val_loss: 1.4744 - val_accuracy: 0.9874\n",
      "current pruning ratio is0.75, goal ratio ist 0.875\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5267 - accuracy: 0.9551 - val_loss: 1.4897 - val_accuracy: 0.9801\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4850 - accuracy: 0.9828 - val_loss: 1.4824 - val_accuracy: 0.9844\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4789 - accuracy: 0.9869 - val_loss: 1.4804 - val_accuracy: 0.9848\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4756 - accuracy: 0.9890 - val_loss: 1.4776 - val_accuracy: 0.9868\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4735 - accuracy: 0.9904 - val_loss: 1.4763 - val_accuracy: 0.9872\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4719 - accuracy: 0.9918 - val_loss: 1.4748 - val_accuracy: 0.9884\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4706 - accuracy: 0.9926 - val_loss: 1.4764 - val_accuracy: 0.9861\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4697 - accuracy: 0.9934 - val_loss: 1.4748 - val_accuracy: 0.9880\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4689 - accuracy: 0.9940 - val_loss: 1.4745 - val_accuracy: 0.9878\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4682 - accuracy: 0.9945 - val_loss: 1.4753 - val_accuracy: 0.9869\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4681 - accuracy: 0.9945 - val_loss: 1.4737 - val_accuracy: 0.9890\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4674 - accuracy: 0.9951 - val_loss: 1.4747 - val_accuracy: 0.9873\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4669 - accuracy: 0.9955 - val_loss: 1.4750 - val_accuracy: 0.9877\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4668 - accuracy: 0.9954 - val_loss: 1.4728 - val_accuracy: 0.9897\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4662 - accuracy: 0.9960 - val_loss: 1.4739 - val_accuracy: 0.9888\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4659 - accuracy: 0.9962 - val_loss: 1.4735 - val_accuracy: 0.9887\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4657 - accuracy: 0.9962 - val_loss: 1.4746 - val_accuracy: 0.9868\n",
      "current pruning ratio is0.875, goal ratio ist 0.875\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5655 - accuracy: 0.9232 - val_loss: 1.5061 - val_accuracy: 0.9665\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5019 - accuracy: 0.9690 - val_loss: 1.4928 - val_accuracy: 0.9759\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4919 - accuracy: 0.9758 - val_loss: 1.4876 - val_accuracy: 0.9793\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4867 - accuracy: 0.9794 - val_loss: 1.4860 - val_accuracy: 0.9792\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4832 - accuracy: 0.9823 - val_loss: 1.4836 - val_accuracy: 0.9807\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4812 - accuracy: 0.9836 - val_loss: 1.4819 - val_accuracy: 0.9813\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4793 - accuracy: 0.9849 - val_loss: 1.4808 - val_accuracy: 0.9828\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4779 - accuracy: 0.9860 - val_loss: 1.4807 - val_accuracy: 0.9818\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4768 - accuracy: 0.9869 - val_loss: 1.4799 - val_accuracy: 0.9830\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4759 - accuracy: 0.9879 - val_loss: 1.4791 - val_accuracy: 0.9833\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4747 - accuracy: 0.9889 - val_loss: 1.4790 - val_accuracy: 0.9837\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4740 - accuracy: 0.9895 - val_loss: 1.4786 - val_accuracy: 0.9841\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4729 - accuracy: 0.9906 - val_loss: 1.4774 - val_accuracy: 0.9855\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4725 - accuracy: 0.9907 - val_loss: 1.4775 - val_accuracy: 0.9852\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4719 - accuracy: 0.9914 - val_loss: 1.4772 - val_accuracy: 0.9857\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9918 - val_loss: 1.4766 - val_accuracy: 0.9856\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4707 - accuracy: 0.9923 - val_loss: 1.4764 - val_accuracy: 0.9857\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4702 - accuracy: 0.9927 - val_loss: 1.4759 - val_accuracy: 0.9869\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4702 - accuracy: 0.9928 - val_loss: 1.4760 - val_accuracy: 0.9862\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4694 - accuracy: 0.9932 - val_loss: 1.4758 - val_accuracy: 0.9864\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4690 - accuracy: 0.9937 - val_loss: 1.4761 - val_accuracy: 0.9857\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4691 - accuracy: 0.9936 - val_loss: 1.4756 - val_accuracy: 0.9864\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4685 - accuracy: 0.9939 - val_loss: 1.4752 - val_accuracy: 0.9866\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4683 - accuracy: 0.9941 - val_loss: 1.4756 - val_accuracy: 0.9868\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4680 - accuracy: 0.9944 - val_loss: 1.4752 - val_accuracy: 0.9870\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4679 - accuracy: 0.9944 - val_loss: 1.4756 - val_accuracy: 0.9860\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [18:43, 284.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, goal ratio ist 0.9375\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5052 - accuracy: 0.9593 - val_loss: 1.4984 - val_accuracy: 0.9653\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4923 - accuracy: 0.9709 - val_loss: 1.4893 - val_accuracy: 0.9726\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4861 - accuracy: 0.9766 - val_loss: 1.4869 - val_accuracy: 0.9752\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4822 - accuracy: 0.9801 - val_loss: 1.4826 - val_accuracy: 0.9788\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4796 - accuracy: 0.9824 - val_loss: 1.4823 - val_accuracy: 0.9800\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4781 - accuracy: 0.9840 - val_loss: 1.4817 - val_accuracy: 0.9804\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4760 - accuracy: 0.9860 - val_loss: 1.4775 - val_accuracy: 0.9839\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4747 - accuracy: 0.9873 - val_loss: 1.4787 - val_accuracy: 0.9829\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4736 - accuracy: 0.9882 - val_loss: 1.4757 - val_accuracy: 0.9860\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4724 - accuracy: 0.9894 - val_loss: 1.4778 - val_accuracy: 0.9837\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4723 - accuracy: 0.9895 - val_loss: 1.4770 - val_accuracy: 0.9843\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4713 - accuracy: 0.9903 - val_loss: 1.4773 - val_accuracy: 0.9844\n",
      "current pruning ratio is0.5, goal ratio ist 0.9375\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4858 - accuracy: 0.9835 - val_loss: 1.4802 - val_accuracy: 0.9842\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4755 - accuracy: 0.9887 - val_loss: 1.4762 - val_accuracy: 0.9860\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4728 - accuracy: 0.9906 - val_loss: 1.4769 - val_accuracy: 0.9860\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4709 - accuracy: 0.9920 - val_loss: 1.4745 - val_accuracy: 0.9876\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4699 - accuracy: 0.9926 - val_loss: 1.4753 - val_accuracy: 0.9878\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4693 - accuracy: 0.9930 - val_loss: 1.4754 - val_accuracy: 0.9868\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4686 - accuracy: 0.9937 - val_loss: 1.4733 - val_accuracy: 0.9892\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4676 - accuracy: 0.9945 - val_loss: 1.4741 - val_accuracy: 0.9879\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4674 - accuracy: 0.9946 - val_loss: 1.4749 - val_accuracy: 0.9871\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4671 - accuracy: 0.9949 - val_loss: 1.4756 - val_accuracy: 0.9859\n",
      "current pruning ratio is0.75, goal ratio ist 0.9375\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5265 - accuracy: 0.9559 - val_loss: 1.4908 - val_accuracy: 0.9780\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4858 - accuracy: 0.9819 - val_loss: 1.4831 - val_accuracy: 0.9827\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4797 - accuracy: 0.9855 - val_loss: 1.4796 - val_accuracy: 0.9847\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4764 - accuracy: 0.9880 - val_loss: 1.4779 - val_accuracy: 0.9854\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4743 - accuracy: 0.9897 - val_loss: 1.4777 - val_accuracy: 0.9850\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4725 - accuracy: 0.9913 - val_loss: 1.4759 - val_accuracy: 0.9878\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4715 - accuracy: 0.9917 - val_loss: 1.4766 - val_accuracy: 0.9862\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4704 - accuracy: 0.9927 - val_loss: 1.4749 - val_accuracy: 0.9874\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4694 - accuracy: 0.9934 - val_loss: 1.4765 - val_accuracy: 0.9859\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4688 - accuracy: 0.9941 - val_loss: 1.4747 - val_accuracy: 0.9875\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4683 - accuracy: 0.9943 - val_loss: 1.4751 - val_accuracy: 0.9867\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4678 - accuracy: 0.9948 - val_loss: 1.4764 - val_accuracy: 0.9860\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4671 - accuracy: 0.9953 - val_loss: 1.4743 - val_accuracy: 0.9876\n",
      "Epoch 14/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4671 - accuracy: 0.9950 - val_loss: 1.4741 - val_accuracy: 0.9877\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4667 - accuracy: 0.9953 - val_loss: 1.4732 - val_accuracy: 0.9889\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4664 - accuracy: 0.9959 - val_loss: 1.4732 - val_accuracy: 0.9886\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4664 - accuracy: 0.9956 - val_loss: 1.4740 - val_accuracy: 0.9875\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4657 - accuracy: 0.9963 - val_loss: 1.4728 - val_accuracy: 0.9891\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4657 - accuracy: 0.9962 - val_loss: 1.4744 - val_accuracy: 0.9873\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4657 - accuracy: 0.9962 - val_loss: 1.4732 - val_accuracy: 0.9884\n",
      "Epoch 21/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4654 - accuracy: 0.9964 - val_loss: 1.4728 - val_accuracy: 0.9893\n",
      "current pruning ratio is0.875, goal ratio ist 0.9375\n",
      "Epoch 1/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5503 - accuracy: 0.9311 - val_loss: 1.5033 - val_accuracy: 0.9682\n",
      "Epoch 2/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4984 - accuracy: 0.9710 - val_loss: 1.4925 - val_accuracy: 0.9732\n",
      "Epoch 3/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4896 - accuracy: 0.9776 - val_loss: 1.4887 - val_accuracy: 0.9764\n",
      "Epoch 4/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4849 - accuracy: 0.9812 - val_loss: 1.4849 - val_accuracy: 0.9800\n",
      "Epoch 5/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4818 - accuracy: 0.9831 - val_loss: 1.4841 - val_accuracy: 0.9790\n",
      "Epoch 6/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9850 - val_loss: 1.4807 - val_accuracy: 0.9839\n",
      "Epoch 7/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4782 - accuracy: 0.9858 - val_loss: 1.4809 - val_accuracy: 0.9829\n",
      "Epoch 8/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4764 - accuracy: 0.9877 - val_loss: 1.4797 - val_accuracy: 0.9831\n",
      "Epoch 9/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9884 - val_loss: 1.4789 - val_accuracy: 0.9846\n",
      "Epoch 10/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4743 - accuracy: 0.9894 - val_loss: 1.4784 - val_accuracy: 0.9850\n",
      "Epoch 11/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4735 - accuracy: 0.9901 - val_loss: 1.4778 - val_accuracy: 0.9850\n",
      "Epoch 12/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4726 - accuracy: 0.9908 - val_loss: 1.4782 - val_accuracy: 0.9842\n",
      "Epoch 13/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4721 - accuracy: 0.9913 - val_loss: 1.4782 - val_accuracy: 0.9846\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4717 - accuracy: 0.9913 - val_loss: 1.4772 - val_accuracy: 0.9855\n",
      "Epoch 15/100\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4710 - accuracy: 0.9920 - val_loss: 1.4768 - val_accuracy: 0.9861\n",
      "Epoch 16/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4706 - accuracy: 0.9924 - val_loss: 1.4776 - val_accuracy: 0.9848\n",
      "Epoch 17/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4701 - accuracy: 0.9929 - val_loss: 1.4764 - val_accuracy: 0.9857\n",
      "Epoch 18/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4699 - accuracy: 0.9931 - val_loss: 1.4778 - val_accuracy: 0.9850\n",
      "Epoch 19/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4693 - accuracy: 0.9934 - val_loss: 1.4790 - val_accuracy: 0.9837\n",
      "Epoch 20/100\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4689 - accuracy: 0.9939 - val_loss: 1.4767 - val_accuracy: 0.9860\n",
      "current pruning ratio is0.9375, goal ratio ist 0.9375\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.6684 - accuracy: 0.8258 - val_loss: 1.5547 - val_accuracy: 0.9282\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5447 - accuracy: 0.9314 - val_loss: 1.5294 - val_accuracy: 0.9456\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5270 - accuracy: 0.9448 - val_loss: 1.5159 - val_accuracy: 0.9544\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5168 - accuracy: 0.9525 - val_loss: 1.5082 - val_accuracy: 0.9604\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5100 - accuracy: 0.9579 - val_loss: 1.5037 - val_accuracy: 0.9641\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5054 - accuracy: 0.9617 - val_loss: 1.5016 - val_accuracy: 0.9658\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5016 - accuracy: 0.9651 - val_loss: 1.4990 - val_accuracy: 0.9674\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4985 - accuracy: 0.9678 - val_loss: 1.4956 - val_accuracy: 0.9696\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4964 - accuracy: 0.9700 - val_loss: 1.4942 - val_accuracy: 0.9703\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4941 - accuracy: 0.9718 - val_loss: 1.4949 - val_accuracy: 0.9704\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4927 - accuracy: 0.9729 - val_loss: 1.4929 - val_accuracy: 0.9711\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4912 - accuracy: 0.9740 - val_loss: 1.4916 - val_accuracy: 0.9728\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4902 - accuracy: 0.9745 - val_loss: 1.4914 - val_accuracy: 0.9719\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4888 - accuracy: 0.9761 - val_loss: 1.4908 - val_accuracy: 0.9722\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4882 - accuracy: 0.9768 - val_loss: 1.4901 - val_accuracy: 0.9731\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4869 - accuracy: 0.9779 - val_loss: 1.4893 - val_accuracy: 0.9745\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4863 - accuracy: 0.9782 - val_loss: 1.4889 - val_accuracy: 0.9751\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4854 - accuracy: 0.9791 - val_loss: 1.4895 - val_accuracy: 0.9747\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4848 - accuracy: 0.9796 - val_loss: 1.4884 - val_accuracy: 0.9750\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4844 - accuracy: 0.9800 - val_loss: 1.4881 - val_accuracy: 0.9743\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4838 - accuracy: 0.9804 - val_loss: 1.4868 - val_accuracy: 0.9761\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4832 - accuracy: 0.9809 - val_loss: 1.4880 - val_accuracy: 0.9752\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4826 - accuracy: 0.9815 - val_loss: 1.4864 - val_accuracy: 0.9770\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4821 - accuracy: 0.9819 - val_loss: 1.4865 - val_accuracy: 0.9772\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4820 - accuracy: 0.9818 - val_loss: 1.4870 - val_accuracy: 0.9762\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4811 - accuracy: 0.9827 - val_loss: 1.4867 - val_accuracy: 0.9767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [23:01, 345.33s/it]\n",
      "  0%|          | 0/10 [23:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-85b2f47289a4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run_experiment(\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unstructured'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'random'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0miterations\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mITERATIONS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-0854c8183525>\u001b[0m in \u001b[0;36mrun_experiment\u001b[0;34m(structure, method, scope, iterations)\u001b[0m\n\u001b[1;32m     81\u001b[0m                     \u001b[0maccuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m                     \u001b[0mpgd_success_rate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpgd_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                     \u001b[0mcw_success_rate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcw2_attack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     84\u001b[0m         \u001b[0mall_accuracies\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maccuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m         \u001b[0mpgd_success_rates\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpgd_success_rate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-309400c4b696>\u001b[0m in \u001b[0;36mcw2_attack\u001b[0;34m(model_to_attack)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mfmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensorFlowModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_to_attack\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mattack\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mL2CarliniWagnerAttack\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     adversarials = attack(\n\u001b[0m\u001b[1;32m     97\u001b[0m         \u001b[0mfmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m         \u001b[0mx_to_attack\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/attacks/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    409\u001b[0m         \u001b[0;31m# run the actual attack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 410\u001b[0;31m         \u001b[0mxp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stop\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mearly_stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    411\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mxpcs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/foolbox/attacks/carlini_wagner.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m                 \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mperturbed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_aux_and_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdelta\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconsts_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m                 \u001b[0mdelta\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstepsize\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/eagerpy/tensor/tensorflow.py\u001b[0m in \u001b[0;36mvalue_and_grad\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m    463\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    464\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 465\u001b[0;31m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    466\u001b[0m             \u001b[0mgrad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTensorFlowTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    467\u001b[0m             \u001b[0;32massert\u001b[0m \u001b[0mgrad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mx_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                           for x in nest.flatten(output_gradients)]\n\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m     flat_grad = imperative_grad.imperative_grad(\n\u001b[0m\u001b[1;32m   1043\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m         \u001b[0mflat_targets\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     69\u001b[0m         \"Unknown value for unconnected_gradients: %r\" % unconnected_gradients)\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m   return pywrap_tfe.TFE_Py_TapeGradient(\n\u001b[0m\u001b[1;32m     72\u001b[0m       \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_tape\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m       \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"gradient_tape/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mname_scope\u001b[0;34m(name, default_name, values, skip_on_eager)\u001b[0m\n\u001b[1;32m   6218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6219\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mskip_on_eager\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6220\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mNullContextmanager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6222\u001b[0m   \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdefault_name\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='struct', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='struct', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='struct', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune, Train Attack Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(structure='unstructured', method='random', scope='global', iterations=10):\n",
    "    \n",
    "    experiment_name = f'{ARCHITECTURE}-{method}-{scope}-{structure}'\n",
    "    pgd_success_rates = []\n",
    "    cw_success_rates = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    #compression_rates = [1, 2, 4, 64]\n",
    "    compression_rates = [tf.math.pow(2, x).numpy() for x in range(7)]\n",
    "    pruning_ratios = [1-1/x for x in compression_rates]\n",
    "\n",
    "    for j in tqdm(range(iterations)):\n",
    "        accuracies = []\n",
    "        pgd_success_rate = []\n",
    "        cw_success_rate = []\n",
    "        try: \n",
    "            del model\n",
    "        except:\n",
    "            ;\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = initialize_base_model(j, experiment_name=experiment_name, save_weights=True)\n",
    "        for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "\n",
    "            model.load_weights(f'./saved-weights/{experiment_name}-{j}')\n",
    "            #print(f'./saved-weights/{experiment_name}-{j}')\n",
    "\n",
    "            for i in range(index + 1):\n",
    "                print(f'current pruning ratio is{pruning_ratios[i]}, goal ratio ist {pruning_ratio}')\n",
    "                if i != index:\n",
    "\n",
    "                    if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_random_global_unstruct(pruning_ratios[i])\n",
    "                    if  method=='random' and scope=='global' and structure=='structured':\n",
    "                        model.prune_random_global_struct(pruning_ratios[i])\n",
    "                    if  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_random_local_unstruct(pruning_ratios[i])\n",
    "                    if  method=='random' and scope=='local' and structure=='structured':\n",
    "                        model.prune_random_local_struct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_magnitude_global_unstruct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                        model.prune_magnitude_global_struct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_magnitude_local_unstruct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                        model.prune_magnitude_local_struct(pruning_ratios[i])\n",
    "\n",
    "\n",
    "                    # must recompile otherwise pruned weights will get updated\n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                                  metrics=['accuracy'],\n",
    "                                  experimental_run_tf_function=False\n",
    "                                 )\n",
    "                    model = train_model(model, to_convergence=False)\n",
    "                if i == index:\n",
    "                    if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_random_global_unstruct(pruning_ratios[i])\n",
    "                    if  method=='random' and scope=='global' and structure=='structured':\n",
    "                        model.prune_random_global_struct(pruning_ratios[i])\n",
    "                    if  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_random_local_unstruct(pruning_ratios[i])\n",
    "                    if  method=='random' and scope=='local' and structure=='structured':\n",
    "                        model.prune_random_local_struct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                        model.prune_magnitude_global_unstruct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                        model.prune_magnitude_global_struct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                        model.prune_magnitude_local_unstruct(pruning_ratios[i])\n",
    "                    if  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                        model.prune_magnitude_local_struct(pruning_ratios[i])\n",
    "                    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                                  metrics=['accuracy'],\n",
    "                                  experimental_run_tf_function=False\n",
    "                                 )\n",
    "                    model = train_model(model, to_convergence=True)\n",
    "                    accuracies.append(model.evaluate(x_test, y_test, verbose=0))\n",
    "                    pgd_success_rate.append(pgd_attack(model))\n",
    "                    cw_success_rate.append(cw2_attack(model))\n",
    "        all_accuracies.append(accuracies)\n",
    "        pgd_success_rates.append(pgd_success_rate)\n",
    "        cw_success_rates.append(cw_success_rate)\n",
    "    #write to csv and json\n",
    "\n",
    "    #pd.DataFrame(all_accuracies).to_csv(f'saved-results/{experiment_name}-accuracies.csv',index=False)\n",
    "    with open(f'saved-results/{experiment_name}-accuracies.json', 'w') as f:\n",
    "        json.dump(all_accuracies, f)\n",
    "\n",
    "    #pd.DataFrame(pgd_success_rates).to_csv(f'saved-results/{experiment_name}-pgd-success.csv',index=False)\n",
    "    with open(f'saved-results/{experiment_name}-pgd-success.json', 'w') as f:\n",
    "        json.dump(pgd_success_rates, f)\n",
    "    #pd.DataFrame(cw_success_rates).to_csv(f'saved-results/{experiment_name}-cw0-success.csv',index=False)\n",
    "    with open(f'saved-results/{experiment_name}-cw2-success', 'w') as f:\n",
    "        json.dump(cw_success_rates, f)\n",
    "\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune, Train Attack Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'all_accuracies' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-71e5815d8ce2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_accuracies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0midy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0mall_accuracies\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midy\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'all_accuracies' is not defined"
     ]
    }
   ],
   "source": [
    "for idx,x in enumerate(all_accuracies):\n",
    "    for idy,y in enumerate(x):\n",
    "        all_accuracies[idx][idy] = [float(y[0]), float(y[1])]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zeros_ratio(weights):\n",
    "    layers_to_examine = [0,3,6,9,12]\n",
    "    all_weights = np.array([])\n",
    "    for x in layers_to_examine:\n",
    "\n",
    "        all_weights = np.append(all_weights, weights[x].flatten())\n",
    "    return np.count_nonzero(all_weights)/len(all_weights), np.count_nonzero(all_weights), len(all_weights)\n",
    "\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "\n",
    "    model = CustomConvModel()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, to_convergence=True):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "def prune_weights(model, pruning_ratio):\n",
    "    weights = model.get_weights()\n",
    "    weights_to_prune = model.get_weights()\n",
    "    for index, weight in enumerate(weights):\n",
    "        \n",
    "        if (index == 9) or (index == 12) :\n",
    "            #print(weight.shape)\n",
    "            #print(index)\n",
    "            flat_weights = weight.flatten()\n",
    "            flat_weights_to_prune = weights_to_prune[index+2].flatten()\n",
    "            #print (flat_weights_to_prune.shape, flat_weights.shape)\n",
    "            flat_weights_df = pd.DataFrame(flat_weights)\n",
    "            flat_weights_to_prune_df = pd.DataFrame(flat_weights_to_prune)\n",
    "            no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "            #print(len(flat_weights))\n",
    "            #print('no of weights',no_of_weights_to_prune)\n",
    "            #print('weights to prune shape', flat_weights_to_prune.shape)\n",
    "            indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "            for idx_to_delete in indices_to_delete:\n",
    "                flat_weights_to_prune[idx_to_delete] = 0\n",
    "            dims = weights_to_prune[index+2].shape\n",
    "            weights_reshaped = flat_weights_to_prune.reshape(dims)\n",
    "            weights_to_prune[index+2] = weights_reshaped\n",
    "    #print(weights_to_prune)\n",
    "    return weights_to_prune\n",
    "\n",
    "def pgd_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        epsilons=[25/255]\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/len(y_to_attack)\n",
    "\n",
    "def cw2_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.L2CarliniWagnerAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        #epsilons=[.5]\n",
    "        epsilons=None\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/len(y_to_attack)\n",
    "\n",
    "\n",
    "def prune_conv_layers(pruning_ratio):\n",
    "    layer_to_prune = [0, 3]\n",
    "    pruned_weights = model.get_weights()\n",
    "    \n",
    "    for layer in layer_to_prune:\n",
    "        converted_weights = convert_from_hwio_to_iohw(model.get_weights()[layer])\n",
    "        converted_mask = convert_from_hwio_to_iohw(model.get_weights()[layer + 2]).numpy()\n",
    "        for input_index, input_layer in enumerate(converted_weights):\n",
    "\n",
    "            for kernel_index, kernel in enumerate(input_layer):\n",
    "                dims = kernel.shape\n",
    "                flat_weights = kernel.numpy().flatten()\n",
    "                flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "                #print(no_of_weights_to_prune)\n",
    "                indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "\n",
    "                converted_mask[input_index][kernel_index] = flat_masks.reshape(dims)\n",
    "        back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "        pruned_weights[layer+2] = back_converted_mask\n",
    "    \n",
    "    return pruned_weights\n",
    "\n",
    "def convert_from_hwio_to_iohw(weights_nchw):\n",
    "    return tf.transpose(weights_nchw, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "def convert_from_iohw_to_hwio(weights_nhwc):\n",
    "    return tf.transpose(weights_nhwc, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "def get_average_accuracies(all_accuracies):\n",
    "    acc_per_pruning_rate=[]\n",
    "    for i in range(len(all_accuracies)):\n",
    "        for j in range(len(all_accuracies[i])):\n",
    "\n",
    "            try:\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "            except:\n",
    "                acc_per_pruning_rate.append([])\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "    avg_acc_per_pruning_rate = [sum(x)/len(x) for x in acc_per_pruning_rate]; avg_acc_per_pruning_rate\n",
    "    return avg_acc_per_pruning_rate\n",
    "\n",
    "def get_average_success_rates(all_success_rates):\n",
    "    success_per_pruning_rate=[]\n",
    "    for i in range(len(all_success_rates)):\n",
    "        for j in range(len(all_success_rates[i])):\n",
    "\n",
    "            try:\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "            except:\n",
    "                success_per_pruning_rate.append([])\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "    avg_success_per_pruning_rate = [sum(x)/len(x) for x in success_per_pruning_rate];avg_success_per_pruning_rate\n",
    "    return avg_success_per_pruning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x_to_attack = tf.convert_to_tensor(x_train[:1000].reshape(1000,28*28))\n",
    "y_to_attack = tf.convert_to_tensor([y_train[:1000]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.config.list_physical_devicesshapes = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'conv_1': (5, 5, 1, 6),\n",
    "    # 5x5 conv, 6 inputs, 16 outputs\n",
    "    'conv_2': (5, 5, 6, 16),\n",
    "    #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "    'conv_3': (1, 1, 16, 120),\n",
    "    # fully connected, 5*5*16 inputs, 120 outputs\n",
    "    'dense_1': (5*5*16, 120),\n",
    "    # fully connected, 120 inputs, 84 outputs\n",
    "    'dense_2': (120, 84),\n",
    "    # 84 inputs, 10 outputs (class prediction)\n",
    "    'dense_3': (84, 10),\n",
    "}\n",
    "bias_shapes = {\n",
    "    #output depth\n",
    "    'conv_1': (6),\n",
    "    'conv_2': (16),\n",
    "    'dense_1': (120),\n",
    "    'dense_2': (84),\n",
    "    'dense_3': (10),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, weights, mask, biases, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = weights\n",
    "        self.m = mask\n",
    "        self.b = biases\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='valid'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "#        return tf.keras.layers.AveragePooling2D(pool_size=(self.k, self.k), strides=None, padding=self.p, data_format='channels_first')(inputs)\n",
    "        return tf.nn.avg_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p,)# data_format='NCHW')\n",
    "    \n",
    "\n",
    "        \n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "\n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'tanh'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'tanh':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        \n",
    "        \n",
    "class CustomConvModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvModel, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], True, 1, 'SAME')#'VALID')\n",
    "        self.maxpool1 = CustomPoolLayer(k=2, padding='SAME')\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], True, 1, 'VALID')\n",
    "        self.maxpool2 = CustomPoolLayer(k=2, padding='VALID')\n",
    "\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], True, 'tanh')\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], True, 'tanh')\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, 'softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, shape=[-1,28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x =  self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            weights = self.get_weights()\n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        \n",
    "                        no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                        # find unpruned weights\n",
    "                        non_zero_weights = np.nonzero(flat_masks)[0]\n",
    "                        # calculate the amount of weights to be pruned this round\n",
    "                        no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                        # shuffle all non-zero weights\n",
    "                        random.shuffle(non_zero_weights)\n",
    "                        # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                        indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                        \n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "                    no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self, ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "        #flat out all weights:\n",
    "        conv_layer_to_prune = [0, 3]\n",
    "        dense_layer_to_prune = [6, 9, 12]\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            flat_weights = np.append(flat_weights, weights[x])\n",
    "            flat_mask = np.append(flat_mask, weights[x+2])\n",
    "            \n",
    "        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[x + 2] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            #print('inside conv prune func',get_zeros_ratio(self.get_weights()))\n",
    "            weights = self.get_weights()\n",
    "            \n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        #flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                        #flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                        #print(no_of_weights_to_prune)\n",
    "                        #indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "\n",
    "\n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            \n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "\n",
    "                    no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                    indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self,ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "        return\n",
    "    \n",
    "    def prune_random_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                #print(channels)\n",
    "                non_zero_channels = np.nonzero([np.sum(channel) for channel in channels])[0]\n",
    "                #print(non_zero_channels)\n",
    "                no_of_channels_to_prune_left = no_of_channels_to_prune - (len(channels) - len(non_zero_channels))\n",
    "                random.shuffle(non_zero_channels)\n",
    "                channels_to_prune = non_zero_channels[:no_of_channels_to_prune_left]\n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return True\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                random.shuffle(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return True\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    def prune_magnitude_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                \n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                for channel in channels:\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(channel)))\n",
    "                channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return weights\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            all_channels = np.empty((0,5,5))\n",
    "            original_shapes = []\n",
    "            for layer_to_prune in conv_layers_to_prune:\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[layer_to_prune])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                all_channels = np.concatenate((all_channels, channels))\n",
    "            mask = np.ones(all_channels.shape)\n",
    "            vals = [np.sum(np.abs(channel)) for channel in all_channels]\n",
    "            no_of_channels_to_prune = int(ratio * len(vals))\n",
    "            channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "            for channel_to_prune in channels_to_prune:\n",
    "                all_channels[channel_to_prune] = tf.zeros((5,5))\n",
    "                mask[channel_to_prune] = tf.zeros((5,5))\n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(conv_layers_to_prune):\n",
    "                original_shape = convert_from_hwio_to_iohw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_channels[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                pruned_mask = tf.reshape(mask[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                weights[layer_to_prune] = convert_from_iohw_to_hwio(pruned_layer)\n",
    "                weights[layer_to_prune + 2] = convert_from_iohw_to_hwio(pruned_mask)\n",
    "                z = original_shape[0]*original_shape[1]    \n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            vals = []\n",
    "            lengths = []\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                lengths.append(weights[layer_to_prune].shape[0])\n",
    "                vals = vals + [np.sum(np.abs(row)) / len(row) for row in weights[layer_to_prune]]\n",
    "            no_of_rows_to_prune = int(ratio * len(vals))\n",
    "            rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "            for i, layer_to_prune in enumerate(dense_layers_to_prune):\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    if row_to_prune in range(int(np.sum(lengths[:i])), int(np.sum(lengths[:i+1]))):\n",
    "                        weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)\n",
    "                        weights[layer_to_prune + 2][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)                \n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return False\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 18s 38ms/step - loss: 1.6374 - accuracy: 0.8581 - val_loss: 1.5225 - val_accuracy: 0.9463\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x16875ce80>"
      ]
     },
     "execution_count": 307,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomConvModel()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "              \n",
    "             )\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )\n",
    "#model.save('./saved-models/mini-pipeline-CNN-baseline-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.055078713099161784, 0.05194876194000244, 0.05243547757466634, 0.05825615326563517, 0.058940446376800536, 0.054818495114644365, 0.05182290474573771, 0.05950473546981812, 0.05782819588979085, 0.05222630500793457, 0.044928797086079914, 0.06369850635528565, 0.07096808751424154, 0.052987829844156904, 0.051033949851989745, 0.05110149383544922, 0.05787749290466308, 0.058394261201222736, 0.04840754270553589, 0.07032877604166667, 0.0569369117418925, 0.048041065533955894, 0.054148324330647785, 0.054701709747314455, 0.06016249656677246, 0.053537468115488686, 0.06361176172892252, 0.05273702144622803, 0.052713934580485025, 0.057096306482950845, 0.05192641814549764, 0.05094271500905355, 0.051139338811238604, 0.06254170735677084, 0.04987547397613525, 0.05601600408554077, 0.05014275312423706, 0.055619581540425615, 0.05105504989624023, 0.05408846139907837, 0.056754199663798015, 0.06746216615041097, 0.056343328952789304, 0.06914838949839273, 0.06787993907928466, 0.04632422924041748, 0.06881059010823568, 0.05366703669230143, 0.0447201132774353, 0.06684943834940592, 0.05260312557220459, 0.05803537368774414, 0.047215716044108076, 0.06395613352457682, 0.05232075055440267, 0.052578314145406084, 0.05686047871907552, 0.0653207500775655, 0.06544831593831381, 0.06963222026824951, 0.06749451160430908, 0.05816073815027873, 0.05811443328857422, 0.05178589820861816, 0.060254422823588054, 0.058175428708394365, 0.06562473773956298, 0.05247944196065267, 0.04900306065877279, 0.055110673109690346, 0.05965835650761922, 0.0469368855158488, 0.04925942023595174, 0.054779303073883054, 0.0698915958404541, 0.06427746216456096, 0.06498010158538818, 0.06517197688420613, 0.055704041322072344, 0.05549939473470052, 0.07210721969604492, 0.06275705496470134, 0.059113856156667074, 0.06495473384857178, 0.05308109919230143, 0.05296398003896077, 0.0581103523572286, 0.04686037302017212, 0.05241897503534953, 0.047505974769592285, 0.06345520814259847, 0.05715783437093099, 0.06840929985046387, 0.07083730697631836, 0.05745824178059896, 0.04291355609893799, 0.07178994019826253, 0.0724952220916748, 0.059722089767456056, 0.051892884572347, 0.05695850849151611, 0.04762712319691976, 0.06460550626118979, 0.060939133167266846, 0.041621128718058266, 0.05255930821100871, 0.072585129737854, 0.05388139883677165, 0.07484761079152426, 0.0587425708770752, 0.05206678708394368, 0.05544909636179606, 0.08411428133646646, 0.056371275583902994, 0.05144882996877034, 0.04954641660054525, 0.0771162192026774, 0.05950926542282105, 0.06010928948720296, 0.056999492645263675, 0.04504451751708984, 0.048413753509521484, 0.07645088831583659, 0.06890880266825358, 0.06700634161631266, 0.06364030440648397, 0.07192869981129964, 0.06143385569254557, 0.0712620496749878, 0.0649619738260905, 0.06159274578094483, 0.04063862959543864, 0.04869121313095093, 0.05686709880828857, 0.08413381576538086, 0.0469531774520874, 0.05334761142730713, 0.04688280423482259, 0.08096752166748047, 0.06225650707880656, 0.06055601437886556, 0.061425638198852536, 0.06327190399169921, 0.04801090558369955, 0.06312602361043294, 0.057286985715230304, 0.06147128740946452, 0.054673806826273603, 0.05745499928792318, 0.0588830312093099, 0.06626944541931153, 0.05116174221038818, 0.07510643005371094, 0.053459604581197105, 0.05680093765258789, 0.06471570332845052, 0.05190703868865967, 0.060790054003397626, 0.06124189297358195, 0.055684566497802734, 0.054410072167714436, 0.05912950833638509, 0.0451172669728597, 0.054558181762695314, 0.0478888193766276, 0.04732500712076823, 0.056560428937276204, 0.04896640380223592, 0.056029987335205075, 0.04568184614181518, 0.06157396634419759, 0.06399752298990885, 0.060004806518554686, 0.059908239046732585, 0.05434439182281494, 0.04745616912841797, 0.06339506308237712, 0.07237441539764404, 0.043310240904490156, 0.0664576013882955, 0.06545406182607015, 0.043048731486002606, 0.058545462290445965, 0.05554872751235962, 0.04846954345703125, 0.057882467905680336, 0.04614519278208415, 0.05306451718012492, 0.05958841244379679, 0.06650102138519287, 0.06312286853790283, 0.05497411092122396, 0.06571104129155476, 0.06412223974863689, 0.04594673315684001, 0.05199726422627767, 0.06638394196828207, 0.05085185368855794, 0.05924302339553833, 0.07041230201721191, 0.053628687063852945, 0.07087890307108562, 0.05686306158701579, 0.07152881622314453, 0.06093360582987468, 0.06396986643473307, 0.06668790181477864, 0.05864423910776774, 0.05739057461420695, 0.06430946985880534, 0.054283873240153, 0.045264116923014325, 0.04890351295471192, 0.058861184120178225, 0.07246827284495036, 0.04778350194295247, 0.05364943742752075, 0.05390397707621256, 0.07346395651499431, 0.06379994551340738, 0.04712309837341309, 0.04782665967941284, 0.0506659189860026, 0.04614240328470866, 0.06354244550069173, 0.05171395142873128, 0.06458350817362467, 0.0622294823328654, 0.054185891151428224, 0.05369322299957276, 0.06368595361709595, 0.05829694668451945, 0.06507161060969034, 0.05116614898045858, 0.05133272012074788, 0.05984498659769694, 0.05236918528874715, 0.05608059167861938, 0.04653769334157308, 0.04888722499211629, 0.0681822935740153, 0.05260227123896281, 0.055081995328267415, 0.0516875425974528, 0.05897382895151774, 0.05254035790761312, 0.06883346239725749, 0.058659827709198, 0.050849982102711994, 0.04077288309733073, 0.0630385398864746, 0.06365791161855062, 0.05754736264546712, 0.05911573568979899, 0.06049004395802816, 0.056157890955607095, 0.06125795841217041, 0.06800165971120199, 0.05925054947535197, 0.050200764338175455, 0.059955906867980954, 0.0477825403213501, 0.04620467821756999, 0.04978048801422119, 0.05161726474761963, 0.056878193219502764, 0.0474017858505249, 0.06602279742558798, 0.0708399772644043, 0.062257758776346844, 0.060464914639790854, 0.05392361084620158, 0.060351896286010745, 0.06184778610865275, 0.04753318230311076, 0.042318673928578694, 0.044749343395233156, 0.04412382046381633, 0.0582806666692098, 0.04927563667297363, 0.056112051010131836, 0.05859185854593913, 0.056745203336079915, 0.050914200146993, 0.06140833695729574, 0.05628443956375122, 0.06136652231216431, 0.0584014892578125, 0.06784617106119792, 0.057416848341623944, 0.0559612234433492, 0.05663132667541504, 0.04788493315378825, 0.047804145018259685, 0.05600146849950154, 0.04204392433166504, 0.06066723267237346, 0.04627486864725749, 0.05743362903594971, 0.06529637575149536, 0.04711243708928426, 0.0564091165860494, 0.05093507766723633, 0.04375489155451457, 0.06598176558812459, 0.06162694692611694, 0.06889858245849609, 0.06930123170216879, 0.0540785551071167, 0.05006047089894613, 0.060812540849049884, 0.05159307320912679, 0.07354849974314372, 0.045570119222005205, 0.05043551921844482, 0.05544602870941162, 0.04731506506601969, 0.05437928835550944, 0.05091630617777507, 0.05861569245656331, 0.05384891033172608, 0.061890141169230146, 0.06068418025970459, 0.05810839335123698, 0.05429731210072835, 0.05510973930358887, 0.05916963418324788, 0.055414708455403645, 0.061358869075775146, 0.05248080889383952, 0.06546881198883056, 0.0614982803662618, 0.04921460151672363, 0.05420010089874268, 0.05263833204905192, 0.054887795448303224, 0.05220687389373779, 0.060624345143636064, 0.0681302785873413, 0.05458854039510091, 0.04225911299387614, 0.06568989753723145, 0.06420389811197917, 0.04498190482457479, 0.058066948254903154, 0.05206499099731445, 0.05532185633977254, 0.06452687581380208, 0.047113196055094404, 0.055017558733622234, 0.051112580299377444, 0.049593889713287355, 0.051191937923431394, 0.05714641412099202, 0.061966562271118165, 0.05146946509679159, 0.056975142161051436, 0.04980399608612061, 0.058541738986968996, 0.050432928403218585, 0.04004480044047038, 0.052089524269104, 0.04531383514404297, 0.050792221228281656, 0.0524101734161377, 0.05664541721343994, 0.042795991897583006, 0.04975115458170573, 0.057105088233947755, 0.061775847276051836, 0.05638496081034342, 0.056110866864522296, 0.06846693356831869, 0.052163036664326985, 0.04813025792439778, 0.06078306039174398, 0.054391602675120033, 0.05077661673227946, 0.044714800516764325, 0.05036328633626302, 0.04566996494928996, 0.062426984310150146, 0.06188247203826904, 0.05394398768742879, 0.05480713446935018, 0.059658360481262204, 0.04998650948206584, 0.05921049118041992, 0.055863690376281736, 0.05586017370223999, 0.06270969708760579, 0.04960085948308309, 0.07458977699279785, 0.05274856090545654, 0.051586302121480306, 0.058330762386322024, 0.04647241433461507, 0.051125343640645346, 0.05855692227681478, 0.053492188453674316, 0.07701630251748222, 0.06940721330188569, 0.07213068008422852, 0.07192413012186687, 0.05928818384806315, 0.07627068814777192, 0.07577900659470331, 0.0789177417755127, 0.07959262530008952, 0.06279925505320232, 0.08290412312462217, 0.080645828020005, 0.06900299730755034, 0.059578600383940195, 0.07378532205309186, 0.07753674756912958, 0.07254951340811593, 0.07854158537728446, 0.08401945659092494, 0.08810278347560338, 0.08303081421625047, 0.06467645508902413, 0.07645245960780553, 0.058319750286283945, 0.05978531496865409, 0.05448582058861142, 0.07544843355814616, 0.08602751436687651, 0.07182109355926514, 0.07733259314582461, 0.08517364660898845, 0.07958304314386278, 0.07807636260986328, 0.08173151243300665, 0.07753071330842518, 0.07750985735938662, 0.0820005734761556, 0.06518488838559106, 0.07635272684551421, 0.07063827628181094, 0.06254244986034575, 0.071652542977106, 0.08015562239147368, 0.07005204473223005, 0.0691096271787371, 0.06550317718869164, 0.08063943613143194, 0.06910788445245653, 0.07025720959617979, 0.07515694413866315, 0.07900433313278925, 0.0643044596626645, 0.05748727208092099, 0.07646960871560234, 0.07147430805932908, 0.07562434105646043, 0.08785228502182733, 0.07741665272485643, 0.07048154444921584, 0.069190399987357, 0.06581100395747594, 0.07536483378637404, 0.1010547365461077, 0.06979607400440034, 0.07148370288667225, 0.07810509772527785, 0.0777433713277181, 0.06561909403119769, 0.08224674065907796, 0.07068746998196557, 0.06371923855372838, 0.07394345033736456, 0.07057795070466541, 0.0888528823852539, 0.0737112703777495, 0.08586901142483666, 0.06676046053568523, 0.06267674196334112, 0.06152905736650739, 0.06201518149602981, 0.08211844308035714, 0.09022619610741026, 0.07416590054829915, 0.07066768691653297, 0.08314341590518043, 0.08480729375566755, 0.0733003162202381, 0.07900675705501012, 0.0738890568415324, 0.0766037248429798, 0.0837302945909046, 0.08380507287524995, 0.06747650532495408, 0.07246248495011103, 0.06837302730196998, 0.06347220284598214, 0.07504643712724958, 0.07652009101141066, 0.06980493522825695, 0.06013166336786179, 0.08584430671873547, 0.08643993877229236, 0.07856392860412598, 0.06547925585792178, 0.07659715697878883, 0.07324164254324776, 0.07749851544698079, 0.07274318309057326, 0.08388322307949975, 0.07659414836338588, 0.059494881402878536, 0.07632791428338914, 0.07205147402627128, 0.06874983651297432, 0.07170794691358294, 0.07926359063103086, 0.06588058812277657, 0.07226480188823882, 0.08361174946739561, 0.0705243916738601, 0.1353389024734497, 0.1433386445045471, 0.12849869728088378, 0.14186737537384034, 0.134263014793396, 0.13495641946792603, 0.12477223873138428, 0.14003410339355468, 0.14326834678649902, 0.15396561622619628, 0.13178321123123168, 0.14001322984695436, 0.1386794328689575, 0.1374420166015625, 0.10855714082717896, 0.12660821676254272, 0.12693560123443604, 0.10992048978805542, 0.11234687566757202, 0.13216265439987182, 0.12820167541503907, 0.1498160719871521, 0.15097471475601196, 0.16673924922943115, 0.13539252281188965, 0.14689066410064697, 0.12827643156051635, 0.13938409090042114, 0.13538000583648682, 0.1452454686164856, 0.16708898544311523, 0.15304869413375854, 0.12439559698104859, 0.14201682806015015, 0.15938248634338378, 0.13755136728286743, 0.14796888828277588, 0.14349617958068847, 0.11003379821777344, 0.1200607419013977, 0.13225327730178832, 0.15248850584030152, 0.15221941471099854, 0.15396227836608886, 0.16755188703536988, 0.15473860502243042, 0.13973373174667358, 0.1420575976371765, 0.13338245153427125, 0.11750253438949584, 0.17342605590820312, 0.14209387302398682, 0.10954296588897705, 0.15661829710006714, 0.13633058071136475, 0.1378672242164612, 0.10097291469573974, 0.13960373401641846, 0.14245359897613524, 0.14572972059249878, 0.12326183319091796, 0.15058062076568604, 0.1258089542388916, 0.1426430344581604, 0.13456077575683595, 0.11941975355148315, 0.16122665405273437, 0.13208208084106446, 0.13738857507705687, 0.11777405738830567, 0.14425302743911744, 0.10919559001922607, 0.16679415702819825, 0.139176607131958, 0.1433345317840576, 0.1514446496963501, 0.1525279998779297, 0.12522008419036865, 0.142475688457489, 0.16985340118408204, 0.13683729171752929, 0.14586790800094604, 0.14368301630020142, 0.14045403003692628]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.014397266959492436, 885, 61470)"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_zeros_ratio(model.get_weights())\n",
    "model.prune_magnitude_global_struct(.9)\n",
    "get_zeros_ratio(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50 150\n",
      "6 6\n",
      "50 150\n",
      "225 2400\n",
      "16 16\n",
      "225 2400\n",
      "6960 48000\n",
      "120 120\n",
      "6960 48000\n",
      "252 10080\n",
      "84 84\n",
      "252 10080\n",
      "0 840\n",
      "10 10\n",
      "0 840\n"
     ]
    }
   ],
   "source": [
    "xx = model.get_weights()\n",
    "for layer in xx:\n",
    "    print(np.count_nonzero(layer),len(layer.flatten()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "              \n",
    "             )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[2.3025097846984863, 0.10320000350475311]"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test, y_test, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 2.3023 - accuracy: 0.1118 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
      "Epoch 2/5\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/5\n",
      "469/469 [==============================] - 19s 40ms/step - loss: 2.3017 - accuracy: 0.1124 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
      "Epoch 4/5\n",
      "469/469 [==============================] - 17s 37ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 5/5\n",
      "469/469 [==============================] - 18s 39ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x167f71ee0>"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=5,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.014397266959492436, 885, 61470)"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_zeros_ratio(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125 150\n",
      "6 6\n",
      "125 150\n",
      "150 2400\n",
      "16 16\n",
      "150 2400\n",
      "0 48000\n",
      "120 120\n",
      "0 48000\n",
      "0 10080\n",
      "84 84\n",
      "0 10080\n",
      "610 840\n",
      "10 10\n",
      "610 840\n"
     ]
    }
   ],
   "source": [
    "xx = model.get_weights()\n",
    "for layer in xx:\n",
    "    print(np.count_nonzero(layer),len(layer.flatten()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
