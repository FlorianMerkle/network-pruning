{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'VGG'\n",
    "EXPERIMENT_TYPE = 'fixed-eps'\n",
    "ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import tensorflow as tf\n",
    "\n",
    "from helperfiles import *\n",
    "import helperfiles.experiment as experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "import gc\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "from secrets import randbelow\n",
    "import foolbox as fb\n",
    "from datetime import datetime\n",
    "from helperfiles.helpers import load_data, initialize_base_model, get_zeros_ratio, train_model, compile_model, bb0_attack, pgd_attack,cw2_attack, plot_hist\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "\n",
    "def run(structure, method, scope, iterations, architecture, experiment_type):\n",
    "    \n",
    "    if architecture == 'ResNet' or architecture == 'VGG':\n",
    "        ds_train, ds_test, attack_images, attack_labels = load_data(\"imagenette\")\n",
    "    if architecture == 'MLP' or architecture == 'CNN':\n",
    "        ds_train, ds_test, attack_images, attack_labels = load_data(\"mnist\")\n",
    "\n",
    "\n",
    "    experiment_name = f'{experiment_type}-{architecture}-{method}-{scope}-{structure}'\n",
    "    cols = ['iteration','experiment_name','structure','method','scope','pruning_ratio','accuracy','loss','pgd_linf','cw_l2','bb_l0', 'total_params', 'params_left']\n",
    "    results = pd.DataFrame(columns=cols, dtype='object')\n",
    "    pgd_success_rates = []\n",
    "    cw_success_rates = []\n",
    "    bb0_success_rates = []\n",
    "    all_accuracies = []\n",
    "\n",
    "\n",
    "    compression_rates = [tf.math.pow(2, x).numpy() for x in range(7)]\n",
    "    pruning_ratios = [1-1/x for x in compression_rates]\n",
    "\n",
    "\n",
    "    for j in tqdm(range(iterations)):\n",
    "        accuracies = []\n",
    "        pgd_success_rate = []\n",
    "        cw_success_rate = []\n",
    "        bb0_success_rate = []\n",
    "\n",
    "\n",
    "        try: \n",
    "            \n",
    "            del model\n",
    "            gc.collect()\n",
    "            print('deleted model')\n",
    "        except:\n",
    "            print('no model to delete')\n",
    "            pass\n",
    "        tf.keras.backend.clear_session()\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        gc.collect()\n",
    "\n",
    "        model = initialize_base_model(architecture, ds_train, j ,experiment_name=experiment_name, lr=1e-3, )\n",
    "        for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "            print(f'current pruning ratio is{pruning_ratio}, current iteration is {j}')\n",
    "            gc.collect()\n",
    "            if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                model.prune_random_global_unstruct(pruning_ratio)\n",
    "            elif  method=='random' and scope=='global' and structure=='structured':\n",
    "                model.prune_random_global_struct(pruning_ratio)\n",
    "            elif  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                model.prune_random_local_unstruct(pruning_ratio)\n",
    "            elif  method=='random' and scope=='local' and structure=='structured':\n",
    "                model.prune_random_local_struct(pruning_ratio)\n",
    "            elif  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                    model.prune_magnitude_global_unstruct(pruning_ratio)\n",
    "            elif  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                model.prune_magnitude_global_struct(pruning_ratio)\n",
    "            elif  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                model.prune_magnitude_local_unstruct(pruning_ratio)\n",
    "            elif  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                model.prune_magnitude_local_struct(pruning_ratio)\n",
    "            else:\n",
    "                raise ValueError(\"pruning method invalid\")\n",
    "\n",
    "            zeros_ratio, non_zeros, param_count = get_zeros_ratio(model)\n",
    "            compile_model(architecture, model, lr=1e-3)\n",
    "            \n",
    "            hist = train_model(architecture, ds_train, ds_test, model, to_convergence=True)\n",
    "            zeros_ratio, non_zeros, param_count = get_zeros_ratio(model)\n",
    "            if architecture == 'ResNet' or architecture=='VGG':\n",
    "                res = model.evaluate(ds_test,verbose=0)\n",
    "            if architecture == 'CNN' or architecture=='MLP':\n",
    "                res = model.evaluate(ds_test[0], ds_test[1],verbose=0)\n",
    "            plot_hist(hist)\n",
    "\n",
    "\n",
    "            if res[1] > .40:\n",
    "                #pass\n",
    "                bb0_success = bb0_attack(architecture, model, attack_images, attack_labels)\n",
    "            else: \n",
    "                bb0_success = 'not successful'\n",
    "            vals = {\n",
    "                'iteration':j,\n",
    "                'experiment_name':experiment_name,\n",
    "                'structure':structure,\n",
    "                'method':method,\n",
    "                'scope':scope,\n",
    "                'pruning_ratio':pruning_ratio,\n",
    "                'accuracy':res[1],\n",
    "                'loss':res[0],\n",
    "                'pgd_linf':pgd_attack(architecture, model, attack_images, attack_labels),\n",
    "                'cw_l2':cw2_attack(architecture, model, attack_images, attack_labels),\n",
    "                'bb_l0':bb0_success,\n",
    "                'total_params':param_count,\n",
    "                'params_left':non_zeros\n",
    "            }\n",
    "            results = results.append(pd.DataFrame([vals], index=[0], dtype='object'))\n",
    "            results.to_pickle(f'./final-results/{experiment_name}.pkl')\n",
    "            results.to_csv(f'./final-results/{experiment_name}.csv', index=False)\n",
    "\n",
    "\n",
    "    results.to_pickle(f'./final-results/{experiment_name}.pkl')\n",
    "    results.to_csv(f'./final-results/{experiment_name}.csv', index=False)\n",
    "   \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no model to delete\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/10 [00:17<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[128,512,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node vg_g11/batch_normalization_4/FusedBatchNormV3 (defined at <ipython-input-8-0449ba79666c>:178) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1221732]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node vg_g11/batch_normalization_4/FusedBatchNormV3:\n vg_g11/custom_conv_layer_4/Relu (defined at <ipython-input-8-0449ba79666c>:77)\n\nFunction call stack:\ntrain_function\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-16-b23bc09b19cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m run(\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mstructure\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'unstructured'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'magnitude'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mscope\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'local'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-8c5d851812c3>\u001b[0m in \u001b[0;36mrun\u001b[0;34m(structure, method, scope, iterations, architecture, experiment_type)\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mgc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 63\u001b[0;31m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minitialize_base_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0marchitecture\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mds_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mj\u001b[0m \u001b[0;34m,\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mexperiment_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     64\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpruning_ratio\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpruning_ratios\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'current pruning ratio is{pruning_ratio}, current iteration is {j}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-9-50e7ac6592fe>\u001b[0m in \u001b[0;36minitialize_base_model\u001b[0;34m(architecture, ds, index, experiment_name, lr, save_weights)\u001b[0m\n\u001b[1;32m    137\u001b[0m             \u001b[0mexperimental_run_tf_function\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m         )\n\u001b[0;32m--> 139\u001b[0;31m         model.fit(\n\u001b[0m\u001b[1;32m    140\u001b[0m             \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mds\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36m_method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     64\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_method_wrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_in_multi_worker_mode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m     \u001b[0;31m# Running inside `run_distribute_coordinator` already.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    846\u001b[0m                 batch_size=batch_size):\n\u001b[1;32m    847\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 848\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    849\u001b[0m               \u001b[0;31m# Catch OutOfRangeError for Datasets of unknown size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m               \u001b[0;31m# This blocks until the batch has finished executing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    642\u001b[0m         \u001b[0;31m# Lifting succeeded, so variables are initialized and we can run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;31m# stateless function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 644\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    645\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    646\u001b[0m       \u001b[0mcanon_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcanon_kwds\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1659\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0margs\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1660\u001b[0m     \"\"\"\n\u001b[0;32m-> 1661\u001b[0;31m     return self._call_flat(\n\u001b[0m\u001b[1;32m   1662\u001b[0m         (t for t in nest.flatten((args, kwargs), expand_composites=True)\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1743\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1745\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1746\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    591\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    592\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 593\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    594\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    595\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/master-thesis/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[128,512,28,28] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node vg_g11/batch_normalization_4/FusedBatchNormV3 (defined at <ipython-input-8-0449ba79666c>:178) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_train_function_1221732]\n\nErrors may have originated from an input operation.\nInput Source operations connected to node vg_g11/batch_normalization_4/FusedBatchNormV3:\n vg_g11/custom_conv_layer_4/Relu (defined at <ipython-input-8-0449ba79666c>:77)\n\nFunction call stack:\ntrain_function\n"
     ]
    }
   ],
   "source": [
    "run(\n",
    "    \n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS,\n",
    "    architecture = ARCHITECTURE,\n",
    "    experiment_type = EXPERIMENT_TYPE,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "from secrets import randbelow\n",
    "import foolbox as fb\n",
    "from datetime import datetime\n",
    "\n",
    "import helperfiles.helpers as helpers\n",
    "\n",
    "shapes = {\n",
    "\n",
    "    'conv_1': (3, 3, 3, 64),\n",
    "    'conv_2': (3, 3, 64, 128),\n",
    "    'conv_3': (3, 3, 128, 256),\n",
    "    'conv_4': (3, 3, 256, 256),\n",
    "    'conv_5': (3, 3, 256, 512),\n",
    "    'conv_6': (3, 3, 512, 512),\n",
    "    'conv_7': (3, 3, 512, 512),\n",
    "    'conv_8': (3, 3, 512, 512),\n",
    "    'dense_1': (7*7*512, 4096),\n",
    "    'dense_2': (4096, 1024),\n",
    "    'dense_3': (1024, 10),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, stride, padding='SAME'):\n",
    "        \n",
    "        #super(CustomConvLayer, self).__init__()\n",
    "        #self.w = weights\n",
    "        #self.m = mask\n",
    "        #self.b = biases\n",
    "        #self.s = strides\n",
    "        #self.p = padding\n",
    "        #self.bn = layers.BatchNormalization()\n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='glorot_uniform',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        if self.bias==True:\n",
    "            self.b = self.add_weight(\n",
    "                shape=shape[-1],\n",
    "                initializer='zeros',\n",
    "                trainable=True,\n",
    "                name='b'\n",
    "            )\n",
    "        self.s = stride\n",
    "        self.p = padding\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)\n",
    "        if self.bias == True:\n",
    "            x = tf.nn.bias_add(x, self.b)\n",
    "        #x = self.bn(x)\n",
    "        return tf.nn.relu(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='SAME'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return tf.nn.max_pool2d(inputs, ksize=[1, self.k, self.k, 1], strides=[1, self.k, self.k, 1], padding=self.p)\n",
    "    \n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'relu'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.bias = bias\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        if self.bias == True:\n",
    "            self.b = self.add_weight(\n",
    "                shape = (shape[-1]),\n",
    "                initializer = 'zeros',\n",
    "                trainable = True,\n",
    "                name='b'\n",
    "            )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        if self.bias == True:\n",
    "            x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'relu':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        \n",
    "class VGG11(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(VGG11, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], False, 1,)\n",
    "        self.maxpool1 = CustomPoolLayer(k=2)\n",
    "        self.bn1 = layers.BatchNormalization()\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], False, 1,)\n",
    "        self.maxpool2 = CustomPoolLayer(k=2)\n",
    "        self.bn2 = layers.BatchNormalization()\n",
    "        self.conv3 = CustomConvLayer(shapes['conv_3'], False, 1,)\n",
    "        self.bn3 = layers.BatchNormalization()\n",
    "        self.conv4 = CustomConvLayer(shapes['conv_4'], False, 1,)\n",
    "        self.maxpool3 = CustomPoolLayer(k=2)\n",
    "        self.bn4 = layers.BatchNormalization()\n",
    "        self.conv5 = CustomConvLayer(shapes['conv_5'], False, 1,)\n",
    "        self.bn5 = layers.BatchNormalization()\n",
    "        self.conv6 = CustomConvLayer(shapes['conv_6'], False, 1,)\n",
    "        self.maxpool4 = CustomPoolLayer(k=2)\n",
    "        self.bn6 = layers.BatchNormalization()\n",
    "        self.conv7 = CustomConvLayer(shapes['conv_7'], False, 1,)\n",
    "        self.bn7 = layers.BatchNormalization()\n",
    "        self.conv8 = CustomConvLayer(shapes['conv_8'], False, 1,)\n",
    "        self.maxpool5 = CustomPoolLayer(k=2)\n",
    "        self.bn8 = layers.BatchNormalization()\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], True, 'relu')\n",
    "        #self.bn9 = layers.BatchNormalization()\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], True, 'relu')\n",
    "        #self.bn10 = layers.BatchNormalization()\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, 'softmax')\n",
    "        self.conv_layers = [0, 6, 12, 18, 24, 30, 36, 42]\n",
    "        self.conv_masks = [1, 7, 13, 19, 25, 31, 37, 43]\n",
    "        self.dense_layers = [48, 51, 54]\n",
    "        self.dense_masks = [50, 53, 56]\n",
    "        #self.conv_layers = []\n",
    "        #self.conv_masks = []\n",
    "        #self.dense_layers = []\n",
    "        #self.dense_masks = []\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.conv1(inputs)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.maxpool3(x)\n",
    "        x = self.bn4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = self.maxpool4(x)\n",
    "        x = self.bn6(x)\n",
    "        x = self.conv7(x)\n",
    "        x = self.bn7(x)\n",
    "        x = self.conv8(x)\n",
    "        x = self.maxpool5(x)\n",
    "        x = self.bn8(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x = self.dense3(x)\n",
    "        return x\n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer in enumerate(self.conv_layers):\n",
    "                #shape = 3,3,64,128\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[self.conv_masks[i]]).numpy()\n",
    "                #shape = 128,64, 3,3\n",
    "                layer_shape = weights[layer].shape\n",
    "                flat_masks = converted_mask.flatten()\n",
    "                no_of_weighs_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                non_zero_weights = np.nonzero(flat_weights)[0]\n",
    "                no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                random.shuffle(non_zero_weights)\n",
    "                indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                converted_mask = flat_masks.reshape(layer_shape)\n",
    "                converted_weights = flat_weights.reshape(layer_shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[self.conv_masks[i]] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "            \n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "#            for index, weight in enumerate(weights):\n",
    "            for i, layer in enumerate(self.dense_layers):\n",
    "#                if index in dense_layer_to_prune:\n",
    "                    shape = weights[layer].shape\n",
    "                    flat_weights = weights[layer].flatten()\n",
    "                    flat_mask = weights[self.dense_masks[i]].flatten()\n",
    "                    no_of_weighs_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[self.dense_masks[i]] = mask_reshaped\n",
    "                    weights[layer] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        weights = prune_conv_layers_locally(self, ratio)\n",
    "        weights = prune_dense_layers_locally(self,ratio)\n",
    "        return True\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        all_masks = self.conv_masks + self.dense_masks\n",
    "        for i, x in enumerate(self.conv_layers + self.dense_layers):\n",
    "            flat_weights = np.append(flat_weights, weights[x].flatten())\n",
    "            flat_mask = np.append(flat_mask, weights[all_masks[i]].flatten())\n",
    "            \n",
    "        no_of_weights_to_prune = int(np.round(len(flat_weights)*ratio))\n",
    "        #print('total weights',len(flat_weights))\n",
    "        #print('weights to prune w/o round',int(len(flat_weights)*ratio))\n",
    "        #print('weights to prune with round',int(np.round(len(flat_weights)*ratio)))\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for i, x in enumerate(self.conv_layers + self.dense_layers):\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[all_masks[i]] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def prune_random_local_struct(self, ratio, prune_dense_layers=False):\n",
    "        def prune_conv_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer in enumerate(self.conv_layers):\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[layer])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[self.conv_masks[i]])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                #print(channels)\n",
    "                non_zero_channels = np.nonzero([np.sum(channel) for channel in channels])[0]\n",
    "                #print(non_zero_channels)\n",
    "                no_of_channels_to_prune_left = no_of_channels_to_prune - (len(channels) - len(non_zero_channels))\n",
    "                random.shuffle(non_zero_channels)\n",
    "                channels_to_prune = non_zero_channels[:no_of_channels_to_prune_left]\n",
    "                mask = tf.reshape(iohw_mask, \n",
    "                                  (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[layer] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[self.conv_masks[i]] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        def prune_dense_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(np.round(ratio * len(weights[layer_to_prune])))\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                random.shuffle(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[self.dense_masks[i]][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers(self, ratio)\n",
    "        if prune_dense_layers==True:\n",
    "            prune_dense_layers(self, ratio)\n",
    "        \n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio, prune_dense_layers=False):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    \n",
    "    def prune_magnitude_local_struct(self, ratio, structure='channel'):\n",
    "        def prune_filters(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, x in enumerate(self.conv_layers):\n",
    "                # shape = (3,3,64,128)\n",
    "                vals = []\n",
    "                oihw_weights = convert_from_hwio_to_oihw(weights[x])\n",
    "                oihw_mask = convert_from_hwio_to_oihw(weights[self.conv_masks[i]])\n",
    "                # shape = (128,64,3,3)\n",
    "                converted_shape = oihw_weights.shape\n",
    "                no_of_filters = converted_shape[0]\n",
    "                no_of_filters_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                for single_filter in oihw_weights:\n",
    "                    #shape of single_filter = (64,3,3)\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(single_filter)))\n",
    "                filters_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for filters_to_prune in no_of_filters_to_prune:\n",
    "                    oihw_weights[filters_to_prune] = tf.zeros([converted_shape[1], converted_shape[2], converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[1], converted_shape[2], converted_shape[3]])\n",
    "\n",
    "                 # shape = (128,64,3,3)\n",
    "                weights[x] = convert_from_oihw_to_hwio(oihw_weights)\n",
    "                weights[self.conv_masks[i]] = convert_from_oihw_to_hwio(mask)\n",
    "                 # shape = (64,128,3,3)\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_channels(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, x in enumerate(self.conv_layers):\n",
    "                # shape = (3,3,64,128)\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[self.conv_masks[i]])\n",
    "                # shape = (64,128,3,3)\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                # shape = (8192,3,3)\n",
    "                for channel in channels:\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(channel)))\n",
    "                channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[self.conv_masks[i]] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        def prune_dense_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                no_of_rows_to_prune = int(np.round(ratio * len(weights[layer_to_prune])))\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[self.dense_masks[i]][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        if structure == 'channel':\n",
    "            prune_channels(self,ratio)\n",
    "        if structure == 'filter':\n",
    "            prune_filter(self,ratio)\n",
    "        \n",
    "        if prune_dense_layers==True:\n",
    "            prune_dense_layers(self, ratio)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "    \n",
    "    \n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio, prune_dense_layers=False,structure='channel'\n",
    "                                     ):\n",
    "        def prune_filters(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            all_filters = []\n",
    "            all_masks = []\n",
    "            vals = []\n",
    "            for i, layer_to_prune in enumerate(self.conv_layers):\n",
    "                # convert from e.g. (3,3,64,128) to (128,64,3,3)\n",
    "                oihw_weights = convert_from_hwio_to_oihw(weights[layer_to_prune])\n",
    "                oihw_mask = convert_from_hwio_to_oihw(weights[self.conv_masks[i]])\n",
    "                converted_shape = oihw_weights.shape\n",
    "                no_of_filters = converted_shape[0]\n",
    "                \n",
    "                #calculate average magnitude for each filter\n",
    "                vals = vals + [np.sum(np.abs(single_filter)) / np.prod(single_filter.shape) for single_filter in oihw_weights]\n",
    "                all_filters = list(all_filters) +  list(oihw_weights)\n",
    "                all_masks = list(all_masks) + list(oihw_mask)\n",
    "            no_of_filters_to_prune = int(np.round(ratio * len(vals)))\n",
    "            filters_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "            \n",
    "            for filter_to_prune in filters_to_prune:\n",
    "                all_filters[filter_to_prune] = tf.zeros(all_filters[filter_to_prune].shape) \n",
    "                all_masks[filter_to_prune] = tf.zeros(all_filters[filter_to_prune].shape) \n",
    "            \n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(self.conv_layers):\n",
    "                original_shape = convert_from_hwio_to_oihw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_filters[z:z + original_shape[0]], original_shape)\n",
    "                pruned_mask = tf.reshape(all_masks[z:z + original_shape[0]], original_shape)\n",
    "                weights[layer_to_prune] = convert_from_oihw_to_hwio(pruned_layer)\n",
    "                weights[self.conv_masks[i]] = convert_from_oihw_to_hwio(pruned_mask)\n",
    "                z = z + original_shape[0]\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_channels(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            all_channels = []\n",
    "            all_masks = []\n",
    "            vals = []\n",
    "            for layer_to_prune in self.conv_layers:\n",
    "                # convert from e.g. (3,3,1,6) to (1,6,3,3)\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[layer_to_prune])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                #convert from (1,6,3,3) to (6,3,3)\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                mask = np.ones((no_of_channels,converted_shape[2],converted_shape[3]))\n",
    "                #calculate average magnitude for each filter\n",
    "                vals = vals + [np.sum(np.abs(channel)) / np.prod(channel.shape) for channel in channels]\n",
    "                all_channels = list(all_channels) +  list(channels)\n",
    "                all_masks = list(all_masks) + list(mask)\n",
    "            no_of_channels_to_prune = int(np.round(ratio * len(vals)))\n",
    "            channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "            \n",
    "            for channel_to_prune in channels_to_prune:\n",
    "                all_channels[channel_to_prune] = tf.zeros(all_channels[channel_to_prune].shape) \n",
    "                all_masks[channel_to_prune] = tf.zeros(all_channels[channel_to_prune].shape) \n",
    "            \n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(self.conv_layers):\n",
    "                original_shape = convert_from_hwio_to_iohw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_channels[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                pruned_mask = tf.reshape(all_masks[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                weights[layer_to_prune] = convert_from_iohw_to_hwio(pruned_layer)\n",
    "                weights[self.conv_masks[i]] = convert_from_iohw_to_hwio(pruned_mask)\n",
    "                z = z + original_shape[0]*original_shape[1]\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            vals = []\n",
    "            lengths = []\n",
    "            for layer_to_prune in self.dense_layers:\n",
    "                lengths.append(weights[layer_to_prune].shape[0])\n",
    "                vals = vals + [np.sum(np.abs(row)) / len(row) for row in weights[layer_to_prune]]\n",
    "            no_of_rows_to_prune = int(np.round(ratio * len(vals)))\n",
    "            rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "            for i, layer_to_prune in enumerate(self.dense_layers):\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    if row_to_prune in range(int(np.sum(lengths[:i])), int(np.sum(lengths[:i+1]))):\n",
    "                        weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)\n",
    "                        \n",
    "                        weights[self.dense_masks[i]][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)                \n",
    "            self.set_weights(weights)        \n",
    "            return weights\n",
    "        if structure == 'filter':\n",
    "            prune_filters(self, ratio)\n",
    "        if structure == 'channel':\n",
    "            prune_channels(self, ratio)\n",
    "        \n",
    "        if prune_dense_layers==True:\n",
    "            prune_dense_layers(self, ratio)\n",
    "\n",
    "        return True\n",
    "    \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        \n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for layer_index, layer in enumerate(self.conv_layers):\n",
    "                #shape = 3,3,64,128\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[self.conv_masks[layer_index]]).numpy()\n",
    "                #shape = 128,64, 3,3\n",
    "                layer_shape = converted_weights.shape\n",
    "                flat_weights = converted_weights.flatten()\n",
    "                flat_masks = converted_mask.flatten()\n",
    "                no_of_weights_to_prune = int(np.round(ratio * len(flat_weights)))\n",
    "                indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                converted_mask = flat_masks.reshape(layer_shape)\n",
    "                converted_weights = flat_weights.reshape(layer_shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[self.conv_masks[layer_index]] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            weights = self.get_weights()\n",
    "            for index, layer in enumerate(self.dense_layers):\n",
    "                shape = weights[layer].shape\n",
    "                flat_weights = weights[layer].flatten()\n",
    "                flat_mask = weights[self.dense_masks[index]].flatten()\n",
    "\n",
    "                no_of_weights_to_prune = int(np.round(len(flat_weights)*ratio))\n",
    "                indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_mask[idx_to_delete] = 0\n",
    "                    flat_weights[idx_to_delete] = 0\n",
    "                mask_reshaped = flat_mask.reshape(shape)\n",
    "                weights_reshaped = flat_weights.reshape(shape)\n",
    "                weights[self.dense_masks[index]] = mask_reshaped\n",
    "                weights[layer] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return weights\n",
    "        \n",
    "        prune_conv_layers_locally(self,ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "        return True\n",
    "    \n",
    "    def find_layers_and_masks(self):\n",
    "        if len(self.conv_layers) != 0:\n",
    "            return True\n",
    "        for i, w in enumerate(self.get_weights()):\n",
    "            print(i ,'/', len(self.get_weights()))\n",
    "            if len(w.shape) == 4 and w.shape[0] != 1: \n",
    "                if np.all([x == 0 or x == 1 for x in w.flatten()[:100]]) == False: \n",
    "                    self.conv_layers.append(i)\n",
    "                else:\n",
    "                    self.conv_masks.append(i)\n",
    "            if len(w.shape) == 2: \n",
    "                if np.all([x == 0 or x == 1 for x in w.flatten()[:100]]) == False: \n",
    "                    self.dense_layers.append(i)\n",
    "                else:\n",
    "                    self.dense_masks.append(i)\n",
    "        return True\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import pathlib\n",
    "import os\n",
    "import random\n",
    "from secrets import randbelow\n",
    "import foolbox as fb\n",
    "from datetime import datetime\n",
    "\n",
    "import helperfiles.vgg11 as vgg\n",
    "import helperfiles.resnet as resnet\n",
    "import helperfiles.mlp as mlp\n",
    "import helperfiles.cnn as cnn\n",
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "def load_data(dataset,ratio='100%'):\n",
    "\n",
    "    def augment(image,label):\n",
    "        #image = tf.image.convert_image_dtype(image, tf.float32)\n",
    "        #image = tf.image.rot90(image, tf.random.uniform(shape=[], minval=0, maxval=4, dtype=tf.int32)) # random rotation\n",
    "        image = tf.image.random_flip_left_right(image)\n",
    "        #image = tf.image.random_flip_up_down(image)\n",
    "        #image = tf.image.random_hue(image, 0.08)\n",
    "        #image = tf.image.random_saturation(image, 0.6, 1.6)\n",
    "        #image = tf.image.random_contrast(image, 0.7, 1.3)\n",
    "        #image = tf.image.random_brightness(image, max_delta=0.5) # Random brightness\n",
    "        image = tf.image.resize_with_crop_or_pad(image, 224+60, 224+60) # Add 60 pixels of padding\n",
    "        image = tf.image.random_crop(image, size=[224,224,3]) # Random crop back to 28x28\n",
    "        return image,label\n",
    "    \n",
    "    @tf.function\n",
    "    def load_image(datapoint):\n",
    "        input_image, label = normalize(datapoint)\n",
    "        return input_image, label\n",
    "       \n",
    "    if dataset=='mnist':\n",
    "        \n",
    "        ds, info = tfds.load(name=dataset, with_info=True, split=[f\"train[:{ratio}]\",f\"test[:{ratio}]\"])\n",
    "        ds_train=ds[0]\n",
    "        ds_test=ds[1]\n",
    "        \n",
    "        def normalize(x):\n",
    "            y = {'image': tf.image.convert_image_dtype(x['image'], tf.float32), 'label': x['label']}\n",
    "            y = (tf.reshape(y['image'],(28*28,1)), y['label'])\n",
    "            return y\n",
    "        ds_test = list(ds_test.map(load_image))\n",
    "        ds_train = list(ds_train.map(load_image))\n",
    "\n",
    "        x_train = tf.convert_to_tensor([sample[0] for sample in ds_train])\n",
    "        y_train = tf.convert_to_tensor([sample[1] for sample in ds_train])\n",
    "        x_test = tf.convert_to_tensor([sample[0] for sample in ds_test])\n",
    "        y_test = tf.convert_to_tensor([sample[1] for sample in ds_test])\n",
    "\n",
    "        return [x_train, y_train], [x_test, y_test], x_test[:1000], y_test[:1000]\n",
    "        \n",
    "    if dataset=='imagenette':\n",
    "        ds, info = tfds.load(name=dataset, with_info=True, split=[f\"train[:{ratio}]\",f\"validation[:{ratio}]\"])\n",
    "        \n",
    "        ds_train=ds[0]\n",
    "        ds_test=ds[1]\n",
    "        def normalize(x):\n",
    "            y = {'image': tf.image.convert_image_dtype(x['image'], tf.float32), 'label': x['label']}\n",
    "            y = (tf.image.resize(y['image'], (224,224)), y['label'])\n",
    "            return y\n",
    "\n",
    "\n",
    "        num_train_examples= info.splits['train'].num_examples\n",
    "        BATCH_SIZE = 128\n",
    "\n",
    "        ds_train = (\n",
    "            ds_train\n",
    "            .map(normalize, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n",
    "            .take(num_train_examples)\n",
    "            .cache()\n",
    "            .shuffle(num_train_examples)\n",
    "            .map(augment, num_parallel_calls=AUTOTUNE)\n",
    "            .batch(BATCH_SIZE)\n",
    "            .prefetch(AUTOTUNE)\n",
    "        ) \n",
    "\n",
    "        ds_test = ds_test.map(\n",
    "            normalize, )\n",
    "        ds_test = ds_test.batch(BATCH_SIZE)\n",
    "        ds_test = ds_test.cache()\n",
    "        ds_test = ds_test.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n",
    "\n",
    "\n",
    "        attack_set = list(ds[1].map(load_image))[:256]\n",
    "\n",
    "        attack_images = tf.convert_to_tensor([sample[0] for sample in attack_set])\n",
    "        attack_labels = tf.convert_to_tensor([sample[1] for sample in attack_set])\n",
    "\n",
    "        return ds_train, ds_test, attack_images, attack_labels\n",
    "    \n",
    "    return False\n",
    "\n",
    "def compile_model(architecture, model, lr=1e-3):\n",
    "    model.compile(\n",
    "        loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "        metrics=['accuracy'],\n",
    "        experimental_run_tf_function=True\n",
    "    )\n",
    "    return True\n",
    "    \n",
    "\n",
    "def initialize_base_model(architecture, ds, index, experiment_name, lr=1e-3, save_weights=False):\n",
    "    if architecture == 'ResNet':\n",
    "        model = resnet.CustomResNetModel()\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            metrics=['accuracy'],\n",
    "            experimental_run_tf_function=True\n",
    "        )\n",
    "        model.fit(\n",
    "            x=ds,\n",
    "            epochs=1,\n",
    "        )\n",
    "    if architecture == 'VGG':\n",
    "        model = VGG11()\n",
    "        model.compile(\n",
    "            loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "            metrics=['accuracy'],\n",
    "            experimental_run_tf_function=True\n",
    "        )\n",
    "        model.fit(\n",
    "            x=ds,\n",
    "            epochs=1,\n",
    "        )\n",
    "    if architecture == 'CNN' :\n",
    "        model = cnn.CustomConvModel()\n",
    "        model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=lr),\n",
    "                      loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                      metrics=['accuracy'],\n",
    "                      experimental_run_tf_function=False\n",
    "                     )\n",
    "        model.fit(\n",
    "            x=ds[0],\n",
    "            y=ds[1],\n",
    "            batch_size=64,\n",
    "            epochs=1,\n",
    "        )\n",
    "    if architecture == 'MLP':\n",
    "        return 'not implemented yet'\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(architecture, ds_train, ds_test, model, to_convergence=True, epochs=5):\n",
    "    if architecture=='CNN':\n",
    "        if to_convergence == True:\n",
    "            epochs=500\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        hist = model.fit(\n",
    "            x=ds_train[0],\n",
    "            y=ds_train[1],\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(ds_test[0], ds_test[1]),\n",
    "        )\n",
    "        return hist\n",
    "    \n",
    "    if architecture=='ResNet' or architecture == 'VGG':\n",
    "\n",
    "        reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "            patience=12,\n",
    "            monitor='val_loss',\n",
    "            factor=.3,\n",
    "            min_lr=9e-5,\n",
    "            min_delta=0\n",
    "        )\n",
    "\n",
    "        early_stopping = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=15)\n",
    "        checkpoint_filepath = '/tmp/checkpoint'\n",
    "        model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "            filepath=checkpoint_filepath,\n",
    "            save_weights_only=True,\n",
    "            monitor='val_loss',\n",
    "            mode='max',\n",
    "            save_best_only=True)\n",
    "\n",
    "        if to_convergence == True:\n",
    "            epochs = 200\n",
    "\n",
    "        hist = model.fit(\n",
    "            x=ds_train,\n",
    "            epochs=epochs,\n",
    "            validation_data=ds_test,\n",
    "            callbacks=[reduce_lr, early_stopping, model_checkpoint_callback],\n",
    "        )\n",
    "        return hist\n",
    "\n",
    "def print_time(text=''):\n",
    "    now = datetime.now()\n",
    "    dt_string = now.strftime(\"%H:%M:%S\")\n",
    "    print(text, dt_string)\n",
    "\n",
    "def pgd_attack(architecture, model_to_attack, attack_images, attack_labels):\n",
    "    print_time(text='starting pgd')\n",
    "    BATCHSIZE = 64\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    if architecture == 'CNN' or architecture == 'MLP':\n",
    "        adversarials, _, success = attack(\n",
    "            fmodel,\n",
    "            attack_images,\n",
    "            attack_labels,\n",
    "            epsilons=[x/255 for x in [2,4,8,16,32]]\n",
    "        )\n",
    "        del fmodel\n",
    "        return [np.count_nonzero(eps_res)/len(y_to_attack) for eps_res in success]\n",
    "    \n",
    "    if architecture == 'ResNet'  or architecture == 'VGG':\n",
    "        res = [[],[],[],[],[],[]]\n",
    "        strengths = [0.125,0.25,0.5,1,2,4]\n",
    "        for i in range(4):\n",
    "            print_time(text=f'pgd batch {i}')\n",
    "            adversarials, _, success = attack(\n",
    "                fmodel,\n",
    "                attack_images[i*BATCHSIZE:(i+1)*BATCHSIZE],\n",
    "                attack_labels[i*BATCHSIZE:(i+1)*BATCHSIZE],\n",
    "                epsilons=[x/255 for x in strengths]\n",
    "            )\n",
    "            for j in range(len(strengths)):\n",
    "                res[j] = res[j]+list(success[j])\n",
    "        print_time(text='ending pgd')\n",
    "        del fmodel\n",
    "        gc.collect()\n",
    "        return [np.count_nonzero(eps_res)/len(attack_labels) for eps_res in res]\n",
    "    return False\n",
    "\n",
    "\n",
    "def cw2_attack(architecture, model_to_attack, attack_images, attack_labels, eps=[100]):\n",
    "    print_time(text=f'starting cw')\n",
    "    BATCHSIZE = 64\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.L2CarliniWagnerAttack(\n",
    "        binary_search_steps = 9,\n",
    "        steps= 5000,\n",
    "        stepsize = 1,\n",
    "        confidence = 0,\n",
    "        initial_const = 100,\n",
    "        abort_early = True,\n",
    "    )\n",
    "    if architecture == 'CNN' or architecture == 'MLP':\n",
    "        adversarials, _, success = attack(\n",
    "            fmodel,\n",
    "            attack_images,\n",
    "            attack_labels,\n",
    "            epsilons=eps\n",
    "        )\n",
    "        dists = [tf.norm(x_to_attack[i]-adversarials[0][i]).numpy() for i in range(len(x_to_attack))]\n",
    "        del fmodel\n",
    "        \n",
    "        return dists, success.numpy().tolist()\n",
    "    if architecture == 'ResNet'  or architecture == 'VGG':\n",
    "        success = []\n",
    "        dists = [] \n",
    "        for i in range(4):\n",
    "            print_time(text=f'cw batch {i}')\n",
    "            attack_batch = attack_images[i*BATCHSIZE:(i+1)*BATCHSIZE]\n",
    "            attack_batch_labels = attack_labels[i*BATCHSIZE:(i+1)*BATCHSIZE]\n",
    "            adversarials, _, batch_success = attack(\n",
    "                fmodel,\n",
    "                attack_batch,\n",
    "                attack_batch_labels,\n",
    "                epsilons=eps\n",
    "            )\n",
    "            success = success + list(batch_success)\n",
    "            dists = dists + [tf.norm(attack_batch[j]-adversarials[0][j]).numpy() for j in range(len(attack_batch))]\n",
    "        print_time(text=f'ending cw')\n",
    "        del fmodel\n",
    "        gc.collect()\n",
    "        return dists, success\n",
    "    return False\n",
    "\n",
    "def bb0_attack(architecture,model_to_attack, attack_images, attack_labels):\n",
    "    print_time(text=f'starting bb0')\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "    if architecture == 'CNN' or architecture == 'MLP':\n",
    "        BATCHSIZE = 250\n",
    "    if architecture == 'ResNet' or architecture == 'VGG':\n",
    "        BATCHSIZE = 64\n",
    "    \n",
    "    batches = [\n",
    "        (attack_images[:BATCHSIZE], attack_labels[:BATCHSIZE]), \n",
    "        (attack_images[BATCHSIZE:2*BATCHSIZE], attack_labels[BATCHSIZE:2*BATCHSIZE]),\n",
    "        (attack_images[2*BATCHSIZE:3*BATCHSIZE], attack_labels[2*BATCHSIZE:3*BATCHSIZE]), \n",
    "        (attack_images[3*BATCHSIZE:4*BATCHSIZE], attack_labels[3*BATCHSIZE:4*BATCHSIZE])\n",
    "    ]\n",
    "\n",
    "    # create attack that picks adversarials from given dataset of samples\n",
    "    #init_attack = fb.attacks.DatasetAttack()\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "\n",
    "    init_attack.feed(fmodel, batches[0][0])   # feed 1st batch of inputs\n",
    "    init_attack.feed(fmodel, batches[1][0])   # feed 2nd batch of inputs\n",
    "    init_attack.feed(fmodel, batches[2][0])   # feed 1st batch of inputs\n",
    "    init_attack.feed(fmodel, batches[3][0])   # feed 2nd batch of inputs\n",
    "    attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=30, steps=500,lr_num_decay=30, lr=1e7, init_attack=init_attack)\n",
    "\n",
    "    success = []\n",
    "    dists = [] \n",
    "    for i in range(4):\n",
    "        print_time(text=f'bb0 batch {i}')\n",
    "        attack_batch = attack_images[i*BATCHSIZE:(i+1)*BATCHSIZE]\n",
    "        attack_batch_labels = attack_labels[i*BATCHSIZE:(i+1)*BATCHSIZE]\n",
    "        adversarials, _, batch_success = attack(\n",
    "            fmodel,\n",
    "            attack_batch,\n",
    "            criterion=fb.criteria.Misclassification(attack_batch_labels),\n",
    "            epsilons=[None]\n",
    "        )\n",
    "        \n",
    "        success = success + list(batch_success)\n",
    "        dists = dists + [np.count_nonzero(attack_batch[j]-adversarials[0][j]) for j in range(len(attack_batch))]\n",
    "    print_time(text=f'ending bb0')\n",
    "    del fmodel\n",
    "    gc.collect()\n",
    "    return dists, success\n",
    "\n",
    "\n",
    "\n",
    "def convert_from_hwio_to_iohw(weights_hwio):\n",
    "    return tf.transpose(weights_hwio, [2, 3, 0, 1])\n",
    "\n",
    "def convert_from_iohw_to_hwio(weights_iohw):\n",
    "    return tf.transpose(weights_iohw, [2, 3, 0, 1])\n",
    "\n",
    "def convert_from_iohw_to_oihw(weights_iohw):\n",
    "    return tf.transpose(weights_iohw, [1, 0, 2, 3])\n",
    "\n",
    "def convert_from_oihw_to_iohw(weights_oihw):\n",
    "    return tf.transpose(weights_oihw, [1, 0, 2, 3])\n",
    "\n",
    "def convert_from_hwio_to_oihw(weights_hwio):\n",
    "    return tf.transpose(weights_hwio, [3, 2, 0, 1])\n",
    "\n",
    "def convert_from_oihw_to_hwio(weights_oihw):\n",
    "    return tf.transpose(weights_oihw, [2, 3, 1, 0])\n",
    "\n",
    "\n",
    "\n",
    "def get_zeros_ratio(model, layers_to_examine=None):\n",
    "    if layers_to_examine==None:\n",
    "        layers_to_examine = model.dense_masks+model.conv_masks\n",
    "    weights = model.get_weights()\n",
    "    all_weights = np.array([])\n",
    "    for x in layers_to_examine:\n",
    "\n",
    "        all_weights = np.append(all_weights, weights[x].flatten())\n",
    "    return np.count_nonzero(all_weights)/len(all_weights), np.count_nonzero(all_weights), len(all_weights)\n",
    "\n",
    "def plot_hist(hist):\n",
    "    # summarize history for accuracy\n",
    "    plt.plot(hist.history['accuracy'])\n",
    "    plt.plot(hist.history['val_accuracy'])\n",
    "    plt.title('model accuracy')\n",
    "    plt.ylabel('accuracy')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for loss\n",
    "    plt.plot(hist.history['loss'])\n",
    "    plt.plot(hist.history['val_loss'])\n",
    "    plt.title('model loss')\n",
    "    plt.ylabel('loss')\n",
    "    plt.xlabel('epoch')\n",
    "    plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()\n",
    "    # summarize history for lr\n",
    "    plt.plot(hist.history['lr'])\n",
    "    plt.title('model lr')\n",
    "    plt.ylabel('lr')\n",
    "    plt.xlabel('epoch')\n",
    "    #plt.legend(['train', 'test'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
