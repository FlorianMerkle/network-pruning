{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'cnn'\n",
    "#EXPERIMENT_NAME = 'cnn-global-magnitude-unstruct'\n",
    "ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import foolbox as fb\n",
    "import random\n",
    "import json\n",
    "from cleverhans.future.tf2.attacks import projected_gradient_descent, fast_gradient_method, carlini_wagner_l2\n",
    "\n",
    "\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 3)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5949 - accuracy: 0.8878 - val_loss: 1.5180 - val_accuracy: 0.9495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 0\n",
      "(1.0, 61470, 61470)\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 4ms/step - loss: 1.5059 - accuracy: 0.9587 - val_loss: 1.4945 - val_accuracy: 0.9693\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4931 - accuracy: 0.9701 - val_loss: 1.4897 - val_accuracy: 0.9730\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4870 - accuracy: 0.9754 - val_loss: 1.4849 - val_accuracy: 0.9774\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4826 - accuracy: 0.9796 - val_loss: 1.4800 - val_accuracy: 0.9821\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4800 - accuracy: 0.9822 - val_loss: 1.4815 - val_accuracy: 0.9805\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4782 - accuracy: 0.9834 - val_loss: 1.4789 - val_accuracy: 0.9832\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4762 - accuracy: 0.9857 - val_loss: 1.4781 - val_accuracy: 0.9829\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4743 - accuracy: 0.9875 - val_loss: 1.4800 - val_accuracy: 0.9822\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4735 - accuracy: 0.9883 - val_loss: 1.4776 - val_accuracy: 0.9844\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4729 - accuracy: 0.9887 - val_loss: 1.4768 - val_accuracy: 0.9848\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4719 - accuracy: 0.9899 - val_loss: 1.4770 - val_accuracy: 0.9848\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4718 - accuracy: 0.9899 - val_loss: 1.4755 - val_accuracy: 0.9859\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4710 - accuracy: 0.9906 - val_loss: 1.4770 - val_accuracy: 0.9842\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4700 - accuracy: 0.9916 - val_loss: 1.4766 - val_accuracy: 0.9844\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4699 - accuracy: 0.9918 - val_loss: 1.4740 - val_accuracy: 0.9871\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4694 - accuracy: 0.9923 - val_loss: 1.4739 - val_accuracy: 0.9878\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4696 - accuracy: 0.9920 - val_loss: 1.4749 - val_accuracy: 0.9866\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4686 - accuracy: 0.9929 - val_loss: 1.4748 - val_accuracy: 0.9863\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4682 - accuracy: 0.9935 - val_loss: 1.4752 - val_accuracy: 0.9862\n",
      "(1.0, 61470, 61470)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [04:57, 297.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 0\n",
      "(0.5008296730112249, 30786, 61470)\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4825 - accuracy: 0.9849 - val_loss: 1.4782 - val_accuracy: 0.9861\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4731 - accuracy: 0.9905 - val_loss: 1.4744 - val_accuracy: 0.9890\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4710 - accuracy: 0.9919 - val_loss: 1.4737 - val_accuracy: 0.9884\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4700 - accuracy: 0.9926 - val_loss: 1.4735 - val_accuracy: 0.9887\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4687 - accuracy: 0.9937 - val_loss: 1.4760 - val_accuracy: 0.9863\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4682 - accuracy: 0.9940 - val_loss: 1.4736 - val_accuracy: 0.9881\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4677 - accuracy: 0.9944 - val_loss: 1.4730 - val_accuracy: 0.9888\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4670 - accuracy: 0.9949 - val_loss: 1.4763 - val_accuracy: 0.9857\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4670 - accuracy: 0.9949 - val_loss: 1.4728 - val_accuracy: 0.9888\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4662 - accuracy: 0.9957 - val_loss: 1.4732 - val_accuracy: 0.9884\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4659 - accuracy: 0.9959 - val_loss: 1.4737 - val_accuracy: 0.9879\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4660 - accuracy: 0.9959 - val_loss: 1.4746 - val_accuracy: 0.9869\n",
      "(0.5008296730112249, 30786, 61470)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [08:55, 279.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 0\n",
      "(0.25124450951683747, 15444, 61470)\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5099 - accuracy: 0.9690 - val_loss: 1.4864 - val_accuracy: 0.9814\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4823 - accuracy: 0.9847 - val_loss: 1.4811 - val_accuracy: 0.9834\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4776 - accuracy: 0.9876 - val_loss: 1.4782 - val_accuracy: 0.9850\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4745 - accuracy: 0.9896 - val_loss: 1.4773 - val_accuracy: 0.9865\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4729 - accuracy: 0.9906 - val_loss: 1.4762 - val_accuracy: 0.9876\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4718 - accuracy: 0.9915 - val_loss: 1.4762 - val_accuracy: 0.9870\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4702 - accuracy: 0.9930 - val_loss: 1.4749 - val_accuracy: 0.9874\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4696 - accuracy: 0.9934 - val_loss: 1.4739 - val_accuracy: 0.9891\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4690 - accuracy: 0.9938 - val_loss: 1.4738 - val_accuracy: 0.9885\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4683 - accuracy: 0.9941 - val_loss: 1.4757 - val_accuracy: 0.9867\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4675 - accuracy: 0.9950 - val_loss: 1.4750 - val_accuracy: 0.9869\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4674 - accuracy: 0.9949 - val_loss: 1.4740 - val_accuracy: 0.9881\n",
      "(0.25124450951683747, 15444, 61470)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [13:28, 277.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 0\n",
      "(0.12645192776964373, 7773, 61470)\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5733 - accuracy: 0.9190 - val_loss: 1.5128 - val_accuracy: 0.9620\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5052 - accuracy: 0.9671 - val_loss: 1.4954 - val_accuracy: 0.9737\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4933 - accuracy: 0.9754 - val_loss: 1.4895 - val_accuracy: 0.9772\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4876 - accuracy: 0.9795 - val_loss: 1.4872 - val_accuracy: 0.9786\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4839 - accuracy: 0.9816 - val_loss: 1.4837 - val_accuracy: 0.9804\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4812 - accuracy: 0.9841 - val_loss: 1.4818 - val_accuracy: 0.9816\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4792 - accuracy: 0.9853 - val_loss: 1.4823 - val_accuracy: 0.9818\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4777 - accuracy: 0.9864 - val_loss: 1.4800 - val_accuracy: 0.9832\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4764 - accuracy: 0.9878 - val_loss: 1.4791 - val_accuracy: 0.9836\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4750 - accuracy: 0.9888 - val_loss: 1.4792 - val_accuracy: 0.9832\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4744 - accuracy: 0.9892 - val_loss: 1.4798 - val_accuracy: 0.9831\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4734 - accuracy: 0.9899 - val_loss: 1.4785 - val_accuracy: 0.9845\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4726 - accuracy: 0.9908 - val_loss: 1.4793 - val_accuracy: 0.9840\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4721 - accuracy: 0.9911 - val_loss: 1.4785 - val_accuracy: 0.9839\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4714 - accuracy: 0.9917 - val_loss: 1.4777 - val_accuracy: 0.9849\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4709 - accuracy: 0.9922 - val_loss: 1.4773 - val_accuracy: 0.9858\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4704 - accuracy: 0.9924 - val_loss: 1.4774 - val_accuracy: 0.9851\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4700 - accuracy: 0.9930 - val_loss: 1.4768 - val_accuracy: 0.9853\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4696 - accuracy: 0.9931 - val_loss: 1.4765 - val_accuracy: 0.9856\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4691 - accuracy: 0.9936 - val_loss: 1.4769 - val_accuracy: 0.9860\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4688 - accuracy: 0.9936 - val_loss: 1.4762 - val_accuracy: 0.9859\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4686 - accuracy: 0.9939 - val_loss: 1.4755 - val_accuracy: 0.9862\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4683 - accuracy: 0.9942 - val_loss: 1.4760 - val_accuracy: 0.9856\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4679 - accuracy: 0.9944 - val_loss: 1.4762 - val_accuracy: 0.9857\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4680 - accuracy: 0.9945 - val_loss: 1.4755 - val_accuracy: 0.9862\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4672 - accuracy: 0.9952 - val_loss: 1.4756 - val_accuracy: 0.9864\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4672 - accuracy: 0.9953 - val_loss: 1.4761 - val_accuracy: 0.9863\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4669 - accuracy: 0.9954 - val_loss: 1.4757 - val_accuracy: 0.9863\n",
      "(0.12645192776964373, 7773, 61470)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune, Train Attack Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(structure='unstructured', method='random', scope='global', iterations=10):\n",
    "    \n",
    "    experiment_name = f'{ARCHITECTURE}-{method}-{scope}-{structure}'\n",
    "    pgd_success_rates = []\n",
    "    cw_success_rates = []\n",
    "    bb0_success_rates = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    \n",
    "    compression_rates = [tf.math.pow(2, x).numpy() for x in range(7)]\n",
    "    pruning_ratios = [1-1/x for x in compression_rates]\n",
    "\n",
    "    for j in tqdm(range(iterations)):\n",
    "        accuracies = []\n",
    "        pgd_success_rate = []\n",
    "        cw_success_rate = []\n",
    "        bb0_success_rate = []\n",
    "        try: \n",
    "            del model\n",
    "        except:\n",
    "            ;\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = initialize_base_model(j, experiment_name=experiment_name, save_weights=True)\n",
    "        for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "\n",
    "            #model.load_weights(f'./saved-weights/{experiment_name}-{j}')\n",
    "            #print(f'./saved-weights/{experiment_name}-{j}')\n",
    "\n",
    "            #for i in range(index + 1):\n",
    "                print(f'current pruning ratio is{pruning_ratio}, current iteration is {j}')\n",
    "                #if i != index:\n",
    "\n",
    "                if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                    model.prune_random_global_unstruct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='global' and structure=='structured':\n",
    "                    model.prune_random_global_struct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                    model.prune_random_local_unstruct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='local' and structure=='structured':\n",
    "                    model.prune_random_local_struct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                    model.prune_magnitude_global_unstruct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                    model.prune_magnitude_global_struct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                    model.prune_magnitude_local_unstruct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                    model.prune_magnitude_local_struct(pruning_ratio)\n",
    "                else:\n",
    "                    raise ValueError(\"pruning method invalid\")\n",
    "\n",
    "                print(get_zeros_ratio(model.get_weights()))\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                              experimental_run_tf_function=False\n",
    "                             )\n",
    "                model = train_model(model, to_convergence=True)\n",
    "                print(get_zeros_ratio(model.get_weights()))\n",
    "                accuracies.append(model.evaluate(x_test, y_test, verbose=0))\n",
    "                pgd_success_rate.append(pgd_attack(model))\n",
    "                \n",
    "                bb0_success_rate.append(bb0_attack(model))\n",
    "                cw_success_rate.append(cw2_attack(model))\n",
    "        all_accuracies.append(accuracies)\n",
    "        pgd_success_rates.append(pgd_success_rate)\n",
    "        cw_success_rates.append(cw_success_rate)\n",
    "        bb0_success_rates.append(bb0_success_rate)\n",
    "    #write to csv and json\n",
    "    with open(f'saved-results/{experiment_name}-accuracies.json', 'w') as f:\n",
    "        json.dump(all_accuracies, f)\n",
    "\n",
    "    with open(f'saved-results/{experiment_name}-pgd-success.json', 'w') as f:\n",
    "        json.dump(pgd_success_rates, f)\n",
    "        \n",
    "    with open(f'saved-results/{experiment_name}-cw2-success.json', 'w') as f:\n",
    "        json.dump(cw_success_rates, f)\n",
    "        \n",
    "    with open(f'saved-results/{experiment_name}-bb0-success.json', 'w') as f:\n",
    "        json.dump(bb0_success_rates, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zeros_ratio(weights):\n",
    "    layers_to_examine = [0,3,6,9,12]\n",
    "    all_weights = np.array([])\n",
    "    for x in layers_to_examine:\n",
    "\n",
    "        all_weights = np.append(all_weights, weights[x].flatten())\n",
    "    return np.count_nonzero(all_weights)/len(all_weights), np.count_nonzero(all_weights), len(all_weights)\n",
    "\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "\n",
    "    model = CustomConvModel()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, to_convergence=True, epochs = 5):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "def pgd_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        epsilons=[8/255]\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/len(y_to_attack)\n",
    "\n",
    "def cw2_attack(model_to_attack, eps=[.5]):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.L2CarliniWagnerAttack()\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        epsilons=eps\n",
    "    )\n",
    "    return np.count_nonzero(success)/len(y_to_attack)\n",
    "\n",
    "def bb0_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    cls_samples = {}\n",
    "    idx = 0\n",
    "    starting_points = []\n",
    "    \n",
    "    while len(list(cls_samples)) < 10 and idx < len(x_test):\n",
    "        clean_pred = int(np.argmax(model_to_attack(x_test[idx])))\n",
    "        cls_samples[clean_pred] = x_test[idx]\n",
    "        idx += 1\n",
    "        #print(idx)\n",
    "        #print(len(list(cls_samples)))\n",
    "    \n",
    "    for x in list(x_to_attack):\n",
    "        counter = 0\n",
    "        try:\n",
    "            least_likely = int(np.argmin(model_to_attack(x)))\n",
    "            if least_likely in cls_samples and np.argmax(model_to_attack(cls_samples[least_likely])) != np.argmax(model_to_attack(x)):\n",
    "                starting_points.append(cls_samples[least_likely])\n",
    "            else:\n",
    "                \n",
    "                print('tüdü')\n",
    "                for cls_sample in cls_samples.values():\n",
    "                    try:\n",
    "                        clean_pred = int(np.argmax(model_to_attack(cls_sample)))\n",
    "                        pred = int(np.argmax(model_to_attack(x)))\n",
    "                        if clean_pred != pred:\n",
    "                            counter +=1\n",
    "                            print(counter, clean_pred, pred)\n",
    "                            starting_points.append(cls_sample)\n",
    "                            break\n",
    "                    except:\n",
    "                        print('trying next ')\n",
    "                        pass\n",
    "        except Exception as err:\n",
    "            print('could not find starting points', err)\n",
    "            return False\n",
    "    are_advs = []\n",
    "    for j, sp in enumerate(starting_points):\n",
    "        are_advs.append(np.argmax(fmodel(sp)) != np.argmax(fmodel(x_to_attack[j])))\n",
    "    #return(are_advs)\n",
    "    starting_points = tf.convert_to_tensor(starting_points)\n",
    "    attack_successful = False\n",
    "    steps = 5000\n",
    "    directions = 5000\n",
    "    \n",
    "    attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=100, lr=1e7, init_attack=fb.attacks.LinearSearchBlendedUniformNoiseAttack(steps=steps,directions=directions,distance=fb.distances.linf))\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        criterion=fb.criteria.Misclassification(y_to_attack),\n",
    "        starting_points=starting_points,\n",
    "        epsilons=[None]\n",
    "    )\n",
    "    \n",
    "    dists = [np.count_nonzero(x_to_attack[i]-adversarials[0][i]) for i in range(len(x_to_attack))]\n",
    "    return dists, ([x.numpy().tolist() for x in adversarials], success.numpy().tolist()), starting_points\n",
    "\n",
    "def bb0_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "    batches = [(x_to_attack[:500], y_to_attack[:500]), (x_to_attack[500:], y_to_attack[500:])]\n",
    "\n",
    "    # create attack that picks adversarials from given dataset of samples\n",
    "    #init_attack = fb.attacks.DatasetAttack()\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "\n",
    "    init_attack.feed(fmodel, batches[0][0])   # feed 1st batch of inputs\n",
    "    init_attack.feed(fmodel, batches[1][0])   # feed 2nd batch of inputs\n",
    "    attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=30, steps=500,lr_num_decay=30, lr=1e7, init_attack=init_attack)\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        criterion=fb.criteria.Misclassification(y_to_attack),\n",
    "        epsilons=[None]\n",
    "    )\n",
    "    dists = [np.count_nonzero(x_to_attack[i]-adversarials[0][i]) for i in range(len(x_to_attack))]\n",
    "    return dists, ([x.numpy().tolist() for x in adversarials], success.numpy().tolist())\n",
    "        \n",
    "def prune_conv_layers(pruning_ratio):\n",
    "    layer_to_prune = [0, 3]\n",
    "    pruned_weights = model.get_weights()\n",
    "    \n",
    "    for layer in layer_to_prune:\n",
    "        converted_weights = convert_from_hwio_to_iohw(model.get_weights()[layer])\n",
    "        converted_mask = convert_from_hwio_to_iohw(model.get_weights()[layer + 2]).numpy()\n",
    "        for input_index, input_layer in enumerate(converted_weights):\n",
    "\n",
    "            for kernel_index, kernel in enumerate(input_layer):\n",
    "                dims = kernel.shape\n",
    "                flat_weights = kernel.numpy().flatten()\n",
    "                flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "                #print(no_of_weights_to_prune)\n",
    "                indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "\n",
    "                converted_mask[input_index][kernel_index] = flat_masks.reshape(dims)\n",
    "        back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "        pruned_weights[layer+2] = back_converted_mask\n",
    "    \n",
    "    return pruned_weights\n",
    "\n",
    "def convert_from_hwio_to_iohw(weights_nchw):\n",
    "    return tf.transpose(weights_nchw, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "def convert_from_iohw_to_hwio(weights_nhwc):\n",
    "    return tf.transpose(weights_nhwc, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "def get_average_accuracies(all_accuracies):\n",
    "    acc_per_pruning_rate=[]\n",
    "    for i in range(len(all_accuracies)):\n",
    "        for j in range(len(all_accuracies[i])):\n",
    "\n",
    "            try:\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "            except:\n",
    "                acc_per_pruning_rate.append([])\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "    avg_acc_per_pruning_rate = [sum(x)/len(x) for x in acc_per_pruning_rate]; avg_acc_per_pruning_rate\n",
    "    return avg_acc_per_pruning_rate\n",
    "\n",
    "def get_average_success_rates(all_success_rates):\n",
    "    success_per_pruning_rate=[]\n",
    "    for i in range(len(all_success_rates)):\n",
    "        for j in range(len(all_success_rates[i])):\n",
    "\n",
    "            try:\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "            except:\n",
    "                success_per_pruning_rate.append([])\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "    avg_success_per_pruning_rate = [sum(x)/len(x) for x in success_per_pruning_rate];avg_success_per_pruning_rate\n",
    "    return avg_success_per_pruning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x_to_attack = tf.convert_to_tensor(x_train[0:1000].reshape(1000,28*28))\n",
    "y_to_attack = tf.convert_to_tensor([y_train[0:1000]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "shapes = {\n",
    "    # 5x5 conv, 1 input, 6 outputs\n",
    "    'conv_1': (5, 5, 1, 6),\n",
    "    # 5x5 conv, 6 inputs, 16 outputs\n",
    "    'conv_2': (5, 5, 6, 16),\n",
    "    #5x5 conv as in paper, 16 inputs, 120 outputs\n",
    "    'conv_3': (1, 1, 16, 120),\n",
    "    # fully connected, 5*5*16 inputs, 120 outputs\n",
    "    'dense_1': (5*5*16, 120),\n",
    "    # fully connected, 120 inputs, 84 outputs\n",
    "    'dense_2': (120, 84),\n",
    "    # 84 inputs, 10 outputs (class prediction)\n",
    "    'dense_3': (84, 10),\n",
    "}\n",
    "bias_shapes = {\n",
    "    #output depth\n",
    "    'conv_1': (6),\n",
    "    'conv_2': (16),\n",
    "    'dense_1': (120),\n",
    "    'dense_2': (84),\n",
    "    'dense_3': (10),\n",
    "}\n",
    "\n",
    "#conv2D with bias and relu activation\n",
    "\n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, weights, mask, biases, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = weights\n",
    "        self.m = mask\n",
    "        self.b = biases\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "        \n",
    "\n",
    "#Average Pooling Layer\n",
    "class CustomPoolLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, k=2, padding='valid'):#padding='VALID'):\n",
    "        super(CustomPoolLayer, self).__init__()\n",
    "        self.k = k\n",
    "        self.p = padding\n",
    "    \n",
    "    def call(self, inputs):\n",
    "#        return tf.keras.layers.AveragePooling2D(pool_size=(self.k, self.k), strides=None, padding=self.p, data_format='channels_first')(inputs)\n",
    "        return tf.nn.avg_pool2d(inputs, ksize=[1, self.k, self.k,1], strides=[1, self.k, self.k, 1], padding=self.p,)# data_format='NCHW')\n",
    "    \n",
    "\n",
    "        \n",
    "class CustomConvLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self, shape, bias, strides, padding='SAME'):\n",
    "        \n",
    "        super(CustomConvLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='random_normal',\n",
    "            trainable=True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape=shape,\n",
    "            initializer='ones',\n",
    "            trainable=False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.s = strides\n",
    "        self.p = padding\n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.nn.conv2d(inputs, tf.multiply(self.w, self.m), strides=[1, self.s, self.s, 1], padding=self.p,)# data_format='NCHW')\n",
    "        x = tf.nn.bias_add(x, self.b,)# 'NC...')\n",
    "        return tf.nn.tanh(x)\n",
    "\n",
    "#Dense Layer with Bias\n",
    "class CustomDenseLayer(layers.Layer):\n",
    "    def __init__(self, shape, bias, activation = 'tanh'):\n",
    "        super(CustomDenseLayer, self).__init__()\n",
    "        self.w = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='random_normal',\n",
    "            trainable = True,\n",
    "            name='w'\n",
    "        )\n",
    "        self.m = self.add_weight(\n",
    "            shape = shape,\n",
    "            initializer='ones',\n",
    "            trainable = False,\n",
    "            name='m'\n",
    "        )\n",
    "        self.b = self.add_weight(\n",
    "            shape = (shape[-1]),\n",
    "            initializer = 'zeros',\n",
    "            trainable = True,\n",
    "            name='b'\n",
    "        )\n",
    "        self.a = activation\n",
    "        \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.matmul(inputs, tf.multiply(self.w, self.m))\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        if self.a == 'tanh':\n",
    "            return tf.nn.tanh(x)\n",
    "        if self.a == 'softmax':\n",
    "            return tf.nn.softmax(x)\n",
    "        \n",
    "        \n",
    "class CustomConvModel(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(CustomConvModel, self).__init__()\n",
    "        self.conv1 = CustomConvLayer(shapes['conv_1'], True, 1, 'SAME')#'VALID')\n",
    "        self.maxpool1 = CustomPoolLayer(k=2, padding='SAME')\n",
    "        self.conv2 = CustomConvLayer(shapes['conv_2'], True, 1, 'VALID')\n",
    "        self.maxpool2 = CustomPoolLayer(k=2, padding='VALID')\n",
    "\n",
    "        self.dense1 = CustomDenseLayer(shapes['dense_1'], True, 'tanh')\n",
    "        self.dense2 = CustomDenseLayer(shapes['dense_2'], True, 'tanh')\n",
    "        self.dense3 = CustomDenseLayer(shapes['dense_3'], True, 'softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = tf.reshape(inputs, shape=[-1,28, 28, 1])\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.maxpool2(x)\n",
    "        x = layers.Flatten()(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        x =  self.dense3(x)\n",
    "        return x\n",
    "    \n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            weights = self.get_weights()\n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        \n",
    "                        no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                        # find unpruned weights\n",
    "                        non_zero_weights = np.nonzero(flat_masks)[0]\n",
    "                        # calculate the amount of weights to be pruned this round\n",
    "                        no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                        # shuffle all non-zero weights\n",
    "                        random.shuffle(non_zero_weights)\n",
    "                        # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                        indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                        \n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "                    no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self, ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "        #flat out all weights:\n",
    "        conv_layer_to_prune = [0, 3]\n",
    "        dense_layer_to_prune = [6, 9, 12]\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            flat_weights = np.append(flat_weights, weights[x])\n",
    "            flat_mask = np.append(flat_mask, weights[x+2])\n",
    "            \n",
    "        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[x + 2] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = [0, 3]\n",
    "            #print('inside conv prune func',get_zeros_ratio(self.get_weights()))\n",
    "            weights = self.get_weights()\n",
    "            \n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        #flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                        #flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                        #print(no_of_weights_to_prune)\n",
    "                        #indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "\n",
    "\n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            \n",
    "            dense_layer_to_prune = [6, 9, 12]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "\n",
    "                    no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                    indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self,ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "        return\n",
    "    \n",
    "    def prune_random_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                #print(channels)\n",
    "                non_zero_channels = np.nonzero([np.sum(channel) for channel in channels])[0]\n",
    "                #print(non_zero_channels)\n",
    "                no_of_channels_to_prune_left = no_of_channels_to_prune - (len(channels) - len(non_zero_channels))\n",
    "                random.shuffle(non_zero_channels)\n",
    "                channels_to_prune = non_zero_channels[:no_of_channels_to_prune_left]\n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return True\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                random.shuffle(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return True\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    def prune_magnitude_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                \n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                for channel in channels:\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(channel)))\n",
    "                channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return weights\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            all_channels = np.empty((0,5,5))\n",
    "            original_shapes = []\n",
    "            for layer_to_prune in conv_layers_to_prune:\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[layer_to_prune])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                all_channels = np.concatenate((all_channels, channels))\n",
    "            mask = np.ones(all_channels.shape)\n",
    "            vals = [np.sum(np.abs(channel)) for channel in all_channels]\n",
    "            no_of_channels_to_prune = int(ratio * len(vals))\n",
    "            channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "            for channel_to_prune in channels_to_prune:\n",
    "                all_channels[channel_to_prune] = tf.zeros((5,5))\n",
    "                mask[channel_to_prune] = tf.zeros((5,5))\n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(conv_layers_to_prune):\n",
    "                original_shape = convert_from_hwio_to_iohw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_channels[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                pruned_mask = tf.reshape(mask[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                weights[layer_to_prune] = convert_from_iohw_to_hwio(pruned_layer)\n",
    "                weights[layer_to_prune + 2] = convert_from_iohw_to_hwio(pruned_mask)\n",
    "                z = original_shape[0]*original_shape[1]    \n",
    "            return weights\n",
    "        \n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            vals = []\n",
    "            lengths = []\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                lengths.append(weights[layer_to_prune].shape[0])\n",
    "                vals = vals + [np.sum(np.abs(row)) / len(row) for row in weights[layer_to_prune]]\n",
    "            no_of_rows_to_prune = int(ratio * len(vals))\n",
    "            rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "            for i, layer_to_prune in enumerate(dense_layers_to_prune):\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    if row_to_prune in range(int(np.sum(lengths[:i])), int(np.sum(lengths[:i+1]))):\n",
    "                        weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)\n",
    "                        weights[layer_to_prune + 2][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)                \n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = [0,3]\n",
    "        dense_layers_to_prune = [6,9,12]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 3ms/step - loss: 1.6413 - accuracy: 0.8511 - val_loss: 1.5217 - val_accuracy: 0.9464\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f50e84364f0>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = CustomConvModel()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "              \n",
    "             )\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )\n",
    "#model.save('./saved-models/mini-pipeline-CNN-baseline-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5944 - accuracy: 0.8848 - val_loss: 1.5112 - val_accuracy: 0.9530\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5043 - accuracy: 0.9603 - val_loss: 1.4958 - val_accuracy: 0.9673\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4919 - accuracy: 0.9715 - val_loss: 1.4893 - val_accuracy: 0.9740\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4855 - accuracy: 0.9776 - val_loss: 1.4838 - val_accuracy: 0.9786\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4825 - accuracy: 0.9797 - val_loss: 1.4833 - val_accuracy: 0.9790\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4796 - accuracy: 0.9824 - val_loss: 1.4791 - val_accuracy: 0.9832\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4774 - accuracy: 0.9847 - val_loss: 1.4787 - val_accuracy: 0.9833\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4757 - accuracy: 0.9861 - val_loss: 1.4774 - val_accuracy: 0.9849\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4739 - accuracy: 0.9880 - val_loss: 1.4777 - val_accuracy: 0.9837\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4726 - accuracy: 0.9892 - val_loss: 1.4762 - val_accuracy: 0.9853\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4720 - accuracy: 0.9897 - val_loss: 1.4766 - val_accuracy: 0.9849\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4717 - accuracy: 0.9901 - val_loss: 1.4758 - val_accuracy: 0.9853\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4706 - accuracy: 0.9911 - val_loss: 1.4750 - val_accuracy: 0.9864\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4694 - accuracy: 0.9922 - val_loss: 1.4774 - val_accuracy: 0.9838\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4695 - accuracy: 0.9919 - val_loss: 1.4750 - val_accuracy: 0.9864\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4699 - accuracy: 0.9916 - val_loss: 1.4761 - val_accuracy: 0.9858\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CustomConvModel at 0x7f50f804a220>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = initialize_base_model(999,'')\n",
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "going to be asserted TensorFlowTensor(<tf.Tensor: shape=(1000,), dtype=bool, numpy=\n",
      "                 array([ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
      "                         True])>)\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 0 0\n",
      "in attack 5 5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 5 5\n",
      "in attack 0 0\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 5 5\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 6 6\n",
      "in attack 5 5\n",
      "in attack 2 2\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 0 0\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 5 5\n",
      "in attack 3 3\n",
      "in attack 6 6\n",
      "in attack 5 5\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 6 6\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 2 2\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 9 9\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 9 9\n",
      "in attack 6 6\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 0 0\n",
      "in attack 0 0\n",
      "in attack 3 3\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 0 0\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 8 8\n",
      "in attack 2 2\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 3 3\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 3 3\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 0 0\n",
      "in attack 2 2\n",
      "in attack 0 0\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 6 6\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 5 5\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 6 6\n",
      "in attack 7 7\n",
      "in attack 6 6\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 3 3\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 2 2\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 0 0\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 2 2\n",
      "in attack 5 5\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 6 6\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 3 3\n",
      "in attack 3 3\n",
      "in attack 2 2\n",
      "in attack 6 6\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 3 3\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 0 0\n",
      "in attack 8 8\n",
      "in attack 6 6\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 2 2\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 5 5\n",
      "in attack 2 2\n",
      "in attack 6 6\n",
      "in attack 9 9\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 2 2\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 0 0\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 8 8\n",
      "in attack 2 2\n",
      "in attack 0 0\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 5 5\n",
      "in attack 7 7\n",
      "in attack 2 2\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 0 0\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 6 6\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 7 7\n",
      "in attack 3 3\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 3 3\n",
      "in attack 2 2\n",
      "in attack 2 2\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 7 7\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 8 8\n",
      "in attack 8 8\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 0 0\n",
      "in attack 3 3\n",
      "in attack 5 5\n",
      "in attack 0 0\n",
      "in attack 5 5\n",
      "in attack 9 9\n",
      "in attack 7 7\n",
      "in attack 7 7\n",
      "in attack 9 9\n",
      "in attack 2 2\n",
      "in attack 9 9\n",
      "in attack 9 9\n",
      "in attack 6 6\n",
      "in attack 8 8\n",
      "in attack 9 9\n"
     ]
    }
   ],
   "source": [
    "res = bb0_attack(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 2.2468 - accuracy: 0.2511 - val_loss: 2.1632 - val_accuracy: 0.3376\n",
      "Epoch 2/5\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.1188 - accuracy: 0.3875 - val_loss: 2.0858 - val_accuracy: 0.4054\n",
      "Epoch 3/5\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.0621 - accuracy: 0.4275 - val_loss: 2.0286 - val_accuracy: 0.4638\n",
      "Epoch 4/5\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 2.0145 - accuracy: 0.4689 - val_loss: 2.0005 - val_accuracy: 0.4819\n",
      "Epoch 5/5\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.9934 - accuracy: 0.4827 - val_loss: 1.9861 - val_accuracy: 0.4888\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.CustomConvModel at 0x7f50f804a220>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.prune_magnitude_global_unstruct(.995)\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "             )\n",
    "train_model(model, to_convergence=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.005010574263868554, 308, 61470)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_zeros_ratio(model.get_weights())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(starting_points)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "5 in cls_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yay\n"
     ]
    }
   ],
   "source": [
    "least_likely = 5\n",
    "if least_likely in cls_samples and np.argmax(model_to_attack(cls_samples[least_likely])) != np.argmax(model_to_attack(x)):\n",
    "    starting_points.append(cls_samples[least_likely])\n",
    "else:\n",
    "    print('yay')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model_to_attack = model\n",
    "fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "init_attack = fb.attacks.DatasetAttack()\n",
    "batches = [(x_to_attack[:500], y_to_attack[:500]), (x_to_attack[500:], y_to_attack[500:])]\n",
    "\n",
    "# create attack that picks adversarials from given dataset of samples\n",
    "#init_attack = fb.attacks.DatasetAttack()\n",
    "init_attack = fb.attacks.DatasetAttack()\n",
    "\n",
    "init_attack.feed(fmodel, batches[0][0])   # feed 1st batch of inputs\n",
    "init_attack.feed(fmodel, batches[1][0])   # feed 2nd batch of inputs\n",
    "attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=30, steps=500,lr_num_decay=30, lr=1e7, init_attack=init_attack)\n",
    "adversarials, _, success = attack(\n",
    "    fmodel,\n",
    "    x_to_attack,\n",
    "    criterion=fb.criteria.Misclassification(y_to_attack),\n",
    "    epsilons=[None]\n",
    ")\n",
    "dists = [np.count_nonzero(x_to_attack[i]-adversarials[0][i]) for i in range(len(x_to_attack))]\n",
    "#return dists, ([x.numpy().tolist() for x in adversarials], success.numpy().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample = 429"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "label =  0\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(adversarials[0][sample], (28,28)))\n",
    "print(np.argmax(model_to_attack(adversarials[0][sample])))\n",
    "print('label = ',y_to_attack[sample].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.imshow(tf.reshape(x_to_attack[sample], (28,28)))\n",
    "np.argmax(model_to_attack(x_to_attack[sample]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.261"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e7\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.258"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e10\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.779"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e15\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.635"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e3 mit l0 dist datasetattack\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.218"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e3\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18.635"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e7 mit l0 dist datasetattack\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.352"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e7 mit l2 dist datasetattack\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.402"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e7 mit 5000 steps 50 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.19"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e5 mit 5000 steps 50 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14.014"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e3 mit 5000 steps 50 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.332"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr 1e3 mit 5000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16.215"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=5, lr 1e3 mit 5000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11.85"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=40, lr 1e3 mit 5000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13.066"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=40, lr 1e7 mit 5000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.63"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=30, lr 1e3 mit 1000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10.005"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=30, lr 1e2 mit 1000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.635"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e4 mit 1000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.667"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e4 mit 1000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e4 mit 1000 steps 20 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.125"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e5 mit 2000 steps 10 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.041"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e5 mit 500 steps 30 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.655"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e6 mit 500 steps 30 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.211"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e7 mit 500 steps 30 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8.208"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=40, lr 1e6 mit 500 steps 30 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12.12"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lr num decay=20, lr 1e7 mit 500 steps 40 binary steps\n",
    "sum(dists)/1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1000), dtype=bool, numpy=\n",
       "array([[ True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True,  True,  True,  True,  True,  True,  True,  True,  True,\n",
       "         True]])>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "success"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
