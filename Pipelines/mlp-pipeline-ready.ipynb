{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'MLP'\n",
    "#EXPERIMENT_NAME = 'cnn-global-magnitude-unstruct'\n",
    "ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import foolbox as fb\n",
    "import random\n",
    "import json\n",
    "\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.50.0'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numba\n",
    "numba.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 3)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5963 - accuracy: 0.8787 - val_loss: 1.5214 - val_accuracy: 0.9430\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 0\n",
      "(1.0, 266200, 266200)\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.5123 - accuracy: 0.9515 - val_loss: 1.5034 - val_accuracy: 0.9593\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4985 - accuracy: 0.9641 - val_loss: 1.4979 - val_accuracy: 0.9639\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4911 - accuracy: 0.9712 - val_loss: 1.4941 - val_accuracy: 0.9687\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4867 - accuracy: 0.9752 - val_loss: 1.4907 - val_accuracy: 0.9710\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4834 - accuracy: 0.9784 - val_loss: 1.4905 - val_accuracy: 0.9705\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4818 - accuracy: 0.9800 - val_loss: 1.4905 - val_accuracy: 0.9710\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4804 - accuracy: 0.9812 - val_loss: 1.4919 - val_accuracy: 0.9703\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4773 - accuracy: 0.9845 - val_loss: 1.4868 - val_accuracy: 0.9743\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4764 - accuracy: 0.9850 - val_loss: 1.4847 - val_accuracy: 0.9768\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4753 - accuracy: 0.9862 - val_loss: 1.4848 - val_accuracy: 0.9765\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4744 - accuracy: 0.9870 - val_loss: 1.4924 - val_accuracy: 0.9689\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4746 - accuracy: 0.9867 - val_loss: 1.4882 - val_accuracy: 0.9724\n",
      "(1.0, 266200, 266200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [03:30, 210.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 0\n",
      "(0.5, 133100, 266200)\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.4885 - accuracy: 0.9810 - val_loss: 1.4879 - val_accuracy: 0.9756\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4768 - accuracy: 0.9874 - val_loss: 1.4867 - val_accuracy: 0.9755\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4736 - accuracy: 0.9898 - val_loss: 1.4842 - val_accuracy: 0.9783\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4714 - accuracy: 0.9912 - val_loss: 1.4842 - val_accuracy: 0.9781\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4704 - accuracy: 0.9920 - val_loss: 1.4845 - val_accuracy: 0.9777\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4694 - accuracy: 0.9928 - val_loss: 1.4818 - val_accuracy: 0.9808\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4692 - accuracy: 0.9928 - val_loss: 1.4838 - val_accuracy: 0.9787\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4688 - accuracy: 0.9931 - val_loss: 1.4833 - val_accuracy: 0.9784\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4679 - accuracy: 0.9940 - val_loss: 1.4823 - val_accuracy: 0.9786\n",
      "(0.5, 133100, 266200)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [07:00, 210.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 0\n",
      "(0.25, 66550, 266200)\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 3s 3ms/step - loss: 1.5099 - accuracy: 0.9711 - val_loss: 1.4952 - val_accuracy: 0.9733\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4841 - accuracy: 0.9833 - val_loss: 1.4892 - val_accuracy: 0.9764\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4776 - accuracy: 0.9877 - val_loss: 1.4873 - val_accuracy: 0.9770\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4739 - accuracy: 0.9904 - val_loss: 1.4863 - val_accuracy: 0.9779\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4717 - accuracy: 0.9922 - val_loss: 1.4848 - val_accuracy: 0.9781\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4700 - accuracy: 0.9933 - val_loss: 1.4844 - val_accuracy: 0.9790\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4689 - accuracy: 0.9940 - val_loss: 1.4836 - val_accuracy: 0.9795\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4682 - accuracy: 0.9945 - val_loss: 1.4838 - val_accuracy: 0.9780\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4673 - accuracy: 0.9949 - val_loss: 1.4843 - val_accuracy: 0.9783\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4670 - accuracy: 0.9952 - val_loss: 1.4835 - val_accuracy: 0.9785\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4671 - accuracy: 0.9950 - val_loss: 1.4833 - val_accuracy: 0.9791\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4660 - accuracy: 0.9957 - val_loss: 1.4832 - val_accuracy: 0.9784\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4661 - accuracy: 0.9958 - val_loss: 1.4850 - val_accuracy: 0.9770\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4661 - accuracy: 0.9958 - val_loss: 1.4832 - val_accuracy: 0.9787\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4659 - accuracy: 0.9957 - val_loss: 1.4824 - val_accuracy: 0.9789\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4652 - accuracy: 0.9963 - val_loss: 1.4820 - val_accuracy: 0.9794\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4657 - accuracy: 0.9959 - val_loss: 1.4833 - val_accuracy: 0.9785\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4656 - accuracy: 0.9960 - val_loss: 1.4826 - val_accuracy: 0.9785\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 3ms/step - loss: 1.4654 - accuracy: 0.9962 - val_loss: 1.4821 - val_accuracy: 0.9791\n",
      "(0.25, 66550, 266200)\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune, Train Attack Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(structure='unstructured', method='random', scope='global', iterations=10):\n",
    "    \n",
    "    experiment_name = f'{ARCHITECTURE}-{method}-{scope}-{structure}'\n",
    "    pgd_success_rates = []\n",
    "    cw_success_rates = []\n",
    "    bb0_success_rates = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    #compression_rates = [1, 2, 4, 64]\n",
    "    compression_rates = [tf.math.pow(2, x).numpy() for x in range(7)]\n",
    "    pruning_ratios = [1-1/x for x in compression_rates]\n",
    "\n",
    "    for j in tqdm(range(iterations)):\n",
    "        accuracies = []\n",
    "        pgd_success_rate = []\n",
    "        cw_success_rate = []\n",
    "        bb0_success_rate = []\n",
    "        try: \n",
    "            del model\n",
    "        except:\n",
    "            ;\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = initialize_base_model(j, experiment_name=experiment_name, save_weights=True)\n",
    "        for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "\n",
    "            #model.load_weights(f'./saved-weights/{experiment_name}-{j}')\n",
    "            #print(f'./saved-weights/{experiment_name}-{j}')\n",
    "\n",
    "            #for i in range(index + 1):\n",
    "                print(f'current pruning ratio is{pruning_ratio}, current iteration is {j}')\n",
    "                #if i != index:\n",
    "\n",
    "                if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                    model.prune_random_global_unstruct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='global' and structure=='structured':\n",
    "                    model.prune_random_global_struct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                    model.prune_random_local_unstruct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='local' and structure=='structured':\n",
    "                    model.prune_random_local_struct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                    model.prune_magnitude_global_unstruct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                    model.prune_magnitude_global_struct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                    model.prune_magnitude_local_unstruct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                    model.prune_magnitude_local_struct(pruning_ratio)\n",
    "                else:\n",
    "                    raise ValueError(\"pruning method invalid\")\n",
    "\n",
    "                print(get_zeros_ratio(model.get_weights()))\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                              experimental_run_tf_function=False\n",
    "                             )\n",
    "                model = train_model(model, to_convergence=True)\n",
    "                print(get_zeros_ratio(model.get_weights()))\n",
    "                accuracies.append(model.evaluate(x_test, y_test, verbose=0))\n",
    "                pgd_success_rate.append(pgd_attack(model))\n",
    "                \n",
    "                bb0_success_rate.append(bb0_attack(model))\n",
    "                cw_success_rate.append(cw2_attack(model))\n",
    "        all_accuracies.append(accuracies)\n",
    "        pgd_success_rates.append(pgd_success_rate)\n",
    "        cw_success_rates.append(cw_success_rate)\n",
    "        bb0_success_rates.append(bb0_success_rate)\n",
    "    #write to csv and json\n",
    "    with open(f'saved-results/{experiment_name}-accuracies.json', 'w') as f:\n",
    "        json.dump(all_accuracies, f)\n",
    "\n",
    "    with open(f'saved-results/{experiment_name}-pgd-success.json', 'w') as f:\n",
    "        json.dump(pgd_success_rates, f)\n",
    "        \n",
    "    with open(f'saved-results/{experiment_name}-cw2-success.json', 'w') as f:\n",
    "        json.dump(cw_success_rates, f)\n",
    "        \n",
    "    with open(f'saved-results/{experiment_name}-bb0-success.json', 'w') as f:\n",
    "        json.dump(bb0_success_rates, f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bb0_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    cls_samples = {}\n",
    "    idx = 0\n",
    "    starting_points = []\n",
    "    while len(list(cls_samples)) != 10:\n",
    "        clean_pred = int(np.argmax(model_to_attack(tf.reshape(x_test[idx],(1,28*28)))))\n",
    "        cls_samples[clean_pred] = x_test[idx]\n",
    "        idx += 1\n",
    "    for x in list(x_to_attack):\n",
    "        most_unlikely = int(np.argmin(model_to_attack(tf.reshape(x,(1,28*28)))))\n",
    "        starting_points.append(cls_samples[most_unlikely])\n",
    "    starting_points = tf.convert_to_tensor(starting_points)\n",
    "    attack_successful = False\n",
    "    steps = 5000\n",
    "    directions = 5000\n",
    "    \n",
    "    attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=100, lr=1e7, init_attack=fb.attacks.LinearSearchBlendedUniformNoiseAttack(steps=steps,directions=directions,distance=fb.distances.linf))\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        starting_points=starting_points,\n",
    "        epsilons=[None]\n",
    "    )\n",
    "    \n",
    "    dists = [np.count_nonzero(x_to_attack[i]-adversarials[0][i]) for i in range(len(x_to_attack))]\n",
    "    return dists, ([x.numpy().tolist() for x in adversarials], success.numpy().tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zeros_ratio(weights):\n",
    "    layers_to_examine = [0,3,6]\n",
    "    all_weights = np.array([])\n",
    "    for x in layers_to_examine:\n",
    "\n",
    "        all_weights = np.append(all_weights, weights[x].flatten())\n",
    "    return np.count_nonzero(all_weights)/len(all_weights), np.count_nonzero(all_weights), len(all_weights)\n",
    "\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "\n",
    "    model = LeNet300_100()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, to_convergence=True):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=100,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "def pgd_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    adversarials = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        epsilons=[8/255]\n",
    "    )\n",
    "    return np.count_nonzero(adversarials[2])/len(y_to_attack)\n",
    "\n",
    "def cw2_attack(model_to_attack, eps=[.5]):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.L2CarliniWagnerAttack()\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        epsilons=eps\n",
    "    )\n",
    "    return np.count_nonzero(success)/len(y_to_attack)\n",
    "\n",
    "\n",
    "\n",
    "def bb0_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "    batches = [(x_to_attack[:500], y_to_attack[:500]), (x_to_attack[500:], y_to_attack[500:])]\n",
    "\n",
    "    # create attack that picks adversarials from given dataset of samples\n",
    "    #init_attack = fb.attacks.DatasetAttack()\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "\n",
    "    init_attack.feed(fmodel, batches[0][0])   # feed 1st batch of inputs\n",
    "    init_attack.feed(fmodel, batches[1][0])   # feed 2nd batch of inputs\n",
    "    attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=30, steps=500,lr_num_decay=30, lr=1e7, init_attack=init_attack)\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        criterion=fb.criteria.Misclassification(y_to_attack),\n",
    "        epsilons=[None]\n",
    "    )\n",
    "    dists = [np.count_nonzero(x_to_attack[i]-adversarials[0][i]) for i in range(len(x_to_attack))]\n",
    "    return dists, ([x.numpy().tolist() for x in adversarials], success.numpy().tolist())\n",
    "\n",
    "def prune_conv_layers(pruning_ratio):\n",
    "    layer_to_prune = [0, 3]\n",
    "    pruned_weights = model.get_weights()\n",
    "    \n",
    "    for layer in layer_to_prune:\n",
    "        converted_weights = convert_from_hwio_to_iohw(model.get_weights()[layer])\n",
    "        converted_mask = convert_from_hwio_to_iohw(model.get_weights()[layer + 2]).numpy()\n",
    "        for input_index, input_layer in enumerate(converted_weights):\n",
    "\n",
    "            for kernel_index, kernel in enumerate(input_layer):\n",
    "                dims = kernel.shape\n",
    "                flat_weights = kernel.numpy().flatten()\n",
    "                flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "                #print(no_of_weights_to_prune)\n",
    "                indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "\n",
    "                converted_mask[input_index][kernel_index] = flat_masks.reshape(dims)\n",
    "        back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "        pruned_weights[layer+2] = back_converted_mask\n",
    "    \n",
    "    return pruned_weights\n",
    "\n",
    "def convert_from_hwio_to_iohw(weights_nchw):\n",
    "    return tf.transpose(weights_nchw, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "def convert_from_iohw_to_hwio(weights_nhwc):\n",
    "    return tf.transpose(weights_nhwc, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "def get_average_accuracies(all_accuracies):\n",
    "    acc_per_pruning_rate=[]\n",
    "    for i in range(len(all_accuracies)):\n",
    "        for j in range(len(all_accuracies[i])):\n",
    "\n",
    "            try:\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "            except:\n",
    "                acc_per_pruning_rate.append([])\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "    avg_acc_per_pruning_rate = [sum(x)/len(x) for x in acc_per_pruning_rate]; avg_acc_per_pruning_rate\n",
    "    return avg_acc_per_pruning_rate\n",
    "\n",
    "def get_average_success_rates(all_success_rates):\n",
    "    success_per_pruning_rate=[]\n",
    "    for i in range(len(all_success_rates)):\n",
    "        for j in range(len(all_success_rates[i])):\n",
    "\n",
    "            try:\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "            except:\n",
    "                success_per_pruning_rate.append([])\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "    avg_success_per_pruning_rate = [sum(x)/len(x) for x in success_per_pruning_rate];avg_success_per_pruning_rate\n",
    "    return avg_success_per_pruning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x_to_attack = tf.convert_to_tensor(x_train[:1000].reshape(1000,28*28))\n",
    "y_to_attack = tf.convert_to_tensor([y_train[:1000]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self,inputs, units=32, activation='relu'):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.w = self.add_weight(shape=(inputs, self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                name='unpruned_weights')\n",
    "        self.b = self.add_weight(shape=(self.units), initializer='zeros', trainable=True, name='b')\n",
    "        self.mask = self.add_weight(shape=(self.w.shape),\n",
    "                                    initializer='ones',\n",
    "                                    trainable=False,\n",
    "                                   name='pruning_mask')\n",
    "\n",
    "    #def build(self, input_shape):\n",
    "        #print(input_shape)\n",
    "        \n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #self.mask_2 = tf.multiply(self.mask, self.mask_2)\n",
    "        x = tf.multiply(self.w, self.mask)\n",
    "        #print(self.pruned_w.eval())\n",
    "        x = tf.matmul(inputs, x)\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return tf.keras.activations.relu(x)\n",
    "        if self.activation == 'softmax':\n",
    "            return tf.keras.activations.softmax(x)\n",
    "        raise ValueError('Activation function not implemented')\n",
    "\n",
    "class LeNet300_100(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet300_100, self).__init__()\n",
    "        self.dense1 = CustomLayer(28*28,300)\n",
    "        self.dense2 = CustomLayer(300,100)\n",
    "        self.dense3 = CustomLayer(100,10, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = layers.Flatten()(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "    \n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = []\n",
    "            weights = self.get_weights()\n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        \n",
    "                        no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                        # find unpruned weights\n",
    "                        non_zero_weights = np.nonzero(flat_masks)[0]\n",
    "                        # calculate the amount of weights to be pruned this round\n",
    "                        no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                        # shuffle all non-zero weights\n",
    "                        random.shuffle(non_zero_weights)\n",
    "                        # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                        indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                        \n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            dense_layer_to_prune = [0,3,6]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "                    no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self, ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "        #flat out all weights:\n",
    "        conv_layer_to_prune = [0, 3]\n",
    "        dense_layer_to_prune = [6, 9, 12]\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            flat_weights = np.append(flat_weights, weights[x])\n",
    "            flat_mask = np.append(flat_mask, weights[x+2])\n",
    "            \n",
    "        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[x + 2] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        def prune_conv_layers_locally(self, ratio):\n",
    "            conv_layer_to_prune = []\n",
    "            #print('inside conv prune func',get_zeros_ratio(self.get_weights()))\n",
    "            weights = self.get_weights()\n",
    "            \n",
    "            for layer in conv_layer_to_prune:\n",
    "                converted_weights = convert_from_hwio_to_iohw(weights[layer]).numpy()\n",
    "                converted_mask = convert_from_hwio_to_iohw(weights[layer + 2]).numpy()\n",
    "                for input_index, input_layer in enumerate(converted_weights):\n",
    "                    for kernel_index, kernel in enumerate(input_layer):\n",
    "                        shape = kernel.shape\n",
    "                        flat_weights = kernel.flatten()\n",
    "                        flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                        #flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                        #flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                        #print(no_of_weights_to_prune)\n",
    "                        #indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "\n",
    "\n",
    "                        for idx_to_delete in indices_to_delete:\n",
    "                            flat_masks[idx_to_delete] = 0\n",
    "                            flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                        converted_mask[input_index][kernel_index] = flat_masks.reshape(shape)\n",
    "                        converted_weights[input_index][kernel_index] = flat_weights.reshape(shape)\n",
    "                back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "                back_converted_weights = convert_from_iohw_to_hwio(converted_weights)\n",
    "                weights[layer] = back_converted_weights\n",
    "                weights[layer+2] = back_converted_mask\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            \n",
    "            dense_layer_to_prune = [0,3,6]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "\n",
    "                    no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                    indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_conv_layers_locally(self,ratio)\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "        return\n",
    "    \n",
    "    def prune_random_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                #print(channels)\n",
    "                non_zero_channels = np.nonzero([np.sum(channel) for channel in channels])[0]\n",
    "                #print(non_zero_channels)\n",
    "                no_of_channels_to_prune_left = no_of_channels_to_prune - (len(channels) - len(non_zero_channels))\n",
    "                random.shuffle(non_zero_channels)\n",
    "                channels_to_prune = non_zero_channels[:no_of_channels_to_prune_left]\n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return True\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                random.shuffle(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return True\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = []\n",
    "        dense_layers_to_prune = [0,3,6]\n",
    "        prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    def prune_magnitude_local_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            for x in conv_layers_to_prune:\n",
    "\n",
    "                vals = []\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[x])\n",
    "                iohw_mask = convert_from_hwio_to_iohw(weights[x+2])\n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                \n",
    "                mask = tf.reshape(iohw_mask, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "                for channel in channels:\n",
    "                    vals.append(tf.math.reduce_sum(tf.math.abs(channel)))\n",
    "                channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "                for channel_to_prune in channels_to_prune:\n",
    "                    channels[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "                    mask[channel_to_prune] = tf.zeros([converted_shape[2],converted_shape[3]])\n",
    "\n",
    "                reshaped_mask = tf.reshape(mask, converted_shape)\n",
    "                reshaped_weights = tf.reshape(channels, converted_shape)\n",
    "                weights[x] = convert_from_iohw_to_hwio(reshaped_weights)\n",
    "                weights[x+2] = convert_from_iohw_to_hwio(reshaped_mask)\n",
    "            #self.set_weights(weights)\n",
    "            return weights\n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = []\n",
    "        dense_layers_to_prune = [0,3,6]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio):\n",
    "        def prune_conv_layers(conv_layers_to_prune, weights):\n",
    "            #kernels = []\n",
    "            #converted_shape = iohw_weights.shape\n",
    "            \n",
    "            #no_of_channels_to_prune = int(np.round(ratio * no_of_channels))\n",
    "            #channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "            all_channels = np.empty((0,5,5))\n",
    "            original_shapes = []\n",
    "            for layer_to_prune in conv_layers_to_prune:\n",
    "                iohw_weights = convert_from_hwio_to_iohw(weights[layer_to_prune])\n",
    "                \n",
    "                converted_shape = iohw_weights.shape\n",
    "                no_of_channels = converted_shape[0]*converted_shape[1]\n",
    "                channels = tf.reshape(iohw_weights, (no_of_channels,converted_shape[2],converted_shape[3])).numpy()\n",
    "\n",
    "                all_channels = np.concatenate((all_channels, channels))\n",
    "            mask = np.ones(all_channels.shape)\n",
    "\n",
    "            vals = [np.sum(np.abs(channel)) for channel in all_channels]\n",
    "            no_of_channels_to_prune = int(ratio * len(vals))\n",
    "            channels_to_prune = np.argsort(vals)[:no_of_channels_to_prune]\n",
    "\n",
    "            for channel_to_prune in channels_to_prune:\n",
    "                all_channels[channel_to_prune] = tf.zeros((5,5))\n",
    "                mask[channel_to_prune] = tf.zeros((5,5))\n",
    "                \n",
    "            z = 0\n",
    "            for i, layer_to_prune in enumerate(conv_layers_to_prune):\n",
    "                original_shape = convert_from_hwio_to_iohw(weights[layer_to_prune]).shape\n",
    "                pruned_layer = tf.reshape(all_channels[z:z + original_shape[0]*original_shape[1]], original_shape)\n",
    "                weights[layer_to_prune] = convert_from_iohw_to_hwio(pruned_layer)\n",
    "                z = original_shape[0]*original_shape[1]\n",
    "            \n",
    "            \n",
    "                \n",
    "            return weights\n",
    "        def prune_dense_layers(conv_layers_to_prune, weights):\n",
    "            return True\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = []\n",
    "        dense_layers_to_prune = [0,3,6]\n",
    "        weights = prune_conv_layers(conv_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        #raise Warning('Not yet implemented')\n",
    "        return False\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "469/469 [==============================] - 1s 3ms/step - loss: 1.6021 - accuracy: 0.8780 - val_loss: 1.5294 - val_accuracy: 0.9374\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f8734811850>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LeNet300_100()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "              \n",
    "             )\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )\n",
    "#model.save('./saved-models/mini-pipeline-CNN-baseline-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tf.reshape(x_to_attack,(1000,28,28))\n",
    "fmodel = fb.models.TensorFlowModel(model, bounds=(0,1))\n",
    "attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=10, lr=1e-1,)# init_attack=init_attack)\n",
    "adversarials, _, success = attack(\n",
    "    fmodel,\n",
    "    x_to_attack,\n",
    "    criterion=fb.criteria.Misclassification(y_to_attack),\n",
    "    epsilons=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_attack = model\n",
    "fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "init_attack = fb.attacks.DatasetAttack()\n",
    "batches = [(x_to_attack[:500], y_to_attack[:500]), (x_to_attack[500:], y_to_attack[500:])]\n",
    "\n",
    "# create attack that picks adversarials from given dataset of samples\n",
    "#init_attack = fb.attacks.DatasetAttack()\n",
    "init_attack = fb.attacks.DatasetAttack()\n",
    "\n",
    "init_attack.feed(fmodel, batches[0][0])   # feed 1st batch of inputs\n",
    "init_attack.feed(fmodel, batches[1][0])   # feed 2nd batch of inputs\n",
    "attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=10, steps=5000,lr_num_decay=40, lr=1e7, init_attack=init_attack)\n",
    "adversarials, _, success = attack(\n",
    "    fmodel,\n",
    "    x_to_attack,\n",
    "    criterion=fb.criteria.Misclassification(y_to_attack),\n",
    "    epsilons=[None]\n",
    ")\n",
    "dists = [np.count_nonzero(x_to_attack[i]-adversarials[0][i]) for i in range(len(x_to_attack))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dists=[]\n",
    "\n",
    "dists = [np.count_nonzero(x_to_attack[i]-adversarials[i]) for i in range(len(x_to_attack))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sum(dists)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sum(dists)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(dists)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(sum(dists)/1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=0\n",
    "model_to_attack = model\n",
    "plt.imshow(tf.reshape(adversarials[sample], (28,28)))\n",
    "print(np.argmax(model_to_attack(tf.reshape(adversarials[sample],(1,784)))))\n",
    "print('label = ',y_to_attack[sample].numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.imshow(tf.reshape(x_to_attack[sample], (28,28)))\n",
    "np.argmax(model_to_attack(tf.reshape(x_to_attack[sample],(1,784))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_to_attack(tf.reshape(x_to_attack[sample],(1,784)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
