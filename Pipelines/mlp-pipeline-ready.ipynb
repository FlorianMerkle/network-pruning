{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "ARCHITECTURE = 'MLP'\n",
    "EXPERIMENT_TYPE = 'fixed-eps'\n",
    "ITERATIONS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-64db26dc5a82>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0m__future__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mabsolute_import\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdivision\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprint_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_literals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "from __future__ import absolute_import, division, print_function, unicode_literals\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras import layers\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import foolbox as fb\n",
    "import random\n",
    "import json\n",
    "\n",
    "#tf.compat.v1.enable_eager_execution()\n",
    "tf.keras.backend.clear_session()  # For easy reset of notebook state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'0.45.1'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numba\n",
    "numba.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Physical GPUs, 1 Logical GPUs\n"
     ]
    }
   ],
   "source": [
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    # Restrict TensorFlow to only allocate 1GB * 2 of memory on the first GPU\n",
    "    try:\n",
    "        tf.config.experimental.set_virtual_device_configuration(\n",
    "            gpus[0],\n",
    "            [tf.config.experimental.VirtualDeviceConfiguration(memory_limit=1024 * 4)])\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Virtual devices must be set before GPUs have been initialized\n",
    "        print(e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_experiment(\n",
    "    structure='unstructured', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6257 - accuracy: 0.8465 - val_loss: 1.5209 - val_accuracy: 0.9448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 0\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[515 592 317  82 276 128 685 318 440 217 771 384 430 513 336 630 677   2\n",
      " 499 464 240 246 273 402 482 115 420 313 206 585 693 376 169 472 709 703\n",
      " 738 314 604 262 544 341 488 265 288  54 746 212 387 195 609 179 731 140\n",
      " 474 601 296 226 460 574 250 761 622 180 199 425 421 407  59 343 537 369\n",
      " 669 490 639 570 168 403  63 286  84  62 750 378  35 607   3 697  99 122\n",
      " 173 523 760 254 162 238 442 428 227 734  34  64 680 624 190 494 783  15\n",
      " 575 434 720 616 491 580 395 678 188 681 301 486  44 591 349 150 379 644\n",
      " 252 623 461 182 485 559 423 302 777 579 470 414 175 455 690 554  66 412\n",
      " 132 120 348 401 329 743 596 417 452 229 665 489 324 473  55 255 465   8\n",
      " 542 662 335 581 156 759 769 586 551 664 653 449 500 762 507 184 363 326\n",
      " 304 374  14 621 303 268 144 498 298 659 640 241 538 471 413 754 758 312\n",
      " 239 715 550 689 102 529 204 315 509 235  78 736 147 266 620 541 234 130\n",
      " 126 429  49 589 566 577 399 511 346 328 433 691 632 576 564 552 458 506\n",
      "  27 269 463 114 422 730 740 172 641 626 145 193 478 277 116 360 752 110\n",
      " 295 196  60 701 735 451 569 406 745  65  20 332 508  98 462  31 125 391\n",
      " 309  95 139  93 166 158 553 755 373 361 725 155 299 136 321 439 257 721\n",
      " 718 483 426 456 221 768  97  57 208 264 124 400 119 149 411 631 530  72\n",
      "  17 565 163   1 289 404 354  94 493 578 390 638 410 571 453 137  83 646\n",
      " 611  74 322 517 394 362 258  96 191 650  42 153 614  70  41 484 679 683\n",
      " 181  29 106 534 272 744 606 331  39 652 129 107 599 244 477 389  91 293\n",
      " 279 668 345 726 757 380 357 388 339 216 702 595 358  52 261 682 704 386\n",
      " 590  28 249 290 351 687 248 176 397 528  85 549 447 497 561 330 253 737\n",
      " 291 753  13 111  24 778 323 214 546 405 492 419  73 127 143  69 501 648\n",
      " 535 496 716 635 476 767 666 594 539 712 270 118 525 300 381 242 441 367\n",
      " 219 600 316 385 531 466 567 443 645 260 275 231 603  26 770 121 152 108\n",
      " 613 643 371 597 584 717 532 459 165 724 642 713 656 245 278 556 694 415\n",
      " 608 676 148  48 135 161 133 692 598 469  25 619 131  88 338 101  67 448\n",
      " 174 170 342 522 223 654 398 710 177 197 142 154  79 773 543 782  56 487\n",
      " 113 547  10 185 588  22  87 308 393 516 714 375 636 151 764 292 409 280\n",
      " 352 210 520 700 655 437 334 359  21 183 749 686 673 634 100 282 230 215\n",
      " 141 521   6 392   5 618 481  38 510 661 670 705 695  76 583 445  36 436\n",
      " 198 138 672 205 475 504 612 629 186 756 627 432 533 480 610 502 605 519\n",
      " 742 355 319 776 503 189 526 707 479 236 274 696 164 112 200 663 741 536\n",
      " 733 370 779  92 202 560   7  86 557 209 340 651 457 748 467 558 747 325\n",
      " 382  37  75  11 187 427 602 722 310 514 727 548 167 527 192 593  30  58\n",
      "  23  45 284 568 435 647   4 213 723 350 555 418 134 657 243  68 763 364\n",
      " 468  89 444 728  12 505 637 337 285 649 256 751 625 563  32 159 283 772\n",
      " 109 582 259 711 220  53 450  19 416 103 454 495 524  50  40 224 372  16\n",
      "  81 356 383 587 178 203 671 117   0 368 573 775 540 660 572 320 222 160\n",
      "  18 267 615 617 281 424 237 708 105 353 305 765 311   9 194 225  43 104\n",
      " 263 307 408 333 347 781 123 251 739 396 729 271 297 201 157 438  51 545\n",
      " 774 366 233 698 667 287 766 218  61 699 732 207 431 518 211 171 377  46\n",
      "  77 512 633 344 232  71 247 688  90 658 294 365 674 684 719 446  33  47\n",
      " 228 562  80 628 327 706 306 146 675 780]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[ 61 247  11 111 290  81 150 184 190 131 114  98 201 250  45 277 221 159\n",
      " 175 108 154 206 129  75 153  74 109   1   5 251 121 297 103 173 204  39\n",
      " 298 141 261 289 288 269 118 299 195  52 227 177 276  69 296 170 101 156\n",
      " 210  10  96  70 136 241  27  15 293 151 174 230  46 285 232  66  20  67\n",
      " 219 148 233 249  21  76   8 106 274 105 286  44  63 149 127 242 199  83\n",
      " 235  80 185 246  65  53 234 133 272 191  43 130 279 217 163 116 178 145\n",
      "  38  54 224 212 104 194 107 262  49 122 273 248 140 220 278 229 168  14\n",
      " 226 294  29  18 244 164 275 231 132 115 264  47 160 171 257   0  99 102\n",
      " 256 126  95 162 146  57  13  19  35 222 100 284  92 142 161 187  60  48\n",
      " 120  82 119  58 179 112 237 166 198 213 243 254 152 236  59  79  33 138\n",
      " 228 211  16  87  91  34 216  73  32 200 183 139  64   7 202 110  55 181\n",
      "  93  40 205 292 271  42 143   4  24  56 283 158 255  25 155  22  85 123\n",
      "  50   9 223 203 193  77 182  88  51 147  30 218 215 197  28 260 245 240\n",
      " 265 238 192 225  26  37  84 252  31  36  72 263 117 253 268  71   2 259\n",
      " 189  89  41 134 209  94 180 282 196   3 208  23 124 135 165 157 291  97\n",
      " 267   6 125  17 266 128 144 186 113 270 258 169 188 295  12 167 176  90\n",
      " 214  78 287 137  62  86 207  68 281 172 239 280]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[66 40 13 62 19 52  9 15 88 29 50 25 44 81 28 89 99 24 79 43 39 48 20 78\n",
      " 96 80 17 30 35  6 92 87 57  0 60 45 97 58 37 59 68  3 31 67 75  5 65 93\n",
      " 84 61 26 11 14 72 64 10  7 23 51 22 74 49 36 73 47 18 21 91 90 38 53 12\n",
      "  8 16 95 98 94 63 34 77  4 82 71 76 69 32 86  1 27  2 41 56 42 70 55 83\n",
      " 33 85 46 54]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5128 - accuracy: 0.9509 - val_loss: 1.5030 - val_accuracy: 0.9592\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4987 - accuracy: 0.9643 - val_loss: 1.4960 - val_accuracy: 0.9671\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4919 - accuracy: 0.9702 - val_loss: 1.4917 - val_accuracy: 0.9708\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4869 - accuracy: 0.9752 - val_loss: 1.4901 - val_accuracy: 0.9717\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4837 - accuracy: 0.9780 - val_loss: 1.4875 - val_accuracy: 0.9742\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4814 - accuracy: 0.9804 - val_loss: 1.4899 - val_accuracy: 0.9713\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4794 - accuracy: 0.9823 - val_loss: 1.4843 - val_accuracy: 0.9775\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4774 - accuracy: 0.9841 - val_loss: 1.4872 - val_accuracy: 0.9741\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4770 - accuracy: 0.9845 - val_loss: 1.4864 - val_accuracy: 0.9749\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4756 - accuracy: 0.9859 - val_loss: 1.4831 - val_accuracy: 0.9786\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4746 - accuracy: 0.9869 - val_loss: 1.4835 - val_accuracy: 0.9773\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4742 - accuracy: 0.9871 - val_loss: 1.4838 - val_accuracy: 0.9776\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4730 - accuracy: 0.9883 - val_loss: 1.4856 - val_accuracy: 0.9757\n",
      "WARNING:tensorflow:From /home/florian/dev/foolbox/foolbox/models/tensorflow.py:13: is_gpu_available (from tensorflow.python.framework.test_util) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.config.list_physical_devices('GPU')` instead.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:27, 147.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 0\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[548 660 240 621 140 588 566 205 245  20 283 399 487 764 551 266 509  86\n",
      " 612 370 122  85 238 393 636 541 285 714 307 386 339 332  38 326 586 232\n",
      " 552 436 539 759 260 522 538 396 377 121 350 662 165 686 264 607 349 606\n",
      " 671 401 625 422 767 187 647 234 610 530 311 343  10 136  58 411 410  92\n",
      " 581 644 705 486 201 556  70 755 257 381 652 685 736  13 516 188 553 513\n",
      "  19 346  54  71 557 590 229 269 750 734 561 519 452 465 728  81  39 774\n",
      " 579 694 483 737  24 146 174 748 559  28 655 261 293 223 657 536 450 230\n",
      " 155 443 115 502 363 501 713 565 297 418 642 605 466 376 152 302 583 718\n",
      " 430 697 476 364 281  12 130 231 723 656 354 520 534 626 132 247  59 451\n",
      "  84 725 321 573 670 301  91 479 425 474 766 780 757  95 569 241  17 678\n",
      " 547 407 769 701 531 454 333 217 709 100 741 453 455 416 356 117  14 554\n",
      " 681 611 712 568 515 178 102 719 310 118 335 623 498 546 251 724 433 271\n",
      " 649 695  49   3  37 159  57 624 288 540 684 228 135 189 226 771 254 124\n",
      " 669 371 331 176 687 600 406 482 463 658 111 342  29 591  43 496 284  99\n",
      " 745 221 560 143 274  89 739  80  74 779 139 512 528 444 495 593 353  78\n",
      " 420 120 682  82 403 183 634 236  18 280  23 324 776  40 464 237 732 448\n",
      "  45 510 175 162 394 651 142 367 462 397 544 489   6 385 446  55 700 369\n",
      " 197 646 706 225 303 753 219 368 503 199 227   5 751 618 578 191 777  21\n",
      " 758 497 292 110 334 754 355 337  90 156 535  44   1 360 181 341 571 233\n",
      " 345 180 265 441 783 148 378 580 323 123 286 460 315 564 589 574 633 253\n",
      " 173 126 475 282  11  67 375 211 184 773 164 507 125 212   8 157 438 291\n",
      "  16 224 585 151 772 296 390 672 305 267 105 635 277 688 299 458 641 604\n",
      " 434 400 445 461 562 549 392 720 298 504  77  72 414 484  32 471 128 208\n",
      " 150 715 402 218  25 630 506 213 149  51 429 222 158 101 601 112  87 384\n",
      " 328 493 500 190 729 760 532 104 235 599 243 380 352  94 412 432 648 614\n",
      " 147 765 309 426 439 287 409 653 160 362 276 675 711 730 207 442 166 521\n",
      " 325 256 627  69 659 116 248 194 193 567 608 491 746  36 144 473 749 673\n",
      " 543 677 632  48 374 480 192 290 615 169 304 683 595  60 517 322 587 762\n",
      " 763 703 761 696 255 744 680 499 472  26 131 664   2 258 306 289 249 382\n",
      " 270 145 427 527 505 597  47 676 134 177 637 391 708 469  97 691 576 415\n",
      " 167 467 457 320 727 389 138 209  50 545 273 692 478 327 596 555 206 619\n",
      "  22 572 747 133 308 336 638 667 161 494 344 279  63 435 768 702   9 417\n",
      " 357 272 278 735 663 490 340 413 690 524 103 387  31 594 319 127  64  27\n",
      " 361 419 698   7 643 259  62 470 537 679 640  79 447 348  34 721 752 542\n",
      " 119  53 137 317 108   4 239 584  41 613 294 523 214  68 699 372 533 477\n",
      " 726 141 106 424 202 263   0 775 408 743 778 707 716 182 704 481 525 107\n",
      " 129 616  96 485 244 186 592 405 246 330 220 300 731 602 163  61 570 358\n",
      " 113 379 431 373  66 329 770 404 204 459 312  35 437 395 262  42 488 421\n",
      "  88 338 195 388  75 200 582 456 639 168 423 514 366 563 577 398  46 650\n",
      " 756 198 215  98 468  76 172 347 661 203 603 598 575  83 216 114 314 629\n",
      " 693 109 620  93 674 179  30 268 171 170 252 722 781 250 740 154  33 440\n",
      " 665 645 275  15 742 782  56  52 738 185 710 449 622  65 526 196 518 666\n",
      "  73 733 628 631 153 359 689 668 295 558 550 529 351 318 717 617 365 492\n",
      " 609 511 242 508 654 210 428 383 313 316]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[263  91  25 165 122 108  59 210  18 217 113  96 211 148  72 196  11  71\n",
      "  77 136 151 206 172  38  80 158  51  99 279 232  31 219 155 291  20  94\n",
      " 157 127 164 166 143 281 203 215 135  48  56 109  32 278  30 220  37  35\n",
      " 228 185 176 128 168  29  26 216 134  62  60  86  76 223 141 118 189 107\n",
      " 255 156 295 226 266  36 153  61  79 296 204 260 289 237 175 129 159  23\n",
      "  34 114 254   6 124  90  33 207 188  69  98  21 174 298  95  49 144  57\n",
      " 181 280 235 257 173 200 240 252 275  28 212   3 163 245 119 283 250 115\n",
      " 205 202 213 147 191  78 261  88  93 214   7 292   0 287  39   1 139  42\n",
      " 102 233 251  92  13 195  74  83 137  75 130 218 160  70 268 234 112 146\n",
      " 100 222  81 239 249 246 116 167 224  65  45  85  67 267 120 227 243 126\n",
      " 101 229 285  50 110 262 131  22 270  40  43 244 145 105 103 192 272   4\n",
      " 197  84   9  64 162  17 121  44 149 236 142  53 290 183 177 265 297  82\n",
      " 140 248  41  58 201  66 294 138 133 271 198 208 186 299 256   2 241  12\n",
      " 258 123 247 282  47 231 106 259  52 274  46  19 178  15  63 288 253 225\n",
      " 269 154 161 194 193 276 264  54  97 242  16  24  14 104  68 180 293   8\n",
      " 171  87 190 286  89 199 277 273 170 238  73 221 230 152  55 117   5 182\n",
      " 111 169  27 284 125 187 150 209 184  10 179 132]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[ 5 86 36 35 32 69 28 38 97  0 58 93 64 61 49 96 21 52 20 19 40 76 12  6\n",
      " 24 94 68 74 72 77 92 44 34 26 84 29 73 62 65 55 15 91 71 56 89 16 63 45\n",
      " 82 30  2 53 59  7  3  8 23 22 90 41  1 18 43 27 79 42 75 87 10 51 31 85\n",
      " 14 48 25  9 67 98 95 81 83 37 39 13 70 17 33  4 11 50 47 57 54 66 60 99\n",
      " 88 46 80 78]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4981 - accuracy: 0.9712 - val_loss: 1.4923 - val_accuracy: 0.9726\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4826 - accuracy: 0.9820 - val_loss: 1.4901 - val_accuracy: 0.9725\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4783 - accuracy: 0.9853 - val_loss: 1.4878 - val_accuracy: 0.9752\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9872 - val_loss: 1.4875 - val_accuracy: 0.9743\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4738 - accuracy: 0.9892 - val_loss: 1.4876 - val_accuracy: 0.9748\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4729 - accuracy: 0.9896 - val_loss: 1.4868 - val_accuracy: 0.9750\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4717 - accuracy: 0.9907 - val_loss: 1.4851 - val_accuracy: 0.9776\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4704 - accuracy: 0.9917 - val_loss: 1.4878 - val_accuracy: 0.9748\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4709 - accuracy: 0.9911 - val_loss: 1.4853 - val_accuracy: 0.9762\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4698 - accuracy: 0.9921 - val_loss: 1.4882 - val_accuracy: 0.9733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:24, 138.21s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 0\n",
      "rows to prune in layer 0 : 588\n",
      "[  0   2   4   7   9  15  22  25  26  27  30  31  32  33  34  35  36  41\n",
      "  42  46  47  48  50  51  52  53  56  60  61  62  63  64  65  66  68  69\n",
      "  72  73  75  76  77  79  83  87  88  93  94  96  97  98 101 103 104 106\n",
      " 107 108 109 112 113 114 116 119 127 128 129 131 133 134 137 138 141 144\n",
      " 145 147 149 150 153 154 158 160 161 163 166 167 168 169 170 171 172 177\n",
      " 179 182 185 186 190 192 193 194 195 196 198 200 202 203 204 206 207 208\n",
      " 209 210 213 214 215 216 218 220 222 235 239 242 243 244 246 248 249 250\n",
      " 252 255 256 258 259 262 263 268 270 272 273 275 276 278 279 287 289 290\n",
      " 294 295 298 299 300 304 306 308 309 312 313 314 316 317 318 319 320 322\n",
      " 325 327 328 329 330 336 338 340 344 347 348 351 352 357 358 359 361 362\n",
      " 365 366 372 373 374 379 380 382 383 384 387 388 389 391 392 395 398 400\n",
      " 402 404 405 408 409 412 413 414 415 417 419 421 423 424 426 427 428 429\n",
      " 431 432 434 435 437 439 440 442 445 447 449 456 457 458 459 461 467 468\n",
      " 469 470 471 472 473 477 478 480 481 484 485 488 490 491 492 493 494 499\n",
      " 500 504 505 506 508 511 514 517 518 521 523 524 525 526 527 529 532 533\n",
      " 537 542 543 545 549 550 555 558 562 563 567 570 572 575 576 577 582 584\n",
      " 587 592 594 595 596 597 598 599 601 602 603 604 608 609 613 614 615 616\n",
      " 617 619 620 622 627 628 629 630 631 632 637 638 639 640 641 643 645 648\n",
      " 650 653 654 659 661 663 664 665 666 667 668 673 674 675 676 677 679 680\n",
      " 683 689 690 691 692 693 696 698 699 702 703 704 707 708 710 711 715 716\n",
      " 717 720 721 722 726 727 729 730 731 733 735 738 740 742 743 744 746 747\n",
      " 749 752 756 760 761 762 763 765 768 770 775 778 781 782]\n",
      "[683 344  94  33 309 170 429 481 258 704 768 435 749 314 622 222 244 488\n",
      " 214 601 357 144 218  22 673 308   4 304 472 107 456 404 484 289 781 104\n",
      " 193 702 336 598 620  88 760 147 639 584   9 492 298 198 150 171 711  62\n",
      " 742 545 504  93 707 740  26 592 614 641 527 248 158 477  77 602 374 608\n",
      " 413  83  32 318  66  68 414  35 761 442 380 431  36 106 613 597 149  63\n",
      " 765 663 529 320  96  53 279 782 276 206 338 419 319 192 402 365 500 505\n",
      " 295 467 716 415  51 114  76 424 703 252 667 746 167 494 721 109  69 400\n",
      " 113 654 640  31 562 392 270 366 449 596 674 690 699 543 708 361 300 563\n",
      " 210 362 555 525 432 587 720 340 680 287 391 480 423 537 648 186 108 628\n",
      " 615 717 461 421 514 195 664 154 645 215 630 306 523 675 352 168 485 255\n",
      " 738 691  65 506 730 526 383 131 508  64 372 268 213 459 722 246 558 609\n",
      " 576 220 313 322 428 470 389 661 134 650 275 427 439 638 161 676 533 243\n",
      " 194  27 594 666 491 351 163 250 417 599 518  30 153 698 273 643 729 239\n",
      " 299 604 138 207 763 325 668 582 665 570  72 209 603 204  87 447 141 524\n",
      " 103 166  52  97 348 262 499 312 116 133 747 631 412  47 190  75  42 263\n",
      " 182 358  61 575  60 458 196 256 129 696 290 384 617 235 595 735  15 637\n",
      " 160 542 316 756 317 169 203 762 752  50 202 619 379 408 208 726 770 493\n",
      " 478  73 517 692  25 743 172 457 775 405 629 490 679 119 693 128 185 177\n",
      " 112 409 382 145   0 200 532 715  48  41 689 388 632  46  56 259 440 294\n",
      " 567   2 473 395  98 653 426 359 710 677 549 330 627 328 521 469 445 278\n",
      " 577 733 434 347   7 272  34 550 387 616 471 511 373 249 127 659 179 329\n",
      " 398 572 437 242 216 727 468 327  79 744 778 101 137 731]\n",
      "rows to prune in layer 3 : 225\n",
      "[  2   4   5   8   9  10  12  14  15  16  17  19  22  24  27  40  41  43\n",
      "  44  45  46  47  50  52  53  54  55  58  63  64  65  66  67  68  70  73\n",
      "  74  75  81  82  83  84  85  87  89  97 100 101 103 104 105 106 110 111\n",
      " 112 116 117 120 121 123 125 126 130 131 132 133 137 138 140 142 145 146\n",
      " 149 150 152 154 160 161 162 167 169 170 171 177 178 179 180 182 183 184\n",
      " 186 187 190 192 193 194 197 198 199 201 208 209 218 221 222 224 225 227\n",
      " 229 230 231 234 236 238 239 241 242 243 244 246 247 248 249 253 256 258\n",
      " 259 262 264 265 267 268 269 270 271 272 273 274 276 277 282 284 285 286\n",
      " 288 290 293 294 297 299]\n",
      "[110 180 125 198  47 270  52 265 186 236 247  15 137 194 273 100  83 131\n",
      " 262 132 274 167  16 234 284  70 190 193 177 126 229  14   8 267 288   2\n",
      "  74 106  82 253 249 258 225  24 154  45 290 130  12 239  19  67 286 222\n",
      " 231 179 192 299 162 241 142 246  66  65  89  43  97 133  50 150 161 272\n",
      "  40 218 152   4 294 149  10 169 140 224 277 120 197 146 116 271 160  63\n",
      " 282 105 104 121  44 293 138 276 230 269  54 201  55  87  22  68  58  64\n",
      "  73 264  81 297  41 256 242 243  75 209 101 199 170 112 184   9 145  84\n",
      " 182   5 227  27  46 248 171  17 244 238 117 123 187  53 221 208 178 259\n",
      " 285 183 103 268  85 111]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 1  2  3  4  7  8  9 10 11 13 14 17 18 22 23 25 27 31 33 37 39 41 42 43\n",
      " 46 47 48 50 51 53 54 57 59 60 66 67 70 75 78 79 80 81 83 85 87 88 90 95\n",
      " 98 99]\n",
      "[ 1 98 33 83 90 41 48 22  2 27 17 43 11 80  8 59 14  7  9  3 70 66 10 53\n",
      " 87 46 31 25 37 18 60 67 81 75  4 51 13 95 47 42 50 78 23 79 85 39 57 99\n",
      " 88 54]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5529 - accuracy: 0.9304 - val_loss: 1.5221 - val_accuracy: 0.9489\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5100 - accuracy: 0.9593 - val_loss: 1.5107 - val_accuracy: 0.9570\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5007 - accuracy: 0.9667 - val_loss: 1.5074 - val_accuracy: 0.9574\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4955 - accuracy: 0.9708 - val_loss: 1.5031 - val_accuracy: 0.9596\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4921 - accuracy: 0.9736 - val_loss: 1.5022 - val_accuracy: 0.9611\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4894 - accuracy: 0.9756 - val_loss: 1.5015 - val_accuracy: 0.9616\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4874 - accuracy: 0.9772 - val_loss: 1.4993 - val_accuracy: 0.9629\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4856 - accuracy: 0.9785 - val_loss: 1.4985 - val_accuracy: 0.9638\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4841 - accuracy: 0.9801 - val_loss: 1.4983 - val_accuracy: 0.9644\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4828 - accuracy: 0.9809 - val_loss: 1.4983 - val_accuracy: 0.9649\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4821 - accuracy: 0.9815 - val_loss: 1.4985 - val_accuracy: 0.9630\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4810 - accuracy: 0.9823 - val_loss: 1.4987 - val_accuracy: 0.9635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:19, 131.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 0\n",
      "rows to prune in layer 0 : 686\n",
      "[  0   2   7  15  25  27  30  34  41  42  46  47  48  50  52  56  60  61\n",
      "  72  73  75  79  87  97  98 101 103 112 116 119 127 128 129 133 134 137\n",
      " 138 141 145 153 160 161 163 166 169 172 177 179 182 185 190 194 196 200\n",
      " 202 203 204 207 208 209 216 220 235 239 242 243 249 250 256 259 262 263\n",
      " 272 273 275 278 290 294 299 312 313 316 317 322 325 327 328 329 330 347\n",
      " 348 351 358 359 373 379 382 384 387 388 389 395 398 405 408 409 412 417\n",
      " 426 427 428 434 437 439 440 445 447 457 458 468 469 470 471 473 478 490\n",
      " 491 493 499 511 517 518 521 524 532 533 542 549 550 558 567 570 572 575\n",
      " 576 577 582 594 595 599 603 604 609 616 617 619 627 629 631 632 637 638\n",
      " 643 650 653 659 661 665 666 668 676 677 679 689 692 693 696 698 710 715\n",
      " 726 727 729 731 733 735 743 744 747 752 756 762 763 770 775 778]\n",
      "[532 629  79  98 575 358 220 137 653 473 208 550 116 203 595  25 417 348\n",
      "  47 163  46 603 659 439 134 328 693 733 434 428 313 427 469 262  56 558\n",
      " 752 395 409 445 196 250 696 661 567 412 166 239 103 447 172 679 698 290\n",
      " 138 756 775 128 359  97 379 329 478 249 470  30 518 542 119 278 263 182\n",
      " 468 322 524 666 643 216 710 207 204 637 325 604 747 577 256 160  60 382\n",
      " 387 145   7 200  75 194 408 312 242 179 549 384  87 161 517 619 317  50\n",
      " 743 457 499  72 327 763 731 299 389 665 426 141 572 533 440 668  27 490\n",
      " 112 770 493 153 127 521 692 177 373 609 398 650 491   2 275 726 631  52\n",
      " 729 778 594  41 169 576 273 101  15  61 347 243 627 190  34 570 677 616\n",
      " 351 471  42  48   0 209 638 599 511 617 235 388 185 405 676  73 316 762\n",
      " 272 202 715 582 437 129 744 259 330 689 632 294 735 133 727 458]\n",
      "rows to prune in layer 3 : 262\n",
      "[  4   5   9  10  17  22  27  41  44  46  53  54  55  58  63  64  68  73\n",
      "  75  81  84  85  87 101 103 104 105 111 112 116 117 120 121 123 138 140\n",
      " 145 146 149 160 169 170 171 178 182 183 184 187 197 199 201 208 209 221\n",
      " 224 227 230 238 242 243 244 248 256 259 264 268 269 271 276 277 282 285\n",
      " 293 294 297]\n",
      "[238 244  87 171 182   5 170  41 140 259 285 209 264  68 282 138 271  64\n",
      "  84  73  54 112  75 121  27   9 269 183  63 169 111  17 120 146 224  10\n",
      " 160 117 145 248  46 101 187 116 123  85 256 277  58 201 297 178 276 293\n",
      " 208 103 221  53 227  55  22 104 197  81 243 199 184 105 230   4 294  44\n",
      " 268 242 149]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 4 13 18 23 25 31 37 39 42 46 47 50 51 54 57 60 67 75 78 79 81 85 88 95\n",
      " 99]\n",
      "[23  4 85 18 67 78 75 54 50 25 81 95 99 42 60 51 57 13 47 31 37 39 46 79\n",
      " 88]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6656 - accuracy: 0.8356 - val_loss: 1.5886 - val_accuracy: 0.8908\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5802 - accuracy: 0.8967 - val_loss: 1.5677 - val_accuracy: 0.9038\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5647 - accuracy: 0.9072 - val_loss: 1.5582 - val_accuracy: 0.9106\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5561 - accuracy: 0.9140 - val_loss: 1.5545 - val_accuracy: 0.9136\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5503 - accuracy: 0.9186 - val_loss: 1.5488 - val_accuracy: 0.9165\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5458 - accuracy: 0.9228 - val_loss: 1.5445 - val_accuracy: 0.9212\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5423 - accuracy: 0.9247 - val_loss: 1.5416 - val_accuracy: 0.9237\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5391 - accuracy: 0.9282 - val_loss: 1.5420 - val_accuracy: 0.9224\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5367 - accuracy: 0.9301 - val_loss: 1.5394 - val_accuracy: 0.9258\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5343 - accuracy: 0.9316 - val_loss: 1.5381 - val_accuracy: 0.9252\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5325 - accuracy: 0.9340 - val_loss: 1.5382 - val_accuracy: 0.9254\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5310 - accuracy: 0.9346 - val_loss: 1.5369 - val_accuracy: 0.9264\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5293 - accuracy: 0.9363 - val_loss: 1.5348 - val_accuracy: 0.9286\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5278 - accuracy: 0.9379 - val_loss: 1.5356 - val_accuracy: 0.9281\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5269 - accuracy: 0.9384 - val_loss: 1.5360 - val_accuracy: 0.9261\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5257 - accuracy: 0.9400 - val_loss: 1.5330 - val_accuracy: 0.9310\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5242 - accuracy: 0.9412 - val_loss: 1.5331 - val_accuracy: 0.9302\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5234 - accuracy: 0.9418 - val_loss: 1.5333 - val_accuracy: 0.9302\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5226 - accuracy: 0.9424 - val_loss: 1.5321 - val_accuracy: 0.9305\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5217 - accuracy: 0.9429 - val_loss: 1.5322 - val_accuracy: 0.9316\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5212 - accuracy: 0.9433 - val_loss: 1.5306 - val_accuracy: 0.9324\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5200 - accuracy: 0.9451 - val_loss: 1.5321 - val_accuracy: 0.9304\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5197 - accuracy: 0.9451 - val_loss: 1.5307 - val_accuracy: 0.9321\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5191 - accuracy: 0.9458 - val_loss: 1.5324 - val_accuracy: 0.9302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [08:56, 139.00s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 0\n",
      "rows to prune in layer 0 : 735\n",
      "[  0   2  15  27  34  41  42  48  50  52  61  72  73  87 101 112 127 129\n",
      " 133 141 153 161 169 177 179 185 190 202 209 235 242 243 259 272 273 275\n",
      " 294 299 316 317 327 330 347 351 373 384 388 389 398 405 426 437 440 457\n",
      " 458 471 490 491 493 499 511 517 521 533 549 570 572 576 582 594 599 609\n",
      " 616 617 619 627 631 632 638 650 665 668 676 677 689 692 715 726 727 729\n",
      " 731 735 743 744 762 763 770 778]\n",
      "[631 770 521 129 458 616 242 426 259 190  87 668 677  50 638 330 511 517\n",
      " 185 762 299 388 177 729 351 243 272  15 676 179  73 778 373 576  42 384\n",
      " 743 112 347  48 437   0 202 689 457 572 665   2 275 715 493 133 169 209\n",
      " 692  72 389 440 570 141 727 609 294 490 533 235 317 582 491 405 273 726\n",
      " 731 398 153 594 744 549  52 101  41 127 499  61  34 627 650 599 327  27\n",
      " 619 161 632 316 735 763 471 617]\n",
      "rows to prune in layer 3 : 281\n",
      "[  4  22  44  46  53  55  58  81  85 101 103 104 105 116 117 123 145 149\n",
      " 178 184 187 197 199 201 208 221 227 230 242 243 248 256 268 276 277 293\n",
      " 294 297]\n",
      "[277 117 256 199  85 184 276   4 243 116 208 294 201  53 227 123 242 187\n",
      " 248  44 297 104  22 103  55 221 178 149  81 268 145 293  58 230  46 101\n",
      " 105 197]\n",
      "rows to prune in layer 6 : 93\n",
      "[13 31 37 39 42 46 47 51 57 60 79 88 99]\n",
      "[60 13 51 31 39 37 42 46 88 47 79 99 57]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0086 - accuracy: 0.4604 - val_loss: 1.8983 - val_accuracy: 0.5699\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8921 - accuracy: 0.5752 - val_loss: 1.8732 - val_accuracy: 0.5932\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8767 - accuracy: 0.5882 - val_loss: 1.8647 - val_accuracy: 0.6006\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8695 - accuracy: 0.5955 - val_loss: 1.8596 - val_accuracy: 0.6027\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8563 - accuracy: 0.6088 - val_loss: 1.8270 - val_accuracy: 0.6397\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7946 - accuracy: 0.6733 - val_loss: 1.7708 - val_accuracy: 0.6977\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7730 - accuracy: 0.6951 - val_loss: 1.7597 - val_accuracy: 0.7086\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7646 - accuracy: 0.7036 - val_loss: 1.7531 - val_accuracy: 0.7130\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7589 - accuracy: 0.7088 - val_loss: 1.7484 - val_accuracy: 0.7173\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7546 - accuracy: 0.7124 - val_loss: 1.7485 - val_accuracy: 0.7183\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7514 - accuracy: 0.7148 - val_loss: 1.7437 - val_accuracy: 0.7216\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7486 - accuracy: 0.7178 - val_loss: 1.7418 - val_accuracy: 0.7232\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7458 - accuracy: 0.7198 - val_loss: 1.7375 - val_accuracy: 0.7255\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7436 - accuracy: 0.7219 - val_loss: 1.7379 - val_accuracy: 0.7271\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7418 - accuracy: 0.7232 - val_loss: 1.7351 - val_accuracy: 0.7279\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7397 - accuracy: 0.7261 - val_loss: 1.7344 - val_accuracy: 0.7282\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7382 - accuracy: 0.7268 - val_loss: 1.7319 - val_accuracy: 0.7314\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7363 - accuracy: 0.7293 - val_loss: 1.7315 - val_accuracy: 0.7323\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7352 - accuracy: 0.7300 - val_loss: 1.7311 - val_accuracy: 0.7312\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7339 - accuracy: 0.7315 - val_loss: 1.7294 - val_accuracy: 0.7320\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7327 - accuracy: 0.7321 - val_loss: 1.7292 - val_accuracy: 0.7323\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7312 - accuracy: 0.7339 - val_loss: 1.7276 - val_accuracy: 0.7342\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7305 - accuracy: 0.7351 - val_loss: 1.7277 - val_accuracy: 0.7340\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7296 - accuracy: 0.7351 - val_loss: 1.7267 - val_accuracy: 0.7360\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7287 - accuracy: 0.7358 - val_loss: 1.7265 - val_accuracy: 0.7355\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7278 - accuracy: 0.7368 - val_loss: 1.7266 - val_accuracy: 0.7341\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7272 - accuracy: 0.7375 - val_loss: 1.7264 - val_accuracy: 0.7365\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7266 - accuracy: 0.7381 - val_loss: 1.7250 - val_accuracy: 0.7376\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7258 - accuracy: 0.7386 - val_loss: 1.7254 - val_accuracy: 0.7367\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7253 - accuracy: 0.7397 - val_loss: 1.7245 - val_accuracy: 0.7380\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7247 - accuracy: 0.7399 - val_loss: 1.7255 - val_accuracy: 0.7376\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7245 - accuracy: 0.7396 - val_loss: 1.7241 - val_accuracy: 0.7394\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7241 - accuracy: 0.7406 - val_loss: 1.7247 - val_accuracy: 0.7378\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7236 - accuracy: 0.7409 - val_loss: 1.7238 - val_accuracy: 0.7389\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7229 - accuracy: 0.7416 - val_loss: 1.7239 - val_accuracy: 0.7389\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7227 - accuracy: 0.7412 - val_loss: 1.7237 - val_accuracy: 0.7377\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7225 - accuracy: 0.7418 - val_loss: 1.7231 - val_accuracy: 0.7383\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7218 - accuracy: 0.7420 - val_loss: 1.7232 - val_accuracy: 0.7381\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7215 - accuracy: 0.7426 - val_loss: 1.7243 - val_accuracy: 0.7382\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7218 - accuracy: 0.7418 - val_loss: 1.7231 - val_accuracy: 0.7384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:07, 154.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 0\n",
      "rows to prune in layer 0 : 759\n",
      "[ 27  34  41  52  61  72 101 127 133 141 153 161 169 209 235 273 294 316\n",
      " 317 327 389 398 405 440 471 490 491 493 499 533 549 570 582 594 599 609\n",
      " 617 619 627 632 650 692 715 726 727 731 735 744 763]\n",
      "[582 153 731  34 398 317 493 549 294 609 235 327 727 440 127 744 599 650\n",
      " 389  27  61 273 627 101 133 499 763 692 169  72 405 619 141  52 726 735\n",
      "  41 209 490 471 570 533 491 316 594 715 617 161 632]\n",
      "rows to prune in layer 3 : 290\n",
      "[ 22  44  46  55  58  81 101 103 104 105 145 149 178 197 221 230 268 293\n",
      " 297]\n",
      "[101  81  46 268 221 293 103  58 145  44 149  22 105 297 178 230 197  55\n",
      " 104]\n",
      "rows to prune in layer 6 : 96\n",
      "[42 46 47 57 79 88 99]\n",
      "[79 42 99 88 47 57 46]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1272 - accuracy: 0.3338 - val_loss: 2.0204 - val_accuracy: 0.4522\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0103 - accuracy: 0.4594 - val_loss: 1.9892 - val_accuracy: 0.4790\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9812 - accuracy: 0.4882 - val_loss: 1.9599 - val_accuracy: 0.5118\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9622 - accuracy: 0.5091 - val_loss: 1.9425 - val_accuracy: 0.5276\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9491 - accuracy: 0.5206 - val_loss: 1.9299 - val_accuracy: 0.5375\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9402 - accuracy: 0.5265 - val_loss: 1.9223 - val_accuracy: 0.5455\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9343 - accuracy: 0.5323 - val_loss: 1.9180 - val_accuracy: 0.5467\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9301 - accuracy: 0.5348 - val_loss: 1.9137 - val_accuracy: 0.5509\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9268 - accuracy: 0.5377 - val_loss: 1.9107 - val_accuracy: 0.5531\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9238 - accuracy: 0.5398 - val_loss: 1.9073 - val_accuracy: 0.5567\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9192 - accuracy: 0.5450 - val_loss: 1.9032 - val_accuracy: 0.5611\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9150 - accuracy: 0.5493 - val_loss: 1.8996 - val_accuracy: 0.5640\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9113 - accuracy: 0.5525 - val_loss: 1.8945 - val_accuracy: 0.5719\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9077 - accuracy: 0.5570 - val_loss: 1.8900 - val_accuracy: 0.5739\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9043 - accuracy: 0.5592 - val_loss: 1.8864 - val_accuracy: 0.5784\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9017 - accuracy: 0.5622 - val_loss: 1.8834 - val_accuracy: 0.5819\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8996 - accuracy: 0.5643 - val_loss: 1.8824 - val_accuracy: 0.5829\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8979 - accuracy: 0.5663 - val_loss: 1.8803 - val_accuracy: 0.5834\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8963 - accuracy: 0.5679 - val_loss: 1.8807 - val_accuracy: 0.5834\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8949 - accuracy: 0.5686 - val_loss: 1.8785 - val_accuracy: 0.5856\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8935 - accuracy: 0.5706 - val_loss: 1.8775 - val_accuracy: 0.5850\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8921 - accuracy: 0.5717 - val_loss: 1.8765 - val_accuracy: 0.5840\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8908 - accuracy: 0.5722 - val_loss: 1.8743 - val_accuracy: 0.5876\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8895 - accuracy: 0.5737 - val_loss: 1.8738 - val_accuracy: 0.5892\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8882 - accuracy: 0.5747 - val_loss: 1.8718 - val_accuracy: 0.5902\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8867 - accuracy: 0.5768 - val_loss: 1.8702 - val_accuracy: 0.5940\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8855 - accuracy: 0.5775 - val_loss: 1.8697 - val_accuracy: 0.5906\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8842 - accuracy: 0.5797 - val_loss: 1.8678 - val_accuracy: 0.5964\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8830 - accuracy: 0.5806 - val_loss: 1.8669 - val_accuracy: 0.5949\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8819 - accuracy: 0.5811 - val_loss: 1.8660 - val_accuracy: 0.5960\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8806 - accuracy: 0.5831 - val_loss: 1.8647 - val_accuracy: 0.5960\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8797 - accuracy: 0.5835 - val_loss: 1.8642 - val_accuracy: 0.5958\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8784 - accuracy: 0.5851 - val_loss: 1.8615 - val_accuracy: 0.6023\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8761 - accuracy: 0.5877 - val_loss: 1.8609 - val_accuracy: 0.6032\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8749 - accuracy: 0.5895 - val_loss: 1.8587 - val_accuracy: 0.6044\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8739 - accuracy: 0.5902 - val_loss: 1.8583 - val_accuracy: 0.6049\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8728 - accuracy: 0.5913 - val_loss: 1.8579 - val_accuracy: 0.6025\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8723 - accuracy: 0.5912 - val_loss: 1.8575 - val_accuracy: 0.6052\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8717 - accuracy: 0.5921 - val_loss: 1.8560 - val_accuracy: 0.6071\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8710 - accuracy: 0.5925 - val_loss: 1.8550 - val_accuracy: 0.6094\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8703 - accuracy: 0.5940 - val_loss: 1.8551 - val_accuracy: 0.6065\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8699 - accuracy: 0.5945 - val_loss: 1.8544 - val_accuracy: 0.6087\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8694 - accuracy: 0.5943 - val_loss: 1.8533 - val_accuracy: 0.6096\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8689 - accuracy: 0.5946 - val_loss: 1.8540 - val_accuracy: 0.6075\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8686 - accuracy: 0.5947 - val_loss: 1.8536 - val_accuracy: 0.6092\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8682 - accuracy: 0.5948 - val_loss: 1.8530 - val_accuracy: 0.6087\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8678 - accuracy: 0.5956 - val_loss: 1.8525 - val_accuracy: 0.6108\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8675 - accuracy: 0.5962 - val_loss: 1.8518 - val_accuracy: 0.6102\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8670 - accuracy: 0.5967 - val_loss: 1.8511 - val_accuracy: 0.6129\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8666 - accuracy: 0.5971 - val_loss: 1.8510 - val_accuracy: 0.6128\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8664 - accuracy: 0.5964 - val_loss: 1.8508 - val_accuracy: 0.6103\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8657 - accuracy: 0.5975 - val_loss: 1.8500 - val_accuracy: 0.6113\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8656 - accuracy: 0.5975 - val_loss: 1.8499 - val_accuracy: 0.6133\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8653 - accuracy: 0.5980 - val_loss: 1.8499 - val_accuracy: 0.6103\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8648 - accuracy: 0.5985 - val_loss: 1.8499 - val_accuracy: 0.6123\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8647 - accuracy: 0.5986 - val_loss: 1.8492 - val_accuracy: 0.6128\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8643 - accuracy: 0.5988 - val_loss: 1.8491 - val_accuracy: 0.6153\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8642 - accuracy: 0.5990 - val_loss: 1.8490 - val_accuracy: 0.6138\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8639 - accuracy: 0.5990 - val_loss: 1.8488 - val_accuracy: 0.6128\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8635 - accuracy: 0.5994 - val_loss: 1.8484 - val_accuracy: 0.6131\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8631 - accuracy: 0.5998 - val_loss: 1.8481 - val_accuracy: 0.6129\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8630 - accuracy: 0.5998 - val_loss: 1.8479 - val_accuracy: 0.6112\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8627 - accuracy: 0.6000 - val_loss: 1.8489 - val_accuracy: 0.6117\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8626 - accuracy: 0.5996 - val_loss: 1.8472 - val_accuracy: 0.6136\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8621 - accuracy: 0.6004 - val_loss: 1.8473 - val_accuracy: 0.6144\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8619 - accuracy: 0.6010 - val_loss: 1.8471 - val_accuracy: 0.6162\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8616 - accuracy: 0.6003 - val_loss: 1.8467 - val_accuracy: 0.6148\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8614 - accuracy: 0.6008 - val_loss: 1.8469 - val_accuracy: 0.6158\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8609 - accuracy: 0.6016 - val_loss: 1.8486 - val_accuracy: 0.6120\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8609 - accuracy: 0.6017 - val_loss: 1.8467 - val_accuracy: 0.6149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [16:02, 178.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 0\n",
      "rows to prune in layer 0 : 771\n",
      "[ 41  52  72 133 141 161 169 209 316 405 471 490 491 499 533 570 594 617\n",
      " 619 632 692 715 726 735 763]\n",
      "[715 617 405 490 491 316 726 133 735 209 471 594 570 763 169 533 161 632\n",
      " 499  52 692 619 141  72  41]\n",
      "rows to prune in layer 3 : 295\n",
      "[ 22  44  55 104 105 149 178 197 230 297]\n",
      "[297 149  44  22  55 104 197 178 105 230]\n",
      "rows to prune in layer 6 : 98\n",
      "[46 47 57 88]\n",
      "[46 88 57 47]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3395 - accuracy: 0.1064 - val_loss: 2.3241 - val_accuracy: 0.1073\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2805 - accuracy: 0.1724 - val_loss: 2.2736 - val_accuracy: 0.1784\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2689 - accuracy: 0.1847 - val_loss: 2.2716 - val_accuracy: 0.1761\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2661 - accuracy: 0.1873 - val_loss: 2.2698 - val_accuracy: 0.1783\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2641 - accuracy: 0.1886 - val_loss: 2.2687 - val_accuracy: 0.1791\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2621 - accuracy: 0.1898 - val_loss: 2.2652 - val_accuracy: 0.1846\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2601 - accuracy: 0.1906 - val_loss: 2.2642 - val_accuracy: 0.1846\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2593 - accuracy: 0.1912 - val_loss: 2.2634 - val_accuracy: 0.1855\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2586 - accuracy: 0.1916 - val_loss: 2.2638 - val_accuracy: 0.1833\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2582 - accuracy: 0.1915 - val_loss: 2.2629 - val_accuracy: 0.1860\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2577 - accuracy: 0.1923 - val_loss: 2.2626 - val_accuracy: 0.1834\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2572 - accuracy: 0.1937 - val_loss: 2.2617 - val_accuracy: 0.1935\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2570 - accuracy: 0.1930 - val_loss: 2.2610 - val_accuracy: 0.1937\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2567 - accuracy: 0.1934 - val_loss: 2.2605 - val_accuracy: 0.1939\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2565 - accuracy: 0.1937 - val_loss: 2.2607 - val_accuracy: 0.1932\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2563 - accuracy: 0.1940 - val_loss: 2.2611 - val_accuracy: 0.1928\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2561 - accuracy: 0.1949 - val_loss: 2.2598 - val_accuracy: 0.1948\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2559 - accuracy: 0.1948 - val_loss: 2.2595 - val_accuracy: 0.1948\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2556 - accuracy: 0.1950 - val_loss: 2.2594 - val_accuracy: 0.1952\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2554 - accuracy: 0.1953 - val_loss: 2.2602 - val_accuracy: 0.1927\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2552 - accuracy: 0.1953 - val_loss: 2.2587 - val_accuracy: 0.1956\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2550 - accuracy: 0.1952 - val_loss: 2.2589 - val_accuracy: 0.1959\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2548 - accuracy: 0.1952 - val_loss: 2.2582 - val_accuracy: 0.1955\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2546 - accuracy: 0.1956 - val_loss: 2.2581 - val_accuracy: 0.1955\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2544 - accuracy: 0.1954 - val_loss: 2.2576 - val_accuracy: 0.1955\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2542 - accuracy: 0.1957 - val_loss: 2.2572 - val_accuracy: 0.1963\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2539 - accuracy: 0.1958 - val_loss: 2.2570 - val_accuracy: 0.1964\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2535 - accuracy: 0.1961 - val_loss: 2.2573 - val_accuracy: 0.1958\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2534 - accuracy: 0.1955 - val_loss: 2.2567 - val_accuracy: 0.1964\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2532 - accuracy: 0.1958 - val_loss: 2.2561 - val_accuracy: 0.1962\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2530 - accuracy: 0.1955 - val_loss: 2.2558 - val_accuracy: 0.1964\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2528 - accuracy: 0.1965 - val_loss: 2.2565 - val_accuracy: 0.1950\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2526 - accuracy: 0.1957 - val_loss: 2.2557 - val_accuracy: 0.1966\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2523 - accuracy: 0.1962 - val_loss: 2.2557 - val_accuracy: 0.1961\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2521 - accuracy: 0.1966 - val_loss: 2.2563 - val_accuracy: 0.1940\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2520 - accuracy: 0.1967 - val_loss: 2.2561 - val_accuracy: 0.1952\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2519 - accuracy: 0.1963 - val_loss: 2.2547 - val_accuracy: 0.1973\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2516 - accuracy: 0.1963 - val_loss: 2.2541 - val_accuracy: 0.1980\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2517 - accuracy: 0.1968 - val_loss: 2.2546 - val_accuracy: 0.1971\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2516 - accuracy: 0.1966 - val_loss: 2.2549 - val_accuracy: 0.1959\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2514 - accuracy: 0.1964 - val_loss: 2.2541 - val_accuracy: 0.1981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [18:06, 155.21s/it]\u001b[A\n",
      " 10%|█         | 1/10 [18:09<2:43:23, 1089.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5761 - accuracy: 0.8994 - val_loss: 1.5188 - val_accuracy: 0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 1\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[723 575 290 765 365  27 516 372 760 699 543  83 353  11 139 545  86 135\n",
      " 455 233 520 309 302 700 505 314 193 766 749 725  29 406  89 377 190 768\n",
      " 224 116 631 698  58 645 638 692 665 685 214 295 452 110  77 722 416   0\n",
      " 316 114 152 238 591 111 651 615 292 526 145 489 361 311  69 272 449 278\n",
      " 601 485 383 739 779 186  56 249  95 371 103 301 454 740 677 573  70 451\n",
      " 619 124 440 656 609 404 463  59  50 229 508 529 130 542  73 117 681 684\n",
      " 446  54 424 287 750 530 629 378 713 437 158 293 276 704 247   9 623 235\n",
      " 595 538  25 102  87 777 610 325 339 527 241 570 312 173 515 216 783  24\n",
      " 727 490 394 550 611  41 349 661 519 533 624 100  61 565 561 367 328 718\n",
      "  47 330   7 185 525 642 412 584 748 459 400 379 346 483 528 465 652 112\n",
      " 279 326 506 226 414 354 315 733 693 239 479 697 596  43 648 405 385 435\n",
      " 382 358 582 521 125   5  45 334  48 553 403 438 413  49  18 554 495 187\n",
      " 136 507 359 778 690 650 197 536 731 200 168 464 181 605 184 299 432 738\n",
      " 724 761 368 616  60 232 706 120 156 179 336 166  94 410 430  80 534 603\n",
      " 132 281 746 387 127 188 350 113 245 240 174 473 708 752 419 138 274 236\n",
      " 628 392 771 569 373 251 753 348 636 659 673 399 587 109 128  53 143 764\n",
      " 540 105 248 329 664 167 755 160 265 572 742 219 217 319 204 478 137 593\n",
      " 686 577 621 649 225 256 133 175 126 231 578 195 484 491 408 192 472 782\n",
      "  71 741 374 728 163 730 612 627 362 155 381 564 305 421 300 407 598 223\n",
      "   6 321 183 456 253 298 176 170 397  84 386 626  85 147 237 680 751 165\n",
      " 675 203  14 259 154 294 140 580 702 523 714 134 453 544 320 637 773 261\n",
      " 780 662 189 715  38 257 344 266 688 343 357  65 759 121 150 142 384 333\n",
      " 390 391  21 726 555 369 442 517 420 683 537 630 644 286 415 282 342  33\n",
      " 532 461 687  12 481 107 425 614 475 366  34   8 129 234 557  62 380  92\n",
      " 552  26 250 720 157  91 448 218 338 743 331  35 323 721 594  67   2  78\n",
      " 267 763 304 562 522 606 716 208  13 202  31  36 772 480 146 242 701  51\n",
      " 215 389 122 513 597 396 599 264 707  66  22 674 213 539 268 263 123 496\n",
      " 148 119 418 104 434 411 775 476 747 275 586 395 171  99 335 632  16 458\n",
      " 563 180 762 756  40 512 666 592 164 571 198 296 370 658 678 734 646  57\n",
      " 736 471 283 547 221 443 583 776 468 462 770 303 161  68 131 423 207 141\n",
      "  88  30 634 604  81 647 625 211  19 466 212 228 101 144 744 441 351  17\n",
      " 488 551 318 574 436 347 567 271 710 511 769 220 576 431  42 285   4 227\n",
      " 717 493 444 639 280 243 705 322 487 774  32 588 297 427 162 492 398 209\n",
      " 579 482 531 781 194 502 494 607   3  44 177 535 568 375 151 509 641 613\n",
      " 549 602 566 153 467 660 324 617 417 695 735 469 559 470 291 172 341  37\n",
      " 477 191 450 426  76 388 635 277 447 307 205 757  52 269 260 556 201 620\n",
      " 541 169 313 600 402 499 663  20 758 560 709  75 590 679  63 745 199 767\n",
      " 589 668  82   1 667 196 754 719 486 546  28 457  79  39 270 439 337 732\n",
      " 622 332 497 640 352 355 360 422 356 244  10  15 306 255 500 230  46 393\n",
      " 182 581 289 689 558  93 682 510 671 222 429 254 118 503 106 643 653 669\n",
      " 284 308 210  72 364 428 703 676  96 262 310 317 524 501 327 258 149 246\n",
      "  23  74 376 711 737 340 504 206 363 672 514 401 433 608 409 159 655 273\n",
      "  55 657 445  97 115 712 345 696  64 108 252 585 618 694 518 654 633  90\n",
      "  98 460 474 729 498 670 548 178 691 288]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[104 151 269 128 177 139 175 248 195 255 134  49  15   1  87  52  57 161\n",
      "  27  37 141 187 213  13 137  45 266 222 276 250 292  84 271 289  12 138\n",
      "  41 235 232  92 298 143 110 220  89  83  35 253 108   9   5 114 224 234\n",
      "  77  14  31   4 153 197  40 207 268  86 194 149 145 287  82  34  70  66\n",
      " 241 102  81  80 127 181  26 281  58 257 210   3  23 166 133 159  65 144\n",
      " 212 150  42 132 192  56 146  72 199 185 189  63  28 196 105  17 107  78\n",
      " 200 295 270 168 286 282  74  60   6 116  73 173 291  22  30 228 180 258\n",
      " 277 279  88   8 186 297 246 229 184 240 214 254  19 152  97  95  59 191\n",
      " 206 147 208 238 236 249 130 118 125 167 216 101  21  33  39  68 129  64\n",
      " 183 252  48   7 165 261  62 205 288 131  94 202 119 285 280 126  61 155\n",
      "  79  46 148  36  24 290 112 111  71 121  93  90 203 190 174 113  85 256\n",
      "   2 272 179 209 215 136 115 251 296 211 247 244 117 169 245 265 231  96\n",
      " 260 284 239  16  69 278  50 109 274 243 140 124  38 264 273 100  43  20\n",
      " 226  29  47 198  76 193 158 204 263 237 225 176 233 182 103  53 201  67\n",
      " 219 283 170 294 242 178 221 135  44 157 163 106 217 230  99 262 172   0\n",
      " 267 299 275 218  51 164  32  10  98  91 142 162  25  11 293 171 156  75\n",
      " 223  54 227  55 123 160 120 188 259 122  18 154]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[51 66 78 52  5 56 75 84 11 47 16 60 91 77 98 38 32 81 80 62 87 61 54  8\n",
      " 12 65 70 37 10 21 88 94 40 50 64 26 13 96 59 29 35 85 71 41 55 28 68  2\n",
      "  4 45 43 17 24 25 19 33 82 86 31  9 89 79 15  1 72 20 67 97 99 92 53 83\n",
      "  7 63 57 49 30 48 76 42 27  6 73 22 95 90 58  3 14 18 36 34 44 46 39 23\n",
      " 74  0 93 69]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5127 - accuracy: 0.9512 - val_loss: 1.5027 - val_accuracy: 0.9603\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4993 - accuracy: 0.9631 - val_loss: 1.4959 - val_accuracy: 0.9671\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4918 - accuracy: 0.9704 - val_loss: 1.4914 - val_accuracy: 0.9707\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4871 - accuracy: 0.9752 - val_loss: 1.4896 - val_accuracy: 0.9729\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4839 - accuracy: 0.9781 - val_loss: 1.4873 - val_accuracy: 0.9738\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4814 - accuracy: 0.9805 - val_loss: 1.4910 - val_accuracy: 0.9701\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4797 - accuracy: 0.9819 - val_loss: 1.4883 - val_accuracy: 0.9726\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4783 - accuracy: 0.9834 - val_loss: 1.4873 - val_accuracy: 0.9738\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4773 - accuracy: 0.9842 - val_loss: 1.4829 - val_accuracy: 0.9783\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4762 - accuracy: 0.9854 - val_loss: 1.4891 - val_accuracy: 0.9726\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9860 - val_loss: 1.4862 - val_accuracy: 0.9748\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4741 - accuracy: 0.9873 - val_loss: 1.4848 - val_accuracy: 0.9759\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:59, 119.19s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 1\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[721 351  85 661 419 266 450 564 663 362 122 349 292  35 560 243 683 192\n",
      "  93 265 499  25  13 744 404 487 250 756 677  23 599 739  40 112 175 685\n",
      "  18 598 524 533 425 483 288 655 312 280 376 252 585 172 707 171 246 159\n",
      " 459 181 365 248 324 398 173 621 479 678 415 256 729 673 697 310 767 439\n",
      " 210 635 475 130 601 467 441 299 650 433 453 333 474 695 578 396 395 160\n",
      " 626 254  99 651 357 282 778 221 261 417  44 385  20 620 627 120 187 573\n",
      " 485  27 152 722 405 422 342 300 588 438 116 372 383 734 718 488 366 283\n",
      " 445 117 379  61  72 571 781 368  45 725  54  48 231 644 309 297 691 472\n",
      " 341 732 771 314 619 446 373 480 668 205 166 353 224 307 662 637 761 287\n",
      " 411 335  98 382 169 567 742 449  17 751 206 200 109 193  82 638 202 149\n",
      "  11 343 126 416 508 694  39 594 247 757 562 774  30   4 227 329 760 518\n",
      " 259 428 675 491 302 642 330 113 534 140 319  69 115  41 278 506 544  43\n",
      "  91 170   2 432 255 267 522 469 726 239 220 520 477  68 646 505  84 584\n",
      " 371 131 125  62 768 157 430 260 369 616 460 731 618  31  59 135 402 687\n",
      " 423 720 350 530  87 587 759 554 550 403 682 773 148 557 686 234 102 612\n",
      "  16 145 666 556  60 211 203 358 303 596  10 703 748 123 640 279 290 444\n",
      " 155 531 111 452 216 624 669 214 670 331 370  46 709  71 717 603 609 577\n",
      " 185 772 561 344 236 639 776 551 110 500 540  95 615 240 764 182 327 610\n",
      "  52 605 178 607 242 177 374 712 337 380   7 156 105 440 359 168 134 198\n",
      " 758  78 481 654 409 413 473 532  51 676 738 264 322 408 701  80  15 741\n",
      " 590  32 643 436 427  24 576 304 783 574 493 465  58 325 434  73 393 600\n",
      " 569 311 108 568 323 545 386 728 421 525 147 631 195 528 636 713 664 645\n",
      " 208 151 128 502 659 648 690 190 163 782 165  94 390 263 558 277 443 766\n",
      " 229 513 542 435 142 179 164 215 538 338 107 137 354  76  36 397 457 747\n",
      " 674 191 315 174 361  29 461 146 235 451 378 623 471 727 519 144  65 537\n",
      " 660 464 671 207 209 124 384 492 275 780 238 161 336 454 490 356 320 106\n",
      " 667 494 708 602 535 334 641 103 706 692  53 281 276 328 770 543 101 503\n",
      " 608 406 442 755 512 401 565 269 186 437 696 629 735 466 769  89 563 391\n",
      " 294 429 201  33  97 326 704 100 740 723 394 688 180  66 296 158 595 375\n",
      "   1 470 762 223 622 257 591 765 698 332 420 352  67 381 489 114 295  64\n",
      " 274 606 625 426  55 553 559 212 388 579 716 672 743 463 750 634 597 700\n",
      " 141 410   8 604 272 321  63 478 752 689   3 364 225 119 306 458 656 218\n",
      " 746 592  37  28 414 293 495 167 139  49 129 424 176  14  21  50 775   6\n",
      " 355 611 136 515 711 680 104 665 549 150 127 476 291 649 431  92 514 222\n",
      " 387 199 118   0 546 566 547 228 468 702 628 539 317 194 552  12 763 308\n",
      " 548 749 570 412 614 285 284 133   9  22 633 286 298 745 237 737 318 705\n",
      "  42 498 657 510 555 699 268 484 244 517 162 418 230 232  26 456 196 392\n",
      " 496 630 777 399 724 217 681 521 523 132 526 340 736 509  90 345 536 301\n",
      " 389  56  96  57 613 143 407 258 653 580 367 245 184 516 347 219 183 482\n",
      "  88 377 581 270 586 693 647 448 529 527 575 715 316 658  70 154 400 305\n",
      " 313 189 582 197 249 348 501 251 346  79  34 754 710  74 652 589 541 262\n",
      "  77 339 617  38  47  75 730 583 507 733  86   5 241 486 593 188 138 714\n",
      " 360 455 204 447 679 572 779  83 233 226  81 363 289 153 684 213  19 632\n",
      " 273 497 271 719 504 511 462 753 253 121]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[138 172 201 247   4 282 228 111 235 257 132 206 218 100 245 225 145 146\n",
      " 274 162 242 275  49 179 131 158 236 126 223 159 276 194   7  61 203 151\n",
      "  37  66 226 180 109 184 258 106 269 102 150  64 183 238 254  39 164 210\n",
      " 256 278 174 241 232  53 283 289 128 178 113 170  95  13 197  15 107 186\n",
      " 152  23   3 143 198 149 291 139 296 255  57 127 239  71 192  93  69  56\n",
      "  10  19 222  59 202  34 266 169  26  97  45 234 262 141  58  16 298 220\n",
      "  85 171 205  14 270 117 243  72 108   9  89 182 163 211 249  99  47   0\n",
      " 153  35 230 190  48 118 122 155 285 156 252  43  27  20 295  38 165  40\n",
      " 130 284 263  32 154 229 261 204  62 244  33  74 133 288 176 191  70 142\n",
      " 292 293 189 187 215 213 167 148 160  73 281 264  51   2 140  18  98 212\n",
      "  12 161  87  96  60  68 129  31  24   6  30 233 248 123  29 157 260 101\n",
      " 240 287  80 185 279  52 121 115 207 177 175 246  84 259 294 217 271 116\n",
      "  94 209 290 188 199  83 120 208  44 251 280 104  55  65 195  25 144 272\n",
      " 273  41 196 119 181  81   1 135  21 214 250 200 297 124  22  67 125 231\n",
      "  90  86  50  88  91 299  79 136  78 268 265 147  82 286   8 166  54 237\n",
      "  46 224  11  92  77  36 103 216 134 137  76 105 227 277 112 219 221 253\n",
      " 173 193  42 267 168  63 110  17   5  28  75 114]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[18 48 12 86 96  4 63 64 17 30 53 59 81 29 50 74  7 94 76 33 89 46 10 77\n",
      " 93 37 90 38 69 27 35 62 45 28 97 21 80 54 98 13  8  5 22 92  0  1 84 72\n",
      " 23 87 25 47 66  2 73 11 40 61 44 26 75  9 71 55 31 20 15 99 83 14 78 32\n",
      " 57 95 19 43 24 60  6 34 16 52 42 51 88 56  3 82 67 39 70 85 58 79 36 68\n",
      " 49 41 91 65]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4997 - accuracy: 0.9696 - val_loss: 1.4942 - val_accuracy: 0.9710\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4830 - accuracy: 0.9816 - val_loss: 1.4891 - val_accuracy: 0.9737\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4785 - accuracy: 0.9849 - val_loss: 1.4878 - val_accuracy: 0.9753\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9872 - val_loss: 1.4875 - val_accuracy: 0.9747\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4740 - accuracy: 0.9888 - val_loss: 1.4854 - val_accuracy: 0.9765\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4730 - accuracy: 0.9895 - val_loss: 1.4856 - val_accuracy: 0.9767\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4713 - accuracy: 0.9912 - val_loss: 1.4844 - val_accuracy: 0.9767\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4712 - accuracy: 0.9908 - val_loss: 1.4850 - val_accuracy: 0.9762\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9917 - val_loss: 1.4848 - val_accuracy: 0.9769\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4697 - accuracy: 0.9923 - val_loss: 1.4843 - val_accuracy: 0.9783\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4693 - accuracy: 0.9925 - val_loss: 1.4819 - val_accuracy: 0.9798\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4693 - accuracy: 0.9926 - val_loss: 1.4835 - val_accuracy: 0.9775\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4691 - accuracy: 0.9926 - val_loss: 1.4843 - val_accuracy: 0.9774\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4681 - accuracy: 0.9937 - val_loss: 1.4844 - val_accuracy: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:01, 120.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 1\n",
      "rows to prune in layer 0 : 588\n",
      "[  0   1   3   5   6   8   9  12  14  19  21  22  26  28  29  33  34  36\n",
      "  37  38  42  47  49  50  53  55  56  57  63  64  65  66  67  70  74  75\n",
      "  76  77  79  81  83  86  88  89  90  92  94  96  97 100 101 103 104 106\n",
      " 107 114 118 119 121 124 127 128 129 132 133 136 137 138 139 141 142 143\n",
      " 144 146 150 151 153 154 158 161 162 163 164 165 167 174 176 179 180 183\n",
      " 184 186 188 189 190 191 194 196 197 199 201 204 207 208 209 212 213 215\n",
      " 217 218 219 222 223 225 226 228 229 230 232 233 235 237 238 241 244 245\n",
      " 249 251 253 257 258 262 263 268 269 270 271 272 273 274 275 276 277 281\n",
      " 284 285 286 289 291 293 294 295 296 298 301 305 306 308 313 315 316 317\n",
      " 318 320 321 326 328 332 334 336 338 339 340 345 346 347 348 352 354 355\n",
      " 356 360 361 363 364 367 375 377 378 381 384 387 388 389 390 391 392 394\n",
      " 397 399 400 401 406 407 410 412 414 418 420 424 426 429 431 435 437 442\n",
      " 443 447 448 451 454 455 456 457 458 461 462 463 464 466 468 470 471 476\n",
      " 478 482 484 486 489 490 492 494 495 496 497 498 501 502 503 504 507 509\n",
      " 510 511 512 513 514 515 516 517 519 521 523 526 527 529 535 536 537 538\n",
      " 539 541 542 543 546 547 548 549 552 553 555 558 559 563 565 566 570 572\n",
      " 575 579 580 581 582 583 586 589 591 592 593 595 597 602 604 606 608 611\n",
      " 613 614 617 622 623 625 628 629 630 632 633 634 636 641 645 647 648 649\n",
      " 652 653 656 657 658 659 660 664 665 667 671 672 674 679 680 681 684 688\n",
      " 689 690 692 693 696 698 699 700 702 704 705 706 708 710 711 713 714 715\n",
      " 716 719 723 724 727 730 733 735 736 737 740 743 745 746 747 749 750 752\n",
      " 753 754 755 762 763 765 766 769 770 775 777 779 780 782]\n",
      "[558 468 356 360 442 674 228  66 275 489 241 509 457  70 137 625 658 406\n",
      " 566 503  89   1 478 699 510 466 723 184 291 229 301 504 296 470 502 332\n",
      " 765 507 219 708  34 755 281 197 367 119 665 431 277  36 546 565 746 245\n",
      " 702 298 592 226 217 553 194 754 657 513 338 490  88 270 688 770 641 258\n",
      " 251 547 730  57 492  38 737  53  26 652 572 766 443 328 597 218 167 232\n",
      " 749 176 740  49 447 244 589  97 364 295 521 537 782 188 476 542 535 306\n",
      "  28 253 575 118 180  19 132 659 397 150 107  64 750 617 165 401 196 704\n",
      " 308 580 613 555 700 762 497 189 141 151 692 602 317 375 777 127 451 263\n",
      "  77 355 636 515 710 320 399 237 129 124 162 736 559 222 207   9 390 591\n",
      " 705 645 693 414  56 143 664 696   0 426 161 429 454  42 484 463 363 775\n",
      " 361 724 418 392 779  12 745 384 163 158 346   5   8 348 714 313 340  47\n",
      " 519 527 780 681  75 389 690 249  65 570 293 213 257 671 456 106 208 769\n",
      " 326 136  63 611 752 743 552  67 174 435 190 410 747 186 100  79 698  74\n",
      " 582 378 733 138 230 209 653 345 471 315 458 464 448 706 593 305 727 649\n",
      " 377 501 142 523 381 104 516  37  90 289 512 667 154 101 269  76 583  83\n",
      " 391 648 235  21  22 614 272 318 233 684 462 623 354 284 679 179  55 689\n",
      " 262 549 581 352 294 183  92 548 144 606 608 586   3  33 103 347 420 713\n",
      " 334 199  29 526 514 412 274  81  50 579 494 387 153 753 128 517  86 394\n",
      " 719 139 225 632 630 511 223 339 634  94 268 763  96 114 496 541 660 133\n",
      " 633 604 539 273 486 629 563 455 201 461 215 715 271 656 647 735 121 437\n",
      " 316 424 628 146 336 538 238   6 529 400 204 672 595 680 212 407 495 711\n",
      " 482 716 543 285 498  14 191 321 536 276 164 388 286 622]\n",
      "rows to prune in layer 3 : 225\n",
      "[  1   2   5   6   8  11  12  17  18  21  22  24  25  28  29  30  31  33\n",
      "  36  41  42  44  46  50  51  52  54  55  60  62  63  65  67  68  70  73\n",
      "  74  75  76  77  78  79  80  81  82  83  84  86  87  88  90  91  92  94\n",
      "  96  98 101 103 104 105 110 112 114 115 116 119 120 121 123 124 125 129\n",
      " 133 134 135 136 137 140 142 144 147 148 157 160 161 166 167 168 173 175\n",
      " 176 177 181 185 187 188 189 191 193 195 196 199 200 204 207 208 209 212\n",
      " 213 214 215 216 217 219 221 224 227 231 233 237 240 244 246 248 250 251\n",
      " 253 259 260 261 264 265 267 268 271 272 273 277 279 280 281 286 287 288\n",
      " 290 292 293 294 297 299]\n",
      "[199  44 297 161  65  52 273  24 125 299  54 160 287  92 195  17  67  31\n",
      "  78  81 167 251 268 133 173  28  88 119  50  91 224 101 267 233   6 168\n",
      " 215 293 204 288  42 260   2 213  73 121  79 253 124 134 114  70  46 185\n",
      " 105 110  18 264 196  80  75  90  77 227 214 261 176 120 294 136  22 216\n",
      " 271 277  21 250 259  12 189 112 135 279  82  41  76 237 286 244 193 209\n",
      "  11 144 103 292  84 217  94  25  62  86  33 142 290  74   8 116 175 137\n",
      " 177 221  60 123 148  30  51 187  83 115  87 191  36 281 104 272  63 240\n",
      " 212 219 166  96   1 208 140  98 248  29 200 181  68 147 188 246  55 265\n",
      " 207 129 157 280 231   5]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 2  3  6  9 11 14 15 16 19 20 24 25 26 31 32 34 36 39 40 41 42 43 44 47\n",
      " 49 51 52 55 56 57 58 60 61 65 66 67 68 70 71 73 75 78 79 82 83 85 88 91\n",
      " 95 99]\n",
      "[71 82 47 49 65 58 15  3 19  2 31 26 75 42 43 32 83 24 51 57 14 85 39 56\n",
      " 20 55 78 60  6 52 91 36 41 61 95 11 66 44 99 16 68 73 67 79 70 34 40  9\n",
      " 88 25]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5330 - accuracy: 0.9463 - val_loss: 1.5100 - val_accuracy: 0.9583\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5002 - accuracy: 0.9679 - val_loss: 1.5020 - val_accuracy: 0.9639\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4932 - accuracy: 0.9735 - val_loss: 1.5002 - val_accuracy: 0.9648\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4888 - accuracy: 0.9765 - val_loss: 1.4989 - val_accuracy: 0.9653\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4859 - accuracy: 0.9791 - val_loss: 1.4945 - val_accuracy: 0.9695\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9812 - val_loss: 1.4942 - val_accuracy: 0.9691\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4817 - accuracy: 0.9826 - val_loss: 1.4937 - val_accuracy: 0.9694\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4802 - accuracy: 0.9836 - val_loss: 1.4933 - val_accuracy: 0.9698\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4788 - accuracy: 0.9849 - val_loss: 1.4930 - val_accuracy: 0.9703\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4779 - accuracy: 0.9857 - val_loss: 1.4940 - val_accuracy: 0.9685\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4763 - accuracy: 0.9870 - val_loss: 1.4951 - val_accuracy: 0.9678\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4761 - accuracy: 0.9870 - val_loss: 1.4906 - val_accuracy: 0.9714\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4750 - accuracy: 0.9880 - val_loss: 1.4917 - val_accuracy: 0.9700\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4748 - accuracy: 0.9878 - val_loss: 1.4911 - val_accuracy: 0.9709\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4741 - accuracy: 0.9887 - val_loss: 1.4913 - val_accuracy: 0.9711\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:13, 123.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 1\n",
      "rows to prune in layer 0 : 686\n",
      "[  3   6  14  21  22  29  33  37  47  50  55  63  65  67  74  75  76  79\n",
      "  81  83  86  90  92  94  96 100 101 103 104 106 114 121 128 133 136 138\n",
      " 139 142 144 146 153 154 164 174 179 183 186 190 191 199 201 204 208 209\n",
      " 212 213 215 223 225 230 233 235 238 249 257 262 268 269 271 272 273 274\n",
      " 276 284 285 286 289 293 294 305 315 316 318 321 326 334 336 339 340 345\n",
      " 347 352 354 377 378 381 387 388 389 391 394 400 407 410 412 420 424 435\n",
      " 437 448 455 456 458 461 462 464 471 482 486 494 495 496 498 501 511 512\n",
      " 514 516 517 519 523 526 527 529 536 538 539 541 543 548 549 552 563 570\n",
      " 579 581 582 583 586 593 595 604 606 608 611 614 622 623 628 629 630 632\n",
      " 633 634 647 648 649 653 656 660 667 671 672 679 680 681 684 689 690 698\n",
      " 706 711 713 715 716 719 727 733 735 743 747 752 753 763 769 780]\n",
      "[420 549 212  96 407 204 412 272 213 743 164 139 183 563 378 747 100 494\n",
      " 389 653 539  76 614 632 517 235  74 274 623   3 269 667 191 595  50 199\n",
      " 271 326 201 628 186 660 293 656 294 106 579 514 763 611  94 138 716 735\n",
      "  21 527 394 464 633  90 753 352 541 387 486 144 548 262 780 496 174 606\n",
      " 391 622 582 410 114 345 512 316 305 752 179 769 698 273 238 249 458  47\n",
      " 276 336 347 536 543 501 286 354 284  75 209 634  81 142 552 498  22 461\n",
      " 339  33  67   6 146 671 377 629  83 121 388 334 340 315  86 672 190  55\n",
      " 516 713 529 538  37 462 104 681 511 289 103 689 711 101  29 715 733 679\n",
      " 400 706  92 223 225 208 437 154 581 684 647 630 318 424 690 257 608 233\n",
      "  14 268 456 526 455 648 230 285 321 593 448  63 680 482 471 649  79 136\n",
      " 128 586 435 153 495 719 133 570 215 523  65 519 583 604 727 381]\n",
      "rows to prune in layer 3 : 262\n",
      "[  1   5   8  11  12  25  29  30  33  36  41  51  55  60  62  63  68  74\n",
      "  76  82  83  84  86  87  94  96  98 103 104 112 115 116 123 129 135 137\n",
      " 140 142 144 147 148 157 166 175 177 181 187 188 189 191 193 200 207 208\n",
      " 209 212 217 219 221 231 237 240 244 246 248 250 259 265 272 279 280 281\n",
      " 286 290 292]\n",
      "[175 209   5 188 250 148 187 292   1 248  96 177 193 144 208 112 103 142\n",
      "  36  83  62 104  51 272  29 290 221  12 129 237  41 181 281 189 217  33\n",
      " 157  76  84 200 166 246  63 286 116 244 265  55 231 115  74 191  86 240\n",
      " 123  25  68 279 135 212 280   8 219  98  30  94  11  60 140 137 147 259\n",
      "  87 207  82]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 6  9 11 16 25 34 36 40 41 44 52 55 60 61 66 67 68 70 73 78 79 88 91 95\n",
      " 99]\n",
      "[25 88 41 99 66 36 78 34  9 52 95 11 70 67 44  6 55 61 79 40 68 91 60 73\n",
      " 16]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7034 - accuracy: 0.7854 - val_loss: 1.6097 - val_accuracy: 0.8687\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6049 - accuracy: 0.8699 - val_loss: 1.5847 - val_accuracy: 0.8882\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5867 - accuracy: 0.8855 - val_loss: 1.5756 - val_accuracy: 0.8948\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5769 - accuracy: 0.8927 - val_loss: 1.5695 - val_accuracy: 0.8982\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5705 - accuracy: 0.8979 - val_loss: 1.5649 - val_accuracy: 0.9021\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5657 - accuracy: 0.9017 - val_loss: 1.5625 - val_accuracy: 0.9045\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5614 - accuracy: 0.9062 - val_loss: 1.5618 - val_accuracy: 0.9049\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5585 - accuracy: 0.9081 - val_loss: 1.5587 - val_accuracy: 0.9066\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5560 - accuracy: 0.9107 - val_loss: 1.5567 - val_accuracy: 0.9092\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5534 - accuracy: 0.9126 - val_loss: 1.5563 - val_accuracy: 0.9098\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5515 - accuracy: 0.9148 - val_loss: 1.5550 - val_accuracy: 0.9091\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5498 - accuracy: 0.9161 - val_loss: 1.5551 - val_accuracy: 0.9087\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5484 - accuracy: 0.9173 - val_loss: 1.5547 - val_accuracy: 0.9100\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5467 - accuracy: 0.9193 - val_loss: 1.5526 - val_accuracy: 0.9129\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5452 - accuracy: 0.9205 - val_loss: 1.5523 - val_accuracy: 0.9111\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5442 - accuracy: 0.9209 - val_loss: 1.5503 - val_accuracy: 0.9144\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5430 - accuracy: 0.9222 - val_loss: 1.5526 - val_accuracy: 0.9106\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5419 - accuracy: 0.9232 - val_loss: 1.5551 - val_accuracy: 0.9084\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5411 - accuracy: 0.9239 - val_loss: 1.5509 - val_accuracy: 0.9124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [08:34, 128.82s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 1\n",
      "rows to prune in layer 0 : 735\n",
      "[  6  14  22  29  33  37  55  63  65  67  75  79  81  83  86  92 101 103\n",
      " 104 121 128 133 136 142 146 153 154 190 208 209 215 223 225 230 233 257\n",
      " 268 284 285 289 315 318 321 334 339 340 377 381 388 400 424 435 437 448\n",
      " 455 456 461 462 471 482 495 498 511 516 519 523 526 529 538 552 570 581\n",
      " 583 586 593 604 608 629 630 634 647 648 649 671 672 679 680 681 684 689\n",
      " 690 706 711 713 715 719 727 733]\n",
      "[ 92 334 529 680 462 208  63 340 681  81 233 400 516 339 526  83 230 435\n",
      " 593 268 388 103  37 133 672  29 727 583 647 190 121  67 381 128 225 604\n",
      " 104  33 223 257 448 455 706 456 689 538 377 154 719  22 519 634 679 713\n",
      "  14 733 495 649 153 284 289 424 690 608 471 511 437 629 321 715  65 630\n",
      " 146 209 461 318 570 215 552  75 285  79 586 142 581   6 101 136  86 648\n",
      " 711 684 482 671  55 315 523 498]\n",
      "rows to prune in layer 3 : 281\n",
      "[  8  11  25  30  55  60  63  68  74  76  82  84  86  87  94  98 115 116\n",
      " 123 135 137 140 147 166 191 200 207 212 219 231 240 244 246 259 265 279\n",
      " 280 286]\n",
      "[  8 166 191 280  68 147 286  84 244 140  60 135  25 259 212 240  82 279\n",
      "  11 137 115 231  94  55 123  76 265  74  87 207 200  30  63 116 219  98\n",
      "  86 246]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 6 16 40 44 55 60 61 67 68 70 73 79 91]\n",
      "[61 60 79 91 44 68 67 16  6 70 73 55 40]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8836 - accuracy: 0.6010 - val_loss: 1.7737 - val_accuracy: 0.7072\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7666 - accuracy: 0.7079 - val_loss: 1.7425 - val_accuracy: 0.7308\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7456 - accuracy: 0.7251 - val_loss: 1.7280 - val_accuracy: 0.7400\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7343 - accuracy: 0.7341 - val_loss: 1.7190 - val_accuracy: 0.7497\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7264 - accuracy: 0.7409 - val_loss: 1.7136 - val_accuracy: 0.7548\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7188 - accuracy: 0.7485 - val_loss: 1.7072 - val_accuracy: 0.7595\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7121 - accuracy: 0.7552 - val_loss: 1.7022 - val_accuracy: 0.7640\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7048 - accuracy: 0.7630 - val_loss: 1.6945 - val_accuracy: 0.7712\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6974 - accuracy: 0.7698 - val_loss: 1.6901 - val_accuracy: 0.7766\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6925 - accuracy: 0.7740 - val_loss: 1.6856 - val_accuracy: 0.7815\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6887 - accuracy: 0.7769 - val_loss: 1.6861 - val_accuracy: 0.7790\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6861 - accuracy: 0.7796 - val_loss: 1.6809 - val_accuracy: 0.7827\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6841 - accuracy: 0.7809 - val_loss: 1.6811 - val_accuracy: 0.7844\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6821 - accuracy: 0.7831 - val_loss: 1.6788 - val_accuracy: 0.7856\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6804 - accuracy: 0.7848 - val_loss: 1.6777 - val_accuracy: 0.7865\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6788 - accuracy: 0.7865 - val_loss: 1.6753 - val_accuracy: 0.7885\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6778 - accuracy: 0.7873 - val_loss: 1.6751 - val_accuracy: 0.7890\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6764 - accuracy: 0.7887 - val_loss: 1.6741 - val_accuracy: 0.7892\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6751 - accuracy: 0.7897 - val_loss: 1.6729 - val_accuracy: 0.7890\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6740 - accuracy: 0.7905 - val_loss: 1.6744 - val_accuracy: 0.7876\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6734 - accuracy: 0.7916 - val_loss: 1.6705 - val_accuracy: 0.7927\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6725 - accuracy: 0.7921 - val_loss: 1.6713 - val_accuracy: 0.7904\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6717 - accuracy: 0.7929 - val_loss: 1.6701 - val_accuracy: 0.7927\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6708 - accuracy: 0.7935 - val_loss: 1.6695 - val_accuracy: 0.7926\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6700 - accuracy: 0.7950 - val_loss: 1.6688 - val_accuracy: 0.7940\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6691 - accuracy: 0.7957 - val_loss: 1.6679 - val_accuracy: 0.7942\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6691 - accuracy: 0.7954 - val_loss: 1.6674 - val_accuracy: 0.7948\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6677 - accuracy: 0.7973 - val_loss: 1.6666 - val_accuracy: 0.7964\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6673 - accuracy: 0.7966 - val_loss: 1.6692 - val_accuracy: 0.7920\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6671 - accuracy: 0.7968 - val_loss: 1.6685 - val_accuracy: 0.7934\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6662 - accuracy: 0.7982 - val_loss: 1.6685 - val_accuracy: 0.7930\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [11:31, 143.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 1\n",
      "rows to prune in layer 0 : 759\n",
      "[  6  14  22  55  65  75  79  86 101 136 142 146 153 209 215 284 285 289\n",
      " 315 318 321 424 437 461 471 482 495 498 511 519 523 552 570 581 586 608\n",
      " 629 630 634 648 649 671 679 684 690 711 713 715 733]\n",
      "[284  14 684 629   6 461 679 630  65 715 136 649  86 713 495 142 437 570\n",
      " 209 608 482 586  55 733  75  79 671 581 552 648 146 153 498  22 321 285\n",
      " 523 318 215 289 711 315 471 511 101 424 634 519 690]\n",
      "rows to prune in layer 3 : 290\n",
      "[ 30  55  63  74  76  86  87  94  98 115 116 123 137 200 207 219 231 246\n",
      " 265]\n",
      "[123 116  87 207 115 231 200  94  76  63  98  86 219  55 265 137  30 246\n",
      "  74]\n",
      "rows to prune in layer 6 : 96\n",
      "[ 6 16 40 55 67 70 73]\n",
      "[40  6 16 70 73 55 67]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1009 - accuracy: 0.3667 - val_loss: 2.0122 - val_accuracy: 0.4514\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9944 - accuracy: 0.4748 - val_loss: 1.9621 - val_accuracy: 0.5093\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9530 - accuracy: 0.5162 - val_loss: 1.9268 - val_accuracy: 0.5437\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9263 - accuracy: 0.5411 - val_loss: 1.9091 - val_accuracy: 0.5603\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9049 - accuracy: 0.5620 - val_loss: 1.8892 - val_accuracy: 0.5795\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8923 - accuracy: 0.5734 - val_loss: 1.8815 - val_accuracy: 0.5840\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8853 - accuracy: 0.5798 - val_loss: 1.8747 - val_accuracy: 0.5913\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8803 - accuracy: 0.5853 - val_loss: 1.8704 - val_accuracy: 0.5953\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8760 - accuracy: 0.5894 - val_loss: 1.8670 - val_accuracy: 0.5996\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8728 - accuracy: 0.5925 - val_loss: 1.8646 - val_accuracy: 0.5985\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8704 - accuracy: 0.5942 - val_loss: 1.8621 - val_accuracy: 0.6001\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8682 - accuracy: 0.5957 - val_loss: 1.8607 - val_accuracy: 0.6012\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8663 - accuracy: 0.5980 - val_loss: 1.8590 - val_accuracy: 0.6024\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8647 - accuracy: 0.5994 - val_loss: 1.8577 - val_accuracy: 0.6056\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8632 - accuracy: 0.6008 - val_loss: 1.8560 - val_accuracy: 0.6059\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8621 - accuracy: 0.6019 - val_loss: 1.8559 - val_accuracy: 0.6052\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8607 - accuracy: 0.6030 - val_loss: 1.8534 - val_accuracy: 0.6089\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8596 - accuracy: 0.6043 - val_loss: 1.8536 - val_accuracy: 0.6090\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8584 - accuracy: 0.6051 - val_loss: 1.8519 - val_accuracy: 0.6115\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8574 - accuracy: 0.6053 - val_loss: 1.8515 - val_accuracy: 0.6119\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8565 - accuracy: 0.6064 - val_loss: 1.8507 - val_accuracy: 0.6109\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8555 - accuracy: 0.6071 - val_loss: 1.8486 - val_accuracy: 0.6145\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8546 - accuracy: 0.6082 - val_loss: 1.8491 - val_accuracy: 0.6131\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8538 - accuracy: 0.6092 - val_loss: 1.8477 - val_accuracy: 0.6145\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8530 - accuracy: 0.6095 - val_loss: 1.8460 - val_accuracy: 0.6163\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8523 - accuracy: 0.6101 - val_loss: 1.8466 - val_accuracy: 0.6159\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8515 - accuracy: 0.6113 - val_loss: 1.8456 - val_accuracy: 0.6189\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8509 - accuracy: 0.6119 - val_loss: 1.8454 - val_accuracy: 0.6173\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8502 - accuracy: 0.6129 - val_loss: 1.8449 - val_accuracy: 0.6177\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8497 - accuracy: 0.6127 - val_loss: 1.8441 - val_accuracy: 0.6184\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8492 - accuracy: 0.6138 - val_loss: 1.8434 - val_accuracy: 0.6184\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8484 - accuracy: 0.6134 - val_loss: 1.8426 - val_accuracy: 0.6177\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8481 - accuracy: 0.6148 - val_loss: 1.8432 - val_accuracy: 0.6188\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8476 - accuracy: 0.6152 - val_loss: 1.8431 - val_accuracy: 0.6190\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8470 - accuracy: 0.6157 - val_loss: 1.8424 - val_accuracy: 0.6180\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8465 - accuracy: 0.6159 - val_loss: 1.8418 - val_accuracy: 0.6204\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8462 - accuracy: 0.6167 - val_loss: 1.8414 - val_accuracy: 0.6224\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8457 - accuracy: 0.6173 - val_loss: 1.8411 - val_accuracy: 0.6212\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8453 - accuracy: 0.6176 - val_loss: 1.8412 - val_accuracy: 0.6213\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8447 - accuracy: 0.6179 - val_loss: 1.8412 - val_accuracy: 0.6199\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8445 - accuracy: 0.6182 - val_loss: 1.8406 - val_accuracy: 0.6201\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8441 - accuracy: 0.6190 - val_loss: 1.8402 - val_accuracy: 0.6219\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8438 - accuracy: 0.6194 - val_loss: 1.8400 - val_accuracy: 0.6219\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8436 - accuracy: 0.6193 - val_loss: 1.8402 - val_accuracy: 0.6212\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8432 - accuracy: 0.6192 - val_loss: 1.8387 - val_accuracy: 0.6223\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8428 - accuracy: 0.6200 - val_loss: 1.8393 - val_accuracy: 0.6231\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8428 - accuracy: 0.6199 - val_loss: 1.8386 - val_accuracy: 0.6235\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8425 - accuracy: 0.6197 - val_loss: 1.8392 - val_accuracy: 0.6218\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8423 - accuracy: 0.6202 - val_loss: 1.8389 - val_accuracy: 0.6232\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8420 - accuracy: 0.6200 - val_loss: 1.8376 - val_accuracy: 0.6231\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8419 - accuracy: 0.6203 - val_loss: 1.8378 - val_accuracy: 0.6246\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8416 - accuracy: 0.6204 - val_loss: 1.8389 - val_accuracy: 0.6237\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8415 - accuracy: 0.6211 - val_loss: 1.8375 - val_accuracy: 0.6244\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8413 - accuracy: 0.6206 - val_loss: 1.8377 - val_accuracy: 0.6227\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8411 - accuracy: 0.6213 - val_loss: 1.8373 - val_accuracy: 0.6247\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8408 - accuracy: 0.6217 - val_loss: 1.8372 - val_accuracy: 0.6245\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8408 - accuracy: 0.6213 - val_loss: 1.8370 - val_accuracy: 0.6244\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8407 - accuracy: 0.6218 - val_loss: 1.8368 - val_accuracy: 0.6230\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8403 - accuracy: 0.6219 - val_loss: 1.8377 - val_accuracy: 0.6237\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8402 - accuracy: 0.6218 - val_loss: 1.8376 - val_accuracy: 0.6232\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8400 - accuracy: 0.6219 - val_loss: 1.8369 - val_accuracy: 0.6254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [15:00, 162.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 1\n",
      "rows to prune in layer 0 : 771\n",
      "[ 22  75  79 101 146 153 215 285 289 315 318 321 424 471 498 511 519 523\n",
      " 552 581 634 648 671 690 711]\n",
      "[581 634 671  79 552 318 321  22 289 711 285 215 511 315 146 498 690 471\n",
      " 424 519 101 153 648  75 523]\n",
      "rows to prune in layer 3 : 295\n",
      "[ 30  55  63  74  86  98 137 219 246 265]\n",
      "[219  98 137  63 246 265  74  86  30  55]\n",
      "rows to prune in layer 6 : 98\n",
      "[55 67 70 73]\n",
      "[70 73 55 67]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2761 - accuracy: 0.1789 - val_loss: 2.2571 - val_accuracy: 0.1971\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2394 - accuracy: 0.2174 - val_loss: 2.2262 - val_accuracy: 0.2290\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2020 - accuracy: 0.2576 - val_loss: 2.1796 - val_accuracy: 0.2837\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1775 - accuracy: 0.2834 - val_loss: 2.1735 - val_accuracy: 0.2863\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1731 - accuracy: 0.2855 - val_loss: 2.1710 - val_accuracy: 0.2874\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1704 - accuracy: 0.2881 - val_loss: 2.1685 - val_accuracy: 0.2888\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1684 - accuracy: 0.2898 - val_loss: 2.1667 - val_accuracy: 0.2900\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1655 - accuracy: 0.2909 - val_loss: 2.1629 - val_accuracy: 0.2915\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1630 - accuracy: 0.2930 - val_loss: 2.1607 - val_accuracy: 0.2952\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1613 - accuracy: 0.2960 - val_loss: 2.1596 - val_accuracy: 0.2962\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1600 - accuracy: 0.2961 - val_loss: 2.1586 - val_accuracy: 0.2965\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1590 - accuracy: 0.2973 - val_loss: 2.1572 - val_accuracy: 0.2978\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1583 - accuracy: 0.2974 - val_loss: 2.1569 - val_accuracy: 0.2995\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1576 - accuracy: 0.2979 - val_loss: 2.1559 - val_accuracy: 0.3000\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1569 - accuracy: 0.2984 - val_loss: 2.1552 - val_accuracy: 0.3000\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1565 - accuracy: 0.2983 - val_loss: 2.1549 - val_accuracy: 0.3011\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1558 - accuracy: 0.2982 - val_loss: 2.1543 - val_accuracy: 0.3014\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1553 - accuracy: 0.2988 - val_loss: 2.1537 - val_accuracy: 0.2994\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1544 - accuracy: 0.2992 - val_loss: 2.1532 - val_accuracy: 0.3014\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1535 - accuracy: 0.2991 - val_loss: 2.1522 - val_accuracy: 0.3003\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1524 - accuracy: 0.3007 - val_loss: 2.1506 - val_accuracy: 0.3017\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1510 - accuracy: 0.3020 - val_loss: 2.1491 - val_accuracy: 0.3036\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1490 - accuracy: 0.3047 - val_loss: 2.1484 - val_accuracy: 0.3066\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1476 - accuracy: 0.3075 - val_loss: 2.1469 - val_accuracy: 0.3102\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1466 - accuracy: 0.3081 - val_loss: 2.1459 - val_accuracy: 0.3090\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1457 - accuracy: 0.3088 - val_loss: 2.1453 - val_accuracy: 0.3120\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1449 - accuracy: 0.3098 - val_loss: 2.1448 - val_accuracy: 0.3085\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1441 - accuracy: 0.3110 - val_loss: 2.1437 - val_accuracy: 0.3118\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1435 - accuracy: 0.3113 - val_loss: 2.1432 - val_accuracy: 0.3122\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1429 - accuracy: 0.3120 - val_loss: 2.1427 - val_accuracy: 0.3108\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1424 - accuracy: 0.3129 - val_loss: 2.1422 - val_accuracy: 0.3126\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1419 - accuracy: 0.3133 - val_loss: 2.1415 - val_accuracy: 0.3150\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1414 - accuracy: 0.3133 - val_loss: 2.1408 - val_accuracy: 0.3152\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1408 - accuracy: 0.3150 - val_loss: 2.1405 - val_accuracy: 0.3164\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1404 - accuracy: 0.3149 - val_loss: 2.1395 - val_accuracy: 0.3175\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1399 - accuracy: 0.3152 - val_loss: 2.1389 - val_accuracy: 0.3165\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1394 - accuracy: 0.3162 - val_loss: 2.1388 - val_accuracy: 0.3182\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1390 - accuracy: 0.3166 - val_loss: 2.1380 - val_accuracy: 0.3192\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1385 - accuracy: 0.3171 - val_loss: 2.1379 - val_accuracy: 0.3186\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1380 - accuracy: 0.3177 - val_loss: 2.1371 - val_accuracy: 0.3190\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1375 - accuracy: 0.3179 - val_loss: 2.1368 - val_accuracy: 0.3184\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1370 - accuracy: 0.3180 - val_loss: 2.1361 - val_accuracy: 0.3199\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1366 - accuracy: 0.3185 - val_loss: 2.1357 - val_accuracy: 0.3209\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1361 - accuracy: 0.3196 - val_loss: 2.1354 - val_accuracy: 0.3204\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1356 - accuracy: 0.3202 - val_loss: 2.1351 - val_accuracy: 0.3196\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1353 - accuracy: 0.3206 - val_loss: 2.1342 - val_accuracy: 0.3216\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1349 - accuracy: 0.3217 - val_loss: 2.1340 - val_accuracy: 0.3214\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1346 - accuracy: 0.3216 - val_loss: 2.1341 - val_accuracy: 0.3212\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1343 - accuracy: 0.3223 - val_loss: 2.1332 - val_accuracy: 0.3191\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1340 - accuracy: 0.3223 - val_loss: 2.1330 - val_accuracy: 0.3233\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1336 - accuracy: 0.3224 - val_loss: 2.1325 - val_accuracy: 0.3227\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1334 - accuracy: 0.3231 - val_loss: 2.1320 - val_accuracy: 0.3241\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1331 - accuracy: 0.3230 - val_loss: 2.1316 - val_accuracy: 0.3248\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1328 - accuracy: 0.3234 - val_loss: 2.1318 - val_accuracy: 0.3247\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1326 - accuracy: 0.3235 - val_loss: 2.1316 - val_accuracy: 0.3247\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1322 - accuracy: 0.3243 - val_loss: 2.1314 - val_accuracy: 0.3252\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1320 - accuracy: 0.3243 - val_loss: 2.1309 - val_accuracy: 0.3245\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1317 - accuracy: 0.3251 - val_loss: 2.1307 - val_accuracy: 0.3255\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1315 - accuracy: 0.3246 - val_loss: 2.1304 - val_accuracy: 0.3261\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1313 - accuracy: 0.3250 - val_loss: 2.1304 - val_accuracy: 0.3266\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1312 - accuracy: 0.3252 - val_loss: 2.1302 - val_accuracy: 0.3257\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1310 - accuracy: 0.3252 - val_loss: 2.1298 - val_accuracy: 0.3269\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1307 - accuracy: 0.3252 - val_loss: 2.1294 - val_accuracy: 0.3262\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1307 - accuracy: 0.3257 - val_loss: 2.1295 - val_accuracy: 0.3264\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1303 - accuracy: 0.3264 - val_loss: 2.1298 - val_accuracy: 0.3251\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1303 - accuracy: 0.3258 - val_loss: 2.1291 - val_accuracy: 0.3268\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1302 - accuracy: 0.3258 - val_loss: 2.1290 - val_accuracy: 0.3268\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1299 - accuracy: 0.3259 - val_loss: 2.1286 - val_accuracy: 0.3267\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1298 - accuracy: 0.3260 - val_loss: 2.1283 - val_accuracy: 0.3292\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1296 - accuracy: 0.3265 - val_loss: 2.1282 - val_accuracy: 0.3292\n",
      "Epoch 71/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1295 - accuracy: 0.3269 - val_loss: 2.1280 - val_accuracy: 0.3291\n",
      "Epoch 72/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1293 - accuracy: 0.3269 - val_loss: 2.1278 - val_accuracy: 0.3287\n",
      "Epoch 73/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1292 - accuracy: 0.3272 - val_loss: 2.1284 - val_accuracy: 0.3276\n",
      "Epoch 74/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1291 - accuracy: 0.3269 - val_loss: 2.1276 - val_accuracy: 0.3293\n",
      "Epoch 75/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1289 - accuracy: 0.3271 - val_loss: 2.1277 - val_accuracy: 0.3282\n",
      "Epoch 76/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1289 - accuracy: 0.3267 - val_loss: 2.1275 - val_accuracy: 0.3289\n",
      "Epoch 77/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1288 - accuracy: 0.3273 - val_loss: 2.1280 - val_accuracy: 0.3287\n",
      "Epoch 78/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1286 - accuracy: 0.3282 - val_loss: 2.1275 - val_accuracy: 0.3299\n",
      "Epoch 79/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1286 - accuracy: 0.3271 - val_loss: 2.1273 - val_accuracy: 0.3290\n",
      "Epoch 80/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1285 - accuracy: 0.3274 - val_loss: 2.1270 - val_accuracy: 0.3303\n",
      "Epoch 81/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1284 - accuracy: 0.3274 - val_loss: 2.1274 - val_accuracy: 0.3291\n",
      "Epoch 82/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1285 - accuracy: 0.3279 - val_loss: 2.1274 - val_accuracy: 0.3307\n",
      "Epoch 83/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1283 - accuracy: 0.3279 - val_loss: 2.1270 - val_accuracy: 0.3296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [18:22, 157.51s/it]\u001b[A\n",
      " 20%|██        | 2/10 [36:34<2:25:51, 1093.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5747 - accuracy: 0.9021 - val_loss: 1.5208 - val_accuracy: 0.9441\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 2\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[648 394 210 491 131 736 110 140 528 766 588 546 212 262 457  14 425 653\n",
      " 742 318 172 504 433 459 372 298 466 388 299 598 290 129 343  39 263 485\n",
      " 540 124 710 422 627 557 260 313 519 486 407 338 686  69 714 569 395 584\n",
      " 753  55 301 417 512 223 597 671 662 492 461 185 410  86 123 780 643 149\n",
      " 182 143  15 678   4 344 216  41  83  33 495 369 767 652 595 165 607 440\n",
      " 587 294 488 514 188  29 367 375 170 778 409 590  27 219 286 226  19 190\n",
      " 310 672 198 640 156 688 426 384 232 447 469 206 740  76 748  74 259 729\n",
      " 331 168 728  49 698 387 589 472 174 137 288  35 434 419 169 644 619   0\n",
      " 661 189 108 243 378 705  89 676 612 272 629 708 470 366 709 700 317 591\n",
      " 471  21 628 777 386 752 261 657  30 759 635 370 413 178 323 119 113 749\n",
      " 443 520 639 412 360 199 102 568 599 145 503 167 631 733 687 125 604 750\n",
      " 551 650 414 768 758 158 302 429 453 549  97 578 775 602 275 345 146 515\n",
      " 271 363 561  78 477 630 567 637 669 217  88 327 558 476   9 151 684 325\n",
      "  90 621 531 106 355 103 647 699  44 139 500 474 274 478  53  96 611 781\n",
      " 405 192 573  45 757 754 726 201 420 613 116 283 741 229 346 701 496  54\n",
      " 670  79 685 315  26 269 581  18 508 706 616 203 721  94  23 473 376 543\n",
      " 195 295 138 136 692 374 329 424 555 720 171 105 371  48 454 463 724 293\n",
      " 763 580 235  59 175 392  98 244 267 623  65 114 177  38 617 717  31 677\n",
      " 493 117 205 626 159 227 202 380 489 737 620 783 695 307 389 306 751 257\n",
      "   2 445 782 556 133 559 161  87 682 730 396 134  99 779 253  67 208 393\n",
      " 130 416 554 553 522  91 761 462 399 160 321 456 200 632 680 237 211 756\n",
      " 633 449 373 214 585 572 521 265  34 247 281 446 236 679 636 314 403 383\n",
      "  51 490  81 126 181 582  71 266 258  66 305 402 128 423 645 273 246 144\n",
      " 498 674 239  32 529 377 220 444 127 743 760 666 711 509  80 451 221 285\n",
      "  24 406 436 319 439 163  62 475 297 467 563 121 332 155 284 368 173 712\n",
      " 511  11 187 618 204 450 347  56 350 349 218 435 609 606 379 252 530 545\n",
      " 111 240 579 222 245 316 320 215 776  82 624 575 193 122 404 255 697 622\n",
      " 311  36 586 312 480  40   6  43 101 400 464  84 184 642 458 411 342 401\n",
      " 231 415 649 352 251  93 694 270 610  68 771  47 497 341 455 722  57 517\n",
      " 773  25  63 525 398 727 534 365 764 250 135  16 334 196 651 583 592 358\n",
      " 162 438 418 109 362 224  52 704 287 322 254 382 209 431 279  28 361 675\n",
      " 715 432 655 186 180 608 230 739 506 769 176 234  20 356  58 638 707 385\n",
      " 336 603 665 747 484  77 179 552 734 228 570 142 481  64 277 770 289 354\n",
      " 154 625 566 541 746 437 523 482 157  12 308 667 542 527 276 328 148 765\n",
      " 762 191 249 574 719 538  10 112 505 353 646 183 357 693 673  92 738 526\n",
      " 337  95   1 324 696 664  46   8 118 658 716 600 282 564 104  17 516 601\n",
      " 659 513 427 725  22 641 452 309  61 533 654 391 660 494 264 421 550 364\n",
      " 577 448 501 359 735 483 147 499 565 560 605 668 507 571 536 241 703 213\n",
      " 544 225 441 381 524  60 150 539 442   5 278 339 300 615  50 340 333  42\n",
      " 296 115 397 755 732 593   3 479 537 663   7 166 691 744 518 100 326 238\n",
      " 718 772 562 330 594  37 207 120  13 689 430 465 153 303 510 335 532 248\n",
      " 683 291 390 702 141 547  73 256 197 723  70 132 487 242 656 713 460 681\n",
      " 348 152 576 164 428 731 194 690  85  75  72 634 535 292 408 304 268 548\n",
      " 774 351 233 502 614 280 596 745 107 468]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[230 272  81  28  95  69  71 126  77 258 248 289  96 244 163 143  32 146\n",
      " 119 202 180 255 226  99 235 103 104 124 192 297 161  30 152 284 211 142\n",
      " 150 141 286 200 148 201 293 239  97 256 273 129 224 172  56 167 106 125\n",
      "   9   5  23 107 213 222 112 140 116 209 231 212 249 205  57 253 109 271\n",
      "  55 290 162  34 185 215 221 122 156 110 275 228 257  70 100 182 195  33\n",
      " 139 236 145 261  52 177 260   2  78 216  22 164 292 196 242 184 217 210\n",
      "  85 247  51  50 240 144 233  75  21  42 108 267 191 206 291  40  18 282\n",
      "  80  92 135  29  76 268 165 159 197 254  62 160 274  54 136 287 194 214\n",
      " 269  41 295 137  89  91  16  87 113  90 283 178  98  68 115 280 121 154\n",
      "  93  61 265 130   3 232 220 120   8  44  12 181  11  65 179  25 102 168\n",
      " 149 296 114 176 263  48  66 246 153 117  83 241  37 251 294 187 158 199\n",
      " 270 111 264 288 183  67 198 203  15 219  73 132   0 277  47 298 225 171\n",
      " 279 281 227 127  31 193  84 166 276 245  38 189  63 170 250  72 128  10\n",
      " 278  45 123 237  36 299 134 186 262 234  43  94 173  14   4  19  88 101\n",
      " 155 133  20 252  39  53 208 147 105   1 218  24  59 169 243  26 151   7\n",
      " 157  13 138  58 229  60 207  79  35  49  74  86 175  27 131   6 266 285\n",
      " 259 174  46 188  17 204 223 118  82 190 238  64]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[70 63 90 61 58 80  8 25 76 68 31  1 57 62 23  0 64 50 11 13 88 97 96 73\n",
      " 98 67  4 60 42 83 65 26 53 74 99 55 47  3 18 78 29 32 49 33 51 79 85 91\n",
      " 27 86 36 82 75 56 41 81 87 95 39 44 94 28  5 84 40 72  7  9 34 22 45 17\n",
      " 35 71 66 12 14 19 77 38 10 92  2 24 21 52 30 16 46 20  6 54 69 89 43 93\n",
      " 37 15 48 59]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5115 - accuracy: 0.9521 - val_loss: 1.5066 - val_accuracy: 0.9566\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4980 - accuracy: 0.9650 - val_loss: 1.4983 - val_accuracy: 0.9638\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4927 - accuracy: 0.9693 - val_loss: 1.5001 - val_accuracy: 0.9618\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4867 - accuracy: 0.9752 - val_loss: 1.4905 - val_accuracy: 0.9709\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4838 - accuracy: 0.9782 - val_loss: 1.4897 - val_accuracy: 0.9718\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4815 - accuracy: 0.9802 - val_loss: 1.4865 - val_accuracy: 0.9754\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4792 - accuracy: 0.9823 - val_loss: 1.4889 - val_accuracy: 0.9727\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4778 - accuracy: 0.9838 - val_loss: 1.4861 - val_accuracy: 0.9751\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9856 - val_loss: 1.4850 - val_accuracy: 0.9758\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4750 - accuracy: 0.9866 - val_loss: 1.4872 - val_accuracy: 0.9738\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4747 - accuracy: 0.9867 - val_loss: 1.4824 - val_accuracy: 0.9793\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4738 - accuracy: 0.9876 - val_loss: 1.4833 - val_accuracy: 0.9777\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4736 - accuracy: 0.9876 - val_loss: 1.4826 - val_accuracy: 0.9786\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4728 - accuracy: 0.9887 - val_loss: 1.4831 - val_accuracy: 0.9778\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:04, 124.75s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 2\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[304 191 321 493 296 625 277 181 153  32 113 320 462 404 702 118 203 487\n",
      "  99 539 400 211 262 233 776 683  14 622 523 222 779 475 636 244 327 312\n",
      " 553 446  80 316 492 501 106  15 266  37 441 578 645 343 444 467 715 214\n",
      " 556 111  44 691 727 511 535  72 340 205 648 135 623 469 628 480 318 432\n",
      " 436 319 429 325 180 194 348 473 330 540 582 730 681 550 763 260 704 549\n",
      " 125 388 238 569 371 241 305 574 199 447 716 354  86 195 555 560 380 398\n",
      " 331 781 680 234 276 659 167 609 530 661 777 300 464  70  17 126 337 684\n",
      " 334 202 150 284   1 248 223 102 710 600  46  33 499 185 433 605 721 724\n",
      " 614 326 328 236 729 765 769 317 690 567 607  36 421 552 735 638 140 706\n",
      " 177 599 620 612  10 685 585  25 644  23 342  38 666 287 281 745 265 420\n",
      " 460 531 122 700 728  92 456  53 283 698 573 712 542 229 418 385 481 228\n",
      " 155 372  24 170  52  82 217  64 594 643 256 313 412 263 409 196 533  98\n",
      "  20 496 274 466  43 536 213 324 558 440 722 310 395  62 736 548 718 286\n",
      " 465 592 128 478 657  11 637 682 156 219 744 780 619 752  16 376  49 439\n",
      " 770 355 486 668 610  95 352 554 231 755 719 751 520 221 193 413 731  87\n",
      " 588 601 358 338 528 190 494 449 517 507   2 693 457 509  42 252 291  47\n",
      " 670 225 279 154 515 384 589 596 215 435 754 364 307 442 273  45 587   7\n",
      " 182 189 309 366 292  76 374  27 663 749 184   3  85 210 709 576 674  61\n",
      " 206 402 504 346 209 178 143 597 761 160 363 541 254 459 738  51 174 107\n",
      " 375 415  40 201 288 679 631 474 360  65 293 220 654  54 417 424 742 163\n",
      " 377 389 114 518 171  78 608 152 362 498 477 676  79 306 510 427 590 651\n",
      " 235 458  50 267 138   5 771 131 616 694  56 725 732 351 454 108 545 522\n",
      " 463 109 357 365 187  84 451 624 746 197 737 130 141 335 208 249 453 572\n",
      " 416  19 259 407 386 655 564 290  96 423 344 124 562  18 361 557 720 445\n",
      " 461 603 243 165  81 630 110 151  34 168 127 568 172  90 503 717 142 345\n",
      " 748 580 750 626 753 584 711 759 505 758 438 123 640 734 527 227 245 406\n",
      " 688 322 686  73 289 264 282  63 144 703 768 349 303 561 169 134 207 782\n",
      " 526 591 116 512 488 559 242 188 333 767 175 369 100 450 665  69 669 516\n",
      " 396 525 529 216 519 394 662 391 336 350 430  21 272 537 723 133 341 485\n",
      " 164   0 419 173 664 577  30 370 218 119 339 103 149  93 641 452 146 508\n",
      " 401 121 482 411 269 200 387 604 431 472 586 583 301 544 707 696 425 323\n",
      " 672 491 611 667 314 280 159 101 157 212 373 428 383 117 701  60 650  71\n",
      " 618 747 434 565 671 656 483 246 692 476 563 240 278 697 733 615 204  35\n",
      " 566 632 145 408 261 353 647 315 524 506 198 147 778 166 329 773 239 783\n",
      " 308 137 634 649 132  67 502  41 764 581 538 695  75 606 161 426 708 633\n",
      " 270 271 230 390 639 570 297  59 347 775 743 162  13 448 179 275 653 176\n",
      " 646 422 642 257 186 756  97 455 760 356 571 139 593 302  94   9 602  57\n",
      " 495  66  91  22 479 112  58 397 500 575 148 774 675  88  28 232 294 714\n",
      " 471   8 359 497 381 687 382 104 255 183 673 677  55 532 368 468 534 579\n",
      "   6  29 437  68 403 392 613 741  77 285  74 739 443 689 247  48 250 621\n",
      " 295 551 762 490   4 470 405 521 652 224 105 514  89 158 705 378 253 311\n",
      " 414  39 120 226 699 617 237 726  83 251 399 393 629 136 772 595  26 410\n",
      " 658  31 268 299 757 379 489 766 367 547 192 660 543 484 635 546 298 513\n",
      " 740 678 713 627  12 258 129 332 598 115]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[231  47 290 256 292 236 180 172  96   7 162  78 268 278  63 175   0 242\n",
      " 110 133 126  88 142 118  79 163  29 109  18   3 289  53 189  30  89  91\n",
      " 108 244 251 166 294 222  52 157 170 101 224  74 100 277  37 232 248 282\n",
      "  16 201 275 160  93  22 146  66  50 214  39  76  33  10 217 123 261 141\n",
      "  60 245 114 174  73 177 270 188 125 187 205 173  35 198 227  49 128 113\n",
      " 218 230 136 116 247 104 197  90  58 295 139  45  44 179  20 279 117  62\n",
      " 228 267  56 122  92  83  87 246 285 120 153 288 119  80 225 212 102 191\n",
      "  57 280 137  65 243 269 183 186 254 168 283  24  64 202 195 258   9 249\n",
      " 154 210 234  32 259 167  71  42  55 199   6 169 215 291 161 252 253  85\n",
      "  23 221   4   1 194 237  46 196 143 260 134  12 200 272 299 192  11  81\n",
      "  86  94 193  95 284  31 266 144 239  75 127 148 297  59 238   5 271 213\n",
      "  54 140 264 111 105  17  43 115 207 145 185 159 190  27 129  77 135 229\n",
      " 203 211  41  19 182 150 208  67 240  26 219  82 296   2  25  21  84 156\n",
      " 223  36  13 147  40  38  48 158 171  15 178 184  72  51 220 106  99 155\n",
      " 216 138 263 257 226 151 206 107 164 233 112 209 204  28  69  70 273 181\n",
      " 281 276 121 152  61 265 165 255 124 262 293 286  34 176 130 235 298  98\n",
      " 131  97  68 103  14 287 250 132 274 241   8 149]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[86 74 89 60  2 40 55  1 51 83 19 80 39 25 28 70 79 16 73 13  8 85 31 65\n",
      " 96 72 87 71  0 62 17 66 64 49 78 32 11 75 23 27 20 45 76 77 88 63 84 95\n",
      " 36 56 90 10 15 94 99 48 38 52  7 92 21 67  3 68 22  9  6 24 61 34 42 57\n",
      " 50 35  4 30 33 41 59 54 29 98 81 91 14 97 47 18 12 26 53 37 82  5 58 44\n",
      " 93 43 46 69]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4971 - accuracy: 0.9729 - val_loss: 1.4929 - val_accuracy: 0.9721\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4820 - accuracy: 0.9822 - val_loss: 1.4920 - val_accuracy: 0.9708\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4779 - accuracy: 0.9856 - val_loss: 1.4883 - val_accuracy: 0.9744\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9873 - val_loss: 1.4875 - val_accuracy: 0.9749\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4739 - accuracy: 0.9888 - val_loss: 1.4884 - val_accuracy: 0.9734\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4729 - accuracy: 0.9894 - val_loss: 1.4877 - val_accuracy: 0.9736\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4721 - accuracy: 0.9903 - val_loss: 1.4885 - val_accuracy: 0.9742\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:51, 119.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 2\n",
      "rows to prune in layer 0 : 588\n",
      "[  0   4   6   8   9  12  13  18  19  21  22  26  28  29  30  31  34  35\n",
      "  39  41  48  55  57  58  59  60  63  66  67  68  69  71  73  74  75  77\n",
      "  81  83  84  88  89  90  91  93  94  96  97 100 101 103 104 105 108 109\n",
      " 110 112 115 116 117 119 120 121 123 124 127 129 130 132 133 134 136 137\n",
      " 139 141 142 144 145 146 147 148 149 151 157 158 159 161 162 164 165 166\n",
      " 168 169 172 173 175 176 179 183 186 187 188 192 197 198 200 204 207 208\n",
      " 212 216 218 224 226 227 230 232 237 239 240 242 243 245 246 247 249 250\n",
      " 251 253 255 257 258 259 261 264 268 269 270 271 272 275 278 280 282 285\n",
      " 289 290 294 295 297 298 299 301 302 303 308 311 314 315 322 323 329 332\n",
      " 333 335 336 339 341 344 345 347 349 350 353 356 357 359 361 365 367 368\n",
      " 369 370 373 378 379 381 382 383 386 387 390 391 392 393 394 396 397 399\n",
      " 401 403 405 406 407 408 410 411 414 416 419 422 423 425 426 428 430 431\n",
      " 434 437 438 443 445 448 450 451 452 453 454 455 461 463 468 470 471 472\n",
      " 476 479 482 483 484 485 488 489 490 491 495 497 500 502 503 505 506 508\n",
      " 512 513 514 516 519 521 522 524 525 526 527 529 532 534 537 538 543 544\n",
      " 545 546 547 551 557 559 561 562 563 564 565 566 568 570 571 572 575 577\n",
      " 579 580 581 583 584 586 591 593 595 598 602 603 604 606 611 613 615 617\n",
      " 618 621 624 626 627 629 630 632 633 634 635 639 640 641 642 646 647 649\n",
      " 650 652 653 655 656 658 660 662 664 665 667 669 671 672 673 675 677 678\n",
      " 686 687 688 689 692 695 696 697 699 701 703 705 707 708 711 713 714 717\n",
      " 720 723 726 733 734 737 739 740 741 743 746 747 748 750 753 756 757 758\n",
      " 759 760 762 764 766 767 768 772 773 774 775 778 782 783]\n",
      "[165 161 339 565 482 416   6 116 692 216 411 124  22 139 200 226 618 687\n",
      " 179 580 720 489 695 242 280 532 382 123 768 173 739  81 604 490 294 767\n",
      " 497 408 431 783 401 183 198 430 448 451 583  58 519 347 566 737  73 564\n",
      " 595  48 115 485 391  29 656  66 137 581 383 361  71 356  59 529 545 649\n",
      " 270  34  21   4 757 157 443 598 249 245 390 350 257 387  28 461 104 353\n",
      " 359 658 311 132 575 344 159 559  69 240 112 653 392 455  13 772 522 671\n",
      " 723 335  18 470 314 505 764 484 613 259 268 176 186 579 419 158 678 476\n",
      " 537 396 136 527 332 289 571 394 450   8 103 299 369 664 290 479 714 232\n",
      " 516  39 162 760 282 261 514 322 373 707 688 271 425 357 378 145 756 512\n",
      " 393 169 218 370 142 526 301 502 345 406 632 775  83  91 105 302 611  88\n",
      " 336 508 503  63  84  19 381 615 264 672 635 758 603 303 577 762 697 778\n",
      " 673 650 561 141 669 188 675 563 642 349 647 278 208 546 602 521 121 629\n",
      " 172   0 617 285 543 452 438 686 253 640 230 572 593 633 689 472 397  89\n",
      " 379 109 454 453 660 341 495 110 368  67 239   9 696 405  90  26 272 133\n",
      " 641 399 197 525 120 151 488  57 483 500 100 297 606 703 759 750 437 547\n",
      "  31 422 634 119 367 774  77 627 747 524 168 534 386 298 251 147 662 734\n",
      " 134 246 365 212  30 717 630 513 269  74 204 743 187 333 463  97 711 506\n",
      "  55 166  75 434 621  41 570  12 586 766 414  60 224 426 705 250 146 323\n",
      " 471 127 407 557 164 544 639  68 227 624 129 746 108 646 149 315 130 753\n",
      " 652  96 562  93 773 101 667 237 748 677 329 665 410 491 308 144 247  94\n",
      " 403 192 117 655 468 708 626 699 255 782 591 713 701 584 538 148 175 295\n",
      "  35 428 243 740 275 445 726 568 423 207 741 551 733 258]\n",
      "rows to prune in layer 3 : 225\n",
      "[  1   2   4   5   6   8  11  12  13  14  15  17  19  21  23  25  26  27\n",
      "  28  31  34  36  38  40  41  42  43  46  48  51  54  55  59  61  67  68\n",
      "  69  70  71  72  75  77  81  82  84  85  86  94  95  97  98  99 103 105\n",
      " 106 107 111 112 115 121 124 127 129 130 131 132 134 135 138 140 143 144\n",
      " 145 147 148 149 150 151 152 155 156 158 159 161 164 165 169 171 176 178\n",
      " 181 182 184 185 190 192 193 194 196 199 200 203 204 206 207 208 209 211\n",
      " 213 215 216 219 220 221 223 226 229 233 235 237 238 239 240 241 250 252\n",
      " 253 255 257 260 262 263 264 265 266 271 272 273 274 276 281 284 286 287\n",
      " 291 293 296 297 298 299]\n",
      "[298  95 194  25 276 211 284 143 286   5 105  51  54 266  97  17  42 281\n",
      "  12 203 287  77 240  85  55  27 112 124 273  98 138 235 237 252  46 200\n",
      " 156  31  86  61 226  11 219 147  70 265 164 182 150  81 171  43 263 178\n",
      " 159 158  68 220 255 216 129   8 206  36 131 297 165 176   4 296 151 233\n",
      " 184 299 239 140 152   1  99 148 149  15 253 134 250 144 115  21 209  82\n",
      " 196 132  38 169 204 221   2 260   6  72  59 155 213 192  69  28  13 238\n",
      " 103  84 135  26 291 107 223 274 241 208 262 127 190 185  40 199 193 145\n",
      "  19  48 207  94 121 181  14 215  23 161 257 106 111 264  71  34  41 293\n",
      "  67 229  75 130 272 271]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 3  4  5  6  7  9 10 12 14 15 18 21 22 24 26 29 30 33 34 35 37 38 41 42\n",
      " 43 44 46 47 48 50 52 53 54 57 58 59 61 67 68 69 81 82 90 91 92 93 94 97\n",
      " 98 99]\n",
      "[57  3 61 43 46 50 48 90 33 21 93 10 24 67 14 52  9 34 59 18 37 29 58 30\n",
      " 92 99  4 35 38 97  6  5 42 53 81 26 22 54 15 82 44 98 69 47 41  7 94 12\n",
      " 68 91]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5652 - accuracy: 0.9202 - val_loss: 1.5298 - val_accuracy: 0.9402\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5193 - accuracy: 0.9509 - val_loss: 1.5193 - val_accuracy: 0.9485\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5096 - accuracy: 0.9585 - val_loss: 1.5141 - val_accuracy: 0.9511\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5039 - accuracy: 0.9628 - val_loss: 1.5124 - val_accuracy: 0.9515\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4999 - accuracy: 0.9663 - val_loss: 1.5083 - val_accuracy: 0.9558\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4965 - accuracy: 0.9693 - val_loss: 1.5079 - val_accuracy: 0.9551\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4941 - accuracy: 0.9711 - val_loss: 1.5073 - val_accuracy: 0.9558\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4920 - accuracy: 0.9724 - val_loss: 1.5067 - val_accuracy: 0.9558\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4906 - accuracy: 0.9737 - val_loss: 1.5045 - val_accuracy: 0.9576\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4889 - accuracy: 0.9754 - val_loss: 1.5052 - val_accuracy: 0.9584\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4876 - accuracy: 0.9763 - val_loss: 1.5042 - val_accuracy: 0.9579\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4867 - accuracy: 0.9769 - val_loss: 1.5053 - val_accuracy: 0.9568\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4860 - accuracy: 0.9779 - val_loss: 1.5041 - val_accuracy: 0.9579\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4850 - accuracy: 0.9785 - val_loss: 1.5056 - val_accuracy: 0.9563\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4839 - accuracy: 0.9793 - val_loss: 1.5081 - val_accuracy: 0.9534\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.9801 - val_loss: 1.5029 - val_accuracy: 0.9592\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4831 - accuracy: 0.9800 - val_loss: 1.5037 - val_accuracy: 0.9585\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4822 - accuracy: 0.9807 - val_loss: 1.5048 - val_accuracy: 0.9570\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4818 - accuracy: 0.9813 - val_loss: 1.5057 - val_accuracy: 0.9554\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:18, 127.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 2\n",
      "rows to prune in layer 0 : 686\n",
      "[  0   9  12  26  30  31  35  41  55  57  60  67  68  74  75  77  89  90\n",
      "  93  94  96  97 100 101 108 109 110 117 119 120 121 127 129 130 133 134\n",
      " 141 144 146 147 148 149 151 164 166 168 172 175 187 188 192 197 204 207\n",
      " 208 212 224 227 230 237 239 243 246 247 250 251 253 255 258 269 272 275\n",
      " 278 285 295 297 298 308 315 323 329 333 341 349 365 367 368 379 386 397\n",
      " 399 403 405 407 410 414 422 423 426 428 434 437 438 445 452 453 454 463\n",
      " 468 471 472 483 488 491 495 500 506 513 521 524 525 534 538 543 544 546\n",
      " 547 551 557 561 562 563 568 570 572 584 586 591 593 602 606 617 621 624\n",
      " 626 627 629 630 633 634 639 640 641 642 646 647 650 652 655 660 662 665\n",
      " 667 669 673 675 677 686 689 696 697 699 701 703 705 708 711 713 717 726\n",
      " 733 734 740 741 743 746 747 748 750 753 759 766 773 774 778 782]\n",
      "[438 711 562 748 453 251 110 491 561 621 127 349  26 584 298 483 379 713\n",
      " 642 606 329  93 641  57 414 246  90 269 109 426 197 253 624 130 403  31\n",
      " 204 454  41 646 669 667 172 247 602 547 543 774 557 699 647 101  55 452\n",
      " 434 134 151 175 365 689 626 773 563 629 308 141 121 708 746 471  97 673\n",
      " 495  77 133 627 258 108 778 546 399 617  68 278 665 207 696 275 733 166\n",
      " 120 164 570 333 437 705 500  94 208 551 759 740 297 734 593  35 544 586\n",
      "  30 743 129 168 367 386 315  67 463 686 639   0 717 397 255 650 148 675\n",
      " 726 272 640 227 513 697 147 341 741 295 633  74 655 703 428 243 782 368\n",
      " 750 677 521  12 192 634 747 572 766 524 422 660 445  75 468 100 423 538\n",
      " 212 701 285  60 224 188 472 662 146 410 568 591   9 525 239 237 652 323\n",
      " 149 405 250 630 488 119  96 506 534 187 407 753 144 230  89 117]\n",
      "rows to prune in layer 3 : 262\n",
      "[  1   2   6  13  14  15  19  21  23  26  28  34  38  40  41  48  59  67\n",
      "  69  71  72  75  82  84  94  99 103 106 107 111 115 121 127 130 132 134\n",
      " 135 140 144 145 148 149 152 155 161 169 181 185 190 192 193 196 199 204\n",
      " 207 208 209 213 215 221 223 229 238 241 250 253 257 260 262 264 271 272\n",
      " 274 291 293]\n",
      "[140 115 215 260  72 208  14 161 130 271 181  15 264  69 229  67  21 193\n",
      "  82 241 145 291 221  41   6  75  59 213  23 155 121 223 135  38  34 152\n",
      "  28 103 148 144 293 134 238   1 132  40 207  19   2 253  84 204 149 185\n",
      " 199 127  13 272 192 274 209  71 196 106  99 190  26 262 257  48 250  94\n",
      " 169 111 107]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 4  5  6  7 12 15 22 26 35 38 41 42 44 47 53 54 68 69 81 82 91 94 97 98\n",
      " 99]\n",
      "[ 4 91 47 44  7 42 82 98 69 53 99 97 38 35 68 26  5 94 12 41 81 22 15 54\n",
      "  6]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6714 - accuracy: 0.8152 - val_loss: 1.5909 - val_accuracy: 0.8837\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5813 - accuracy: 0.8922 - val_loss: 1.5711 - val_accuracy: 0.9007\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5674 - accuracy: 0.9036 - val_loss: 1.5627 - val_accuracy: 0.9049\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5606 - accuracy: 0.9088 - val_loss: 1.5582 - val_accuracy: 0.9091\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5554 - accuracy: 0.9126 - val_loss: 1.5569 - val_accuracy: 0.9096\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5518 - accuracy: 0.9161 - val_loss: 1.5548 - val_accuracy: 0.9097\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5489 - accuracy: 0.9183 - val_loss: 1.5541 - val_accuracy: 0.9088\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5463 - accuracy: 0.9206 - val_loss: 1.5519 - val_accuracy: 0.9114\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5446 - accuracy: 0.9220 - val_loss: 1.5526 - val_accuracy: 0.9113\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5423 - accuracy: 0.9239 - val_loss: 1.5510 - val_accuracy: 0.9127\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5412 - accuracy: 0.9247 - val_loss: 1.5511 - val_accuracy: 0.9129\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5398 - accuracy: 0.9263 - val_loss: 1.5481 - val_accuracy: 0.9157\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5382 - accuracy: 0.9272 - val_loss: 1.5474 - val_accuracy: 0.9164\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5372 - accuracy: 0.9287 - val_loss: 1.5480 - val_accuracy: 0.9140\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5364 - accuracy: 0.9294 - val_loss: 1.5485 - val_accuracy: 0.9148\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5357 - accuracy: 0.9294 - val_loss: 1.5479 - val_accuracy: 0.9159\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [08:22, 126.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 2\n",
      "rows to prune in layer 0 : 735\n",
      "[  0   9  12  30  35  60  67  74  75  89  96 100 117 119 129 144 146 147\n",
      " 148 149 168 187 188 192 208 212 224 227 230 237 239 243 250 255 272 285\n",
      " 295 297 315 323 341 367 368 386 397 405 407 410 422 423 428 445 463 468\n",
      " 472 488 506 513 521 524 525 534 538 544 551 568 572 586 591 593 630 633\n",
      " 634 639 640 650 652 655 660 662 675 677 686 697 701 703 717 726 734 740\n",
      " 741 743 747 750 753 759 766 782]\n",
      "[146 445 149 168 423 686 239 407 747 472 759 227 766 250 697 639 367 525\n",
      " 655 144 538 703 743 544 726 230 586 634 652 463 368   0 782 551 428  12\n",
      " 187 524  74 237 675 397  60 212 323 148 513 422  30 410 295 315 633  75\n",
      " 386 662 568 468 521 243 188 405 740 224 753  89 677 630 147  96 506 591\n",
      " 572 192 593 640 701 534 650 660 741 119 297   9 285 129 341 117 100  35\n",
      " 488  67 717 255 750 272 734 208]\n",
      "rows to prune in layer 3 : 281\n",
      "[  1   2  13  19  26  40  48  71  84  94  99 103 106 107 111 127 132 134\n",
      " 144 148 149 169 185 190 192 196 199 204 207 209 238 250 253 257 262 272\n",
      " 274 293]\n",
      "[ 84 192   2  40  19 199 111 185 169 149 106 274  99 262 134  48 250  13\n",
      " 253 144 204 103 196 257  26 238 107 272 132  94   1 190 209 148  71 293\n",
      " 127 207]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 5  6 12 15 22 26 35 38 41 54 68 81 94]\n",
      "[ 5 15 35 68 81  6 22 38 54 94 26 12 41]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9716 - accuracy: 0.4990 - val_loss: 1.9187 - val_accuracy: 0.5489\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8778 - accuracy: 0.5931 - val_loss: 1.8479 - val_accuracy: 0.6237\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8305 - accuracy: 0.6387 - val_loss: 1.8286 - val_accuracy: 0.6358\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8183 - accuracy: 0.6489 - val_loss: 1.8198 - val_accuracy: 0.6445\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7895 - accuracy: 0.6794 - val_loss: 1.7747 - val_accuracy: 0.6913\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7604 - accuracy: 0.7094 - val_loss: 1.7671 - val_accuracy: 0.6984\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7537 - accuracy: 0.7136 - val_loss: 1.7591 - val_accuracy: 0.7057\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7486 - accuracy: 0.7181 - val_loss: 1.7553 - val_accuracy: 0.7085\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7453 - accuracy: 0.7212 - val_loss: 1.7510 - val_accuracy: 0.7133\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7424 - accuracy: 0.7237 - val_loss: 1.7486 - val_accuracy: 0.7152\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7401 - accuracy: 0.7256 - val_loss: 1.7473 - val_accuracy: 0.7164\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7381 - accuracy: 0.7274 - val_loss: 1.7474 - val_accuracy: 0.7153\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7360 - accuracy: 0.7291 - val_loss: 1.7437 - val_accuracy: 0.7208\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7347 - accuracy: 0.7297 - val_loss: 1.7431 - val_accuracy: 0.7197\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7331 - accuracy: 0.7317 - val_loss: 1.7412 - val_accuracy: 0.7212\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7321 - accuracy: 0.7323 - val_loss: 1.7402 - val_accuracy: 0.7220\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7308 - accuracy: 0.7332 - val_loss: 1.7390 - val_accuracy: 0.7236\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7298 - accuracy: 0.7343 - val_loss: 1.7398 - val_accuracy: 0.7218\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7289 - accuracy: 0.7353 - val_loss: 1.7391 - val_accuracy: 0.7229\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7025 - accuracy: 0.7623 - val_loss: 1.6902 - val_accuracy: 0.7728\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6814 - accuracy: 0.7833 - val_loss: 1.6842 - val_accuracy: 0.7793\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6764 - accuracy: 0.7883 - val_loss: 1.6805 - val_accuracy: 0.7831\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6728 - accuracy: 0.7918 - val_loss: 1.6807 - val_accuracy: 0.7817\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6707 - accuracy: 0.7942 - val_loss: 1.6756 - val_accuracy: 0.7872\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6688 - accuracy: 0.7965 - val_loss: 1.6745 - val_accuracy: 0.7883\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6673 - accuracy: 0.7973 - val_loss: 1.6730 - val_accuracy: 0.7892\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6661 - accuracy: 0.7988 - val_loss: 1.6729 - val_accuracy: 0.7895\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6649 - accuracy: 0.8002 - val_loss: 1.6718 - val_accuracy: 0.7914\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6641 - accuracy: 0.8003 - val_loss: 1.6705 - val_accuracy: 0.7935\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6633 - accuracy: 0.8007 - val_loss: 1.6705 - val_accuracy: 0.7941\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6623 - accuracy: 0.8017 - val_loss: 1.6697 - val_accuracy: 0.7928\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6615 - accuracy: 0.8022 - val_loss: 1.6676 - val_accuracy: 0.7948\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6606 - accuracy: 0.8035 - val_loss: 1.6686 - val_accuracy: 0.7949\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6601 - accuracy: 0.8042 - val_loss: 1.6669 - val_accuracy: 0.7944\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6591 - accuracy: 0.8046 - val_loss: 1.6671 - val_accuracy: 0.7949\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6586 - accuracy: 0.8055 - val_loss: 1.6654 - val_accuracy: 0.7968\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6578 - accuracy: 0.8061 - val_loss: 1.6656 - val_accuracy: 0.7970\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6571 - accuracy: 0.8067 - val_loss: 1.6646 - val_accuracy: 0.7994\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6565 - accuracy: 0.8075 - val_loss: 1.6647 - val_accuracy: 0.7979\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6560 - accuracy: 0.8077 - val_loss: 1.6641 - val_accuracy: 0.7993\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6555 - accuracy: 0.8079 - val_loss: 1.6624 - val_accuracy: 0.7987\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6545 - accuracy: 0.8095 - val_loss: 1.6622 - val_accuracy: 0.7999\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6539 - accuracy: 0.8098 - val_loss: 1.6640 - val_accuracy: 0.7987\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6535 - accuracy: 0.8103 - val_loss: 1.6611 - val_accuracy: 0.8011\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6529 - accuracy: 0.8105 - val_loss: 1.6604 - val_accuracy: 0.8019\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6524 - accuracy: 0.8113 - val_loss: 1.6607 - val_accuracy: 0.8022\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6519 - accuracy: 0.8119 - val_loss: 1.6613 - val_accuracy: 0.8028\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6516 - accuracy: 0.8113 - val_loss: 1.6601 - val_accuracy: 0.8014\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6511 - accuracy: 0.8125 - val_loss: 1.6583 - val_accuracy: 0.8048\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6508 - accuracy: 0.8129 - val_loss: 1.6593 - val_accuracy: 0.8023\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6504 - accuracy: 0.8128 - val_loss: 1.6584 - val_accuracy: 0.8044\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6499 - accuracy: 0.8140 - val_loss: 1.6585 - val_accuracy: 0.8040\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:01, 154.42s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 2\n",
      "rows to prune in layer 0 : 759\n",
      "[  9  35  67  75  89  96 100 117 119 129 147 188 192 208 224 243 255 272\n",
      " 285 295 297 315 341 386 405 410 468 488 506 521 534 568 572 591 593 630\n",
      " 633 640 650 660 662 677 701 717 734 740 741 750 753]\n",
      "[640 315 741 410 660 630 295  89   9 488 386 341  96 129 117 119 734 701\n",
      " 243  75  35 677 750 100 593  67 534 572 192 188 272 717 521 255 662 568\n",
      " 506 468 405 224 650 147 208 740 633 297 285 753 591]\n",
      "rows to prune in layer 3 : 290\n",
      "[  1  26  71  94 103 107 127 132 144 148 190 196 204 207 209 238 257 272\n",
      " 293]\n",
      "[107 272 132 238 196 207 204 127  26 190 209 103  94 293 257 148   1 144\n",
      "  71]\n",
      "rows to prune in layer 6 : 96\n",
      "[12 22 26 38 41 54 94]\n",
      "[41 12 22 38 94 26 54]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1807 - accuracy: 0.2816 - val_loss: 2.0959 - val_accuracy: 0.3756\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0884 - accuracy: 0.3795 - val_loss: 2.0744 - val_accuracy: 0.3902\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0689 - accuracy: 0.3941 - val_loss: 2.0569 - val_accuracy: 0.4057\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0538 - accuracy: 0.4091 - val_loss: 2.0487 - val_accuracy: 0.4110\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0475 - accuracy: 0.4146 - val_loss: 2.0445 - val_accuracy: 0.4169\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0434 - accuracy: 0.4177 - val_loss: 2.0415 - val_accuracy: 0.4181\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0403 - accuracy: 0.4209 - val_loss: 2.0387 - val_accuracy: 0.4196\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0335 - accuracy: 0.4289 - val_loss: 2.0313 - val_accuracy: 0.4300\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0301 - accuracy: 0.4315 - val_loss: 2.0295 - val_accuracy: 0.4303\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0287 - accuracy: 0.4325 - val_loss: 2.0280 - val_accuracy: 0.4327\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0276 - accuracy: 0.4340 - val_loss: 2.0272 - val_accuracy: 0.4330\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0266 - accuracy: 0.4349 - val_loss: 2.0261 - val_accuracy: 0.4336\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0259 - accuracy: 0.4354 - val_loss: 2.0264 - val_accuracy: 0.4329\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0253 - accuracy: 0.4360 - val_loss: 2.0256 - val_accuracy: 0.4342\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0247 - accuracy: 0.4361 - val_loss: 2.0244 - val_accuracy: 0.4357\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0241 - accuracy: 0.4364 - val_loss: 2.0244 - val_accuracy: 0.4370\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0238 - accuracy: 0.4369 - val_loss: 2.0240 - val_accuracy: 0.4348\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0234 - accuracy: 0.4374 - val_loss: 2.0233 - val_accuracy: 0.4367\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0230 - accuracy: 0.4378 - val_loss: 2.0228 - val_accuracy: 0.4368\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0226 - accuracy: 0.4383 - val_loss: 2.0231 - val_accuracy: 0.4376\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0222 - accuracy: 0.4385 - val_loss: 2.0224 - val_accuracy: 0.4383\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0219 - accuracy: 0.4387 - val_loss: 2.0227 - val_accuracy: 0.4362\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0217 - accuracy: 0.4388 - val_loss: 2.0218 - val_accuracy: 0.4375\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0215 - accuracy: 0.4392 - val_loss: 2.0220 - val_accuracy: 0.4365\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0212 - accuracy: 0.4391 - val_loss: 2.0217 - val_accuracy: 0.4375\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0210 - accuracy: 0.4392 - val_loss: 2.0213 - val_accuracy: 0.4375\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0207 - accuracy: 0.4398 - val_loss: 2.0215 - val_accuracy: 0.4380\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0206 - accuracy: 0.4396 - val_loss: 2.0214 - val_accuracy: 0.4381\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0203 - accuracy: 0.4401 - val_loss: 2.0211 - val_accuracy: 0.4385\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0203 - accuracy: 0.4399 - val_loss: 2.0207 - val_accuracy: 0.4395\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0202 - accuracy: 0.4401 - val_loss: 2.0210 - val_accuracy: 0.4390\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0201 - accuracy: 0.4405 - val_loss: 2.0206 - val_accuracy: 0.4390\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0198 - accuracy: 0.4409 - val_loss: 2.0203 - val_accuracy: 0.4389\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0198 - accuracy: 0.4404 - val_loss: 2.0206 - val_accuracy: 0.4398\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0196 - accuracy: 0.4408 - val_loss: 2.0206 - val_accuracy: 0.4393\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0194 - accuracy: 0.4411 - val_loss: 2.0205 - val_accuracy: 0.4401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [15:14, 165.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 2\n",
      "rows to prune in layer 0 : 771\n",
      "[ 67 147 188 192 208 224 255 272 285 297 405 468 506 521 534 568 572 591\n",
      " 593 633 650 662 717 740 753]\n",
      "[650 534 192 208 405 506 224 633 521 593 272 188 662 297  67 568 717 591\n",
      " 147 255 285 740 468 572 753]\n",
      "rows to prune in layer 3 : 295\n",
      "[  1  71  94 103 144 148 190 209 257 293]\n",
      "[  1 144 293 257 190 148 103  94  71 209]\n",
      "rows to prune in layer 6 : 98\n",
      "[26 38 54 94]\n",
      "[38 54 94 26]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2858 - accuracy: 0.1667 - val_loss: 2.2706 - val_accuracy: 0.1803\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2667 - accuracy: 0.1813 - val_loss: 2.2563 - val_accuracy: 0.1817\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2395 - accuracy: 0.1979 - val_loss: 2.2182 - val_accuracy: 0.2363\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2094 - accuracy: 0.2392 - val_loss: 2.1962 - val_accuracy: 0.2501\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1930 - accuracy: 0.2506 - val_loss: 2.1809 - val_accuracy: 0.2602\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1753 - accuracy: 0.2745 - val_loss: 2.1615 - val_accuracy: 0.2850\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1558 - accuracy: 0.3044 - val_loss: 2.1407 - val_accuracy: 0.3229\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1360 - accuracy: 0.3381 - val_loss: 2.1217 - val_accuracy: 0.3715\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1208 - accuracy: 0.3681 - val_loss: 2.1097 - val_accuracy: 0.3748\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1112 - accuracy: 0.3686 - val_loss: 2.1012 - val_accuracy: 0.3752\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1049 - accuracy: 0.3692 - val_loss: 2.0966 - val_accuracy: 0.3768\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1004 - accuracy: 0.3702 - val_loss: 2.0925 - val_accuracy: 0.3775\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0970 - accuracy: 0.3714 - val_loss: 2.0896 - val_accuracy: 0.3794\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0943 - accuracy: 0.3716 - val_loss: 2.0877 - val_accuracy: 0.3797\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0923 - accuracy: 0.3728 - val_loss: 2.0853 - val_accuracy: 0.3806\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0905 - accuracy: 0.3731 - val_loss: 2.0840 - val_accuracy: 0.3820\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0892 - accuracy: 0.3743 - val_loss: 2.0826 - val_accuracy: 0.3810\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0878 - accuracy: 0.3745 - val_loss: 2.0814 - val_accuracy: 0.3816\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0867 - accuracy: 0.3747 - val_loss: 2.0803 - val_accuracy: 0.3825\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0857 - accuracy: 0.3759 - val_loss: 2.0790 - val_accuracy: 0.3828\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0847 - accuracy: 0.3767 - val_loss: 2.0780 - val_accuracy: 0.3840\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0840 - accuracy: 0.3774 - val_loss: 2.0772 - val_accuracy: 0.3844\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0830 - accuracy: 0.3785 - val_loss: 2.0776 - val_accuracy: 0.3872\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0824 - accuracy: 0.3786 - val_loss: 2.0761 - val_accuracy: 0.3856\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0817 - accuracy: 0.3797 - val_loss: 2.0750 - val_accuracy: 0.3873\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0810 - accuracy: 0.3798 - val_loss: 2.0752 - val_accuracy: 0.3872\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0804 - accuracy: 0.3805 - val_loss: 2.0737 - val_accuracy: 0.3874\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0798 - accuracy: 0.3805 - val_loss: 2.0743 - val_accuracy: 0.3877\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0794 - accuracy: 0.3807 - val_loss: 2.0729 - val_accuracy: 0.3877\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0788 - accuracy: 0.3809 - val_loss: 2.0734 - val_accuracy: 0.3892\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0784 - accuracy: 0.3808 - val_loss: 2.0715 - val_accuracy: 0.3892\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0780 - accuracy: 0.3814 - val_loss: 2.0711 - val_accuracy: 0.3880\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0776 - accuracy: 0.3813 - val_loss: 2.0717 - val_accuracy: 0.3893\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0772 - accuracy: 0.3816 - val_loss: 2.0708 - val_accuracy: 0.3898\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0767 - accuracy: 0.3820 - val_loss: 2.0700 - val_accuracy: 0.3897\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0764 - accuracy: 0.3823 - val_loss: 2.0693 - val_accuracy: 0.3903\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0757 - accuracy: 0.3832 - val_loss: 2.0684 - val_accuracy: 0.3908\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0754 - accuracy: 0.3838 - val_loss: 2.0683 - val_accuracy: 0.3906\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0749 - accuracy: 0.3849 - val_loss: 2.0673 - val_accuracy: 0.3936\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0744 - accuracy: 0.3857 - val_loss: 2.0670 - val_accuracy: 0.3938\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0738 - accuracy: 0.3863 - val_loss: 2.0665 - val_accuracy: 0.3938\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0735 - accuracy: 0.3864 - val_loss: 2.0660 - val_accuracy: 0.3942\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0730 - accuracy: 0.3869 - val_loss: 2.0668 - val_accuracy: 0.3947\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0728 - accuracy: 0.3878 - val_loss: 2.0651 - val_accuracy: 0.3952\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0723 - accuracy: 0.3880 - val_loss: 2.0649 - val_accuracy: 0.3961\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0720 - accuracy: 0.3882 - val_loss: 2.0646 - val_accuracy: 0.3970\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0718 - accuracy: 0.3884 - val_loss: 2.0647 - val_accuracy: 0.3966\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0715 - accuracy: 0.3887 - val_loss: 2.0646 - val_accuracy: 0.3954\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0713 - accuracy: 0.3888 - val_loss: 2.0635 - val_accuracy: 0.3971\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0710 - accuracy: 0.3894 - val_loss: 2.0644 - val_accuracy: 0.3960\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0706 - accuracy: 0.3890 - val_loss: 2.0645 - val_accuracy: 0.3978\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0704 - accuracy: 0.3896 - val_loss: 2.0647 - val_accuracy: 0.3982\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [17:53, 153.40s/it]\u001b[A\n",
      " 30%|███       | 3/10 [54:30<2:07:00, 1088.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5780 - accuracy: 0.8975 - val_loss: 1.5224 - val_accuracy: 0.9417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 3\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[291 620 514 693 343  58 265 159 483 323 523  63  11 650 163 187 204 526\n",
      " 631 316 623 270 575 357 432 471  32 646 518  90 770 546 781 400 385 482\n",
      " 289 615 597 569 476 738 267 763 407 235 140 240 522 641 592 644 328 700\n",
      " 544 701  17 215 590  44 238 777  61 350 424 191 195 619 133 487 275 127\n",
      " 178 558 263 322 588  23 469 486 535 247 723 780 114 672 421 652  41 381\n",
      " 119 331 764 175 563 278 541 600  69 696 123 214 110 297 628 395 643 767\n",
      " 176 695 580 704 121 389 612 577 509 441  50 336 658 717 345 593 670 302\n",
      " 131 444 171  49 261 536 111 775 334  46 390 568 155  89 446  70 132  33\n",
      " 742 141 401 746 431 778 405 223 327 756 117 504  22 651 388 426 122  88\n",
      " 582  25 216 266 630   0 567 549 772 710 773 608 189 616 124 360  18 231\n",
      " 318 689 683 440 715 364 338 538 547  56 180 422  53 719 137  62 692 217\n",
      " 105 425 542 415 220 352 667 466 241 565 640 142 164 299   3 656 387 224\n",
      " 359  29  42 629 583 589   4 213 458 293 113 149 581 559 576 655 136 296\n",
      " 248 253 684 286 513 517  97 512 246 661 668 555 254 368 556 228 339 211\n",
      "  73 349 383 335 258 533 479 377 100 529 184  80 642  36 264 496 510  48\n",
      " 520 147  92 109 586 307 459  38 450 290 222 104 741 418 602 758 393 251\n",
      " 454 463  31 660  66 707 475 188 227 391 329 257 193 447 572 591 386 648\n",
      " 177 511 260 451  24 621 614 573 677 342 194 699 413 534 363  85 687 125\n",
      " 762 310 150 462  82 442 659 653 354 540 731 734 627 330 179 702 309 452\n",
      " 548 292 196 551 500 294 502 663 166 402 694 524 192 325 205  30 420 103\n",
      " 206 115 682 457 543 129 332 118 579 761 664 730 197  65 419 379 198 433\n",
      " 269  67 527 449 703 691 489 776 783 625 574 408 632 461 654 256 271 566\n",
      " 484 148 747 760 532 245 106 353 183 305 473 564  86 169 528   9 406 470\n",
      "  55 506 639 733 429  75 288 485 519 201 410 233  93 480  84 101 151 108\n",
      " 373 300 317 436 498  96 168 439 553 714 771 607 638  54  16 181  26 366\n",
      " 190 472 202 743 326  71 675  76 613 306 680 182 276 130 709 230  45   5\n",
      "   7  47 570 376 516 712 126 412 754 757 324 346 337 674 622 596 594 152\n",
      " 467 725 398 161 515 779 606 143 252 282  19 351 571 319  35 287 637 610\n",
      "  28 313  21 347 200 647 411 525 237 673 242 397 768 595 128 186 697 320\n",
      " 435 212   8 348 417 409 609 481 507 208 437  81 160 384 120 311 488  77\n",
      " 369 455 618 427 726 380 396 226  94 162 720 344 561 144 508  15 748 679\n",
      "  95 102 374 745  40 749  20 729 321 378 315 209 355 716  13 423 280 505\n",
      " 585 493 713 626 428  14 394 539 603 617 284 356 601 649 221 531 295 560\n",
      " 244  12 698 232 497 304 662 460 537 635 174 361 250 633 769 774 685  68\n",
      " 358 665 382 724 134 434 604 112 145 681 165 474 370  79 499 173  37 399\n",
      " 666  39 495 301 521 341 671  57 554  83  72 416 443 503 453 611 445 765\n",
      " 340 210 669 243 782 739   6 308  60 491   1 686 153  59 492 255 225 438\n",
      " 578 403 562 170 279  98 676 218 365 199 530 737 744 477 740 752 478 303\n",
      " 605  52 268 464 732 751 367 727 634 107 545 753 430  51 404 711 281 185\n",
      " 203 750 219  64 273  43 736 172 362  27 262 272  34 372 146 448 587 277\n",
      " 283 657 116 154 599 690 645 274 584  10 249 157 735 468 314  99 552 234\n",
      " 207 259 722 375 501 392 465 636 156 624 708 721 550 755 490  91 718 759\n",
      " 298  87 678 706 456 312 333   2 135 285 494 167 138 688  74 557  78 766\n",
      " 598 371 158 414 728 229 239 139 705 236]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[104 265  33  57  23 292 243 246 119 220 139 100 287 264 268  43 116  55\n",
      " 169 166 106 210  45 294 193  64 163 112 298  79 188  29  27 240  42  19\n",
      "  65  71  14  53 205 151 132 296 189  60 236 110 260  66 105  47  44 197\n",
      " 149 277  78  21 182 101 270 122  73  28  24 271 266 127 150 199 133  13\n",
      " 228 135 221  15  82 291 162   7 214  31 208 121  63 299  46 213 147 245\n",
      " 239 212  67  20 259 111 273 200  99 140 145 196 177 153  76 108 218 267\n",
      "  32 225  89 295  91 248  51  12  49 263  59 281  11 226 244 262  77  26\n",
      " 254 234  94  36 117 156 159 184 198 257 183   9 203  38 227 278  54 160\n",
      " 251 103 253 178 118   2  56 171 123  25   8 282  37 274 115 146  92 216\n",
      "  80  61 272 176 143 158 293 222  95  84  35 275  75 154 174  10 209  68\n",
      " 134 242  22 187  88 231  98   0 102 120 148 217 130 241 173 204  97  18\n",
      "  50 137  74  48 201 181 247 167 164 180  70 113 252 224 276  40 290 107\n",
      " 128  90  58 129 206 144 109 185  85 258   1 233 255  83 207 195  52 232\n",
      " 297 142 229 194  81 114 179 223 141 161   5 219 152 289 170 230 175 285\n",
      "  39 168 186 250  72   3 235 155  87 288 238  30 279  16 215 284 172 131\n",
      " 202 125  41 124 157 249 192  34 191 126  86 190  93 261  17   4 165 286\n",
      "  69 283   6 211 256 269 138  96 280 237  62 136]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[ 8  7 52 56 74 36 91 61 15 38 96 44 64 79 16 87 39 70  9 89 63 20 24 65\n",
      " 37 86 84 13  5 58 60  6 19  2 30 50 31 77 55 69 53 67  3 78 46 98  1 93\n",
      " 48 66 42 29 40 41 34 25 54 28 80 27  0 33 81 73 35 10 94 59  4 17 47 21\n",
      " 75 12 68 62 90 14 76 32 72 88 95 82 45 22 71 97 85 57 51 83 99 92 49 43\n",
      " 18 11 26 23]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5125 - accuracy: 0.9511 - val_loss: 1.5051 - val_accuracy: 0.9592\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4987 - accuracy: 0.9640 - val_loss: 1.4950 - val_accuracy: 0.9668\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4917 - accuracy: 0.9707 - val_loss: 1.4941 - val_accuracy: 0.9679\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4870 - accuracy: 0.9750 - val_loss: 1.4904 - val_accuracy: 0.9719\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4833 - accuracy: 0.9785 - val_loss: 1.4883 - val_accuracy: 0.9735\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4811 - accuracy: 0.9807 - val_loss: 1.4865 - val_accuracy: 0.9753\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9819 - val_loss: 1.4860 - val_accuracy: 0.9760\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4779 - accuracy: 0.9835 - val_loss: 1.4844 - val_accuracy: 0.9771\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4768 - accuracy: 0.9846 - val_loss: 1.4860 - val_accuracy: 0.9751\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4762 - accuracy: 0.9851 - val_loss: 1.4852 - val_accuracy: 0.9760\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4749 - accuracy: 0.9866 - val_loss: 1.4834 - val_accuracy: 0.9780\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4743 - accuracy: 0.9871 - val_loss: 1.4960 - val_accuracy: 0.9652\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4737 - accuracy: 0.9876 - val_loss: 1.4809 - val_accuracy: 0.9807\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4735 - accuracy: 0.9878 - val_loss: 1.4823 - val_accuracy: 0.9789\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9883 - val_loss: 1.4837 - val_accuracy: 0.9772\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4727 - accuracy: 0.9886 - val_loss: 1.4804 - val_accuracy: 0.9806\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.9894 - val_loss: 1.4838 - val_accuracy: 0.9774\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9898 - val_loss: 1.4821 - val_accuracy: 0.9789\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4720 - accuracy: 0.9893 - val_loss: 1.4809 - val_accuracy: 0.9806\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [03:22, 202.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 3\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[204 269 263 108 110 600 605 189 121 372 550 165 389 334 502 546 380  67\n",
      " 450 219 656 183 438 429 417 624 505 460 435 173 129 227 131 463 661 566\n",
      "  93 714 443 713 136 433 749 672 701 234 536 342 619 762 547 332 599  55\n",
      " 548 247  25 259 421 535 348 394 693 592 724 732 609 254 242 596 287 646\n",
      " 270 781  86 273 197 125 285 489  62  16 143 203 469 154  22 244 452 422\n",
      " 133 451 561 739 387 191 730 355 356 406 362 437 164 250 715  59 581 148\n",
      " 214 531 640 168  81 665 686  10 386  30 525 365 346 336 210  56   5 602\n",
      " 288 669 185 495 237 161 737 321 510  73 446 621 286 484 371 488   6 395\n",
      " 742 431 627  28 782  38 466 760 112 671 704 537  45 199 322 538 462  84\n",
      " 305 530 759 493 226 638 127 246  52  31 176 139 504 343 694 101 783 676\n",
      " 545  49 520 643 304 529 498 645 524 200 353 766  26 398 282 147 140 351\n",
      " 184 221 543 427 553  51 130  77  94 455 315 706 748 256  87 439  91 573\n",
      " 245 181  57 155 359 490 295 687 258 186 115  35 670  47 415 141 163 135\n",
      "  90 132 354 668 248 120 567 607 611  24 385 323 622 202 697  76 344 500\n",
      " 207 479 540 775 623  92 644 236 703 777 526 483 731 218 727 251 574 768\n",
      " 591 689 252 190 448 678 660 360 610 277 508 299  12  23 280  18 289 175\n",
      "  14 457 105 156 144 719 201  66 629 593 774  40 648  68 718  11 472 182\n",
      " 474 642 453 501 750 470 516   4 717 523  46 634 174 292 290 459 721  88\n",
      " 673 257 220 311 364  58 560 196 456 213 445 514 375 145 684  17 740 556\n",
      " 641 314 328 468 424 275 114 350 551 100  85 584 265 377 366  36 357 414\n",
      " 209 557 746 617 752 211 733 390 391 449 728  64 193 194 542 217 631 225\n",
      " 650  43 420  15   9 397  34 711 729 419 308 725 408 208 309 361 564 300\n",
      " 392 683 337 569 506  98  71 126 444 630 276 345  96 327 233 745 527 497\n",
      " 381 149 772 382 597 318 651 326 157 461 698 302  53 160 598 618 476 407\n",
      " 539 317 340   1 625 653 232 358   0 434 291 654 418  78 477 663  80 162\n",
      "  89 255 528 249 558 441 111   8 180 680 320 738 179 615 743 636 212 146\n",
      " 482 771 324 649 410 583 773 612  70 373 235 753 652 681 228 475 319 603\n",
      " 741 471  20 113 192 393 465 310 513 206 416 430 666   7 710 464 152  29\n",
      "  42 352 279 770 480 744  82 691 571 473 519 767 158 370   3 764 515 436\n",
      " 491 494 608 580  44 187  63 544 306 685 107 562 606 512 151 349 102 122\n",
      " 655 335 763 266 368 241 268 705  97 260 492 735  33 756 709 778 379 339\n",
      " 278 137 369 432 150 109 637 578  27 613 301 388 590 532 757 496 134 754\n",
      " 294 271 454 293 734 296 509 264 511 442 761 307 283 167 570 447 478 577\n",
      " 230 367 499 736 401 118  50 128 572 633 657 679  69 533 747 700 521 124\n",
      " 338 231  54 576 487 347  39 303 726 626 708 658 413  60 274 616 692 467\n",
      " 383 507 384 601 769 585 565 267 159 696 639 702 486 378 399 166 215 272\n",
      "  48 423 104 559 674 402 481 170 555  37 595 117 142 699 682 664  83  41\n",
      " 662 412 723 755 554 695 224  21 582 503 171  74 563 177 428 604 376 517\n",
      " 188  79 223 588 313 751 411 222 405 284 541 281 779  75 298 568 153 123\n",
      " 103 690  61 238 312 458 216 534 720 552 253 667 169 659  19 579 116 178\n",
      " 229 374 172 341  13  99 575 765 119 316 675 240 677 106 331 409  65 647\n",
      " 400 549 297  95   2 261 396 522 620 776 589 195 330 594 758 363 485 722\n",
      " 198 425 628 712 688 239 614 205  72 632 243 716 587 426 404 518  32 329\n",
      " 333 635 586 262 138 440 707 403 325 780]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[  6  30 237  44  72 165  74 162 183 151 264 249 279 185 252  42 271 266\n",
      " 256 213 181 118 106  26  53 191 173   2  75 196 148 153 192 159 263 114\n",
      " 223  60 248 127 220  99 203 171 206 229 193 194 297  59 167 278 276  56\n",
      "  79  50 115 221  86 135  82  77 156 154  94 105 126  17  13 120 212  11\n",
      "  34 281  25 299  57 172 182  93  27 257  87 146 244  61 245 112  19 272\n",
      "  12  95 215 131 210 195   4 113 277 270  83 227  89  84 108 222  29 169\n",
      " 133   3 104  45 268  47 292 190 204 259 143 187  14 111 139 158 240  49\n",
      " 179 225  92 188   8 273 293 286  15 231 184  41 275 119 147   1 232  16\n",
      " 209  55 138  33 197 234 189 132 150  71 170  38  22 199  97  62 287 219\n",
      "  36  24  81 251 145 262 152  21 109 121   5  43  67  63 166  64 116   9\n",
      "  98 255 236 254  20 238 163 211  91 269 274 288 101  76  73 294 291 235\n",
      " 202  96 164 122 261 155 290 242 214  80  32  40  65 117 243 267  52 218\n",
      "  28  90  18 174 141 265 284  35 137 298 233 205  37 296 110 226  85 282\n",
      " 157 103 124  58 129 186 224  31   0 239 176 289 246 200 283 180 228 149\n",
      " 125 285 136 178 140  39 207 258 260 130  69 250 247 217 253  48 216  68\n",
      "  10 160  54 134 208 161  70 102 295 201  88 142 168 144   7 123 198 177\n",
      "  51 100 241 175 107  23 230  66 128 280  46  78]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[88 44 66  3 45 37 55 39 94 82 78 68 96  9 27 54 26 16  5 97 61 79 84 35\n",
      " 52 38 64 32 92  8 63 14 75 23 12  1 95 30 43 90 25 49 17 28 62 20 86 46\n",
      " 29 33  4 15 72 21 65 22 40 89 85 31 19 18 98 53 73 42 76 93 50 58 11 87\n",
      " 34 67  7 36 74 77 47  2 81 71  0 99 51 41 48 10 60 57 83 69 59 24 70 13\n",
      " 91 80 56  6]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4977 - accuracy: 0.9701 - val_loss: 1.4931 - val_accuracy: 0.9708\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4815 - accuracy: 0.9825 - val_loss: 1.4894 - val_accuracy: 0.9745\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4771 - accuracy: 0.9863 - val_loss: 1.4874 - val_accuracy: 0.9749\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4745 - accuracy: 0.9884 - val_loss: 1.4864 - val_accuracy: 0.9763\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4727 - accuracy: 0.9898 - val_loss: 1.4867 - val_accuracy: 0.9750\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.9904 - val_loss: 1.4852 - val_accuracy: 0.9763\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4710 - accuracy: 0.9915 - val_loss: 1.4846 - val_accuracy: 0.9775\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4701 - accuracy: 0.9919 - val_loss: 1.4835 - val_accuracy: 0.9789\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9916 - val_loss: 1.4847 - val_accuracy: 0.9775\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4689 - accuracy: 0.9930 - val_loss: 1.4861 - val_accuracy: 0.9750\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4689 - accuracy: 0.9930 - val_loss: 1.4862 - val_accuracy: 0.9745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [05:23, 178.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 3\n",
      "rows to prune in layer 0 : 588\n",
      "[  0   1   2   3   7   8  13  19  20  21  27  29  32  33  37  39  41  42\n",
      "  44  48  50  53  54  60  61  63  65  69  70  71  72  74  75  78  79  80\n",
      "  82  83  89  95  96  97  98  99 102 103 104 106 107 109 111 113 116 117\n",
      " 118 119 122 123 124 126 128 134 137 138 142 146 149 150 151 152 153 157\n",
      " 158 159 160 162 166 167 169 170 171 172 177 178 179 180 187 188 192 195\n",
      " 198 205 206 212 215 216 222 223 224 228 229 230 231 232 233 235 238 239\n",
      " 240 241 243 249 253 255 260 261 262 264 266 267 268 271 272 274 276 278\n",
      " 279 281 283 284 291 293 294 296 297 298 300 301 302 303 306 307 309 310\n",
      " 312 313 316 317 318 319 320 324 325 326 327 329 330 331 333 335 337 338\n",
      " 339 340 341 345 347 349 352 358 361 363 367 368 369 370 373 374 376 378\n",
      " 379 381 382 383 384 388 392 393 396 399 400 401 402 403 404 405 407 409\n",
      " 410 411 412 413 416 418 423 425 426 428 430 432 434 436 440 441 442 444\n",
      " 447 454 458 461 464 465 467 471 473 475 476 477 478 480 481 482 485 486\n",
      " 487 491 492 494 496 497 499 503 506 507 509 511 512 513 515 517 518 519\n",
      " 521 522 527 528 532 533 534 539 541 544 549 552 554 555 558 559 562 563\n",
      " 564 565 568 569 570 571 572 575 576 577 578 579 580 582 583 585 586 587\n",
      " 588 589 590 594 595 597 598 601 603 604 606 608 612 613 614 615 616 618\n",
      " 620 625 626 628 630 632 633 635 636 637 639 647 649 651 652 653 654 655\n",
      " 657 658 659 662 663 664 666 667 674 675 677 679 680 681 682 683 685 688\n",
      " 690 691 692 695 696 698 699 700 702 705 707 708 709 710 712 716 720 722\n",
      " 723 726 734 735 736 738 741 743 744 745 747 751 753 754 755 756 757 758\n",
      " 761 763 764 765 767 769 770 771 772 773 776 778 779 780]\n",
      "[487 565 499 541  98  63 632 486 428  72 757 571  95 430 170 404 319 603\n",
      " 158 291  60 699 260 751 119 180 116 478 111 149 229 123 497  78 416 393\n",
      "  97 736 700 142 284 512 329  61 688 425 177 264 639 658 583 249 301 157\n",
      " 222 528 515 373 179  79 195   2 588 167 138 340 494  70 276 604 440 297\n",
      " 307   3 770 126 635 172  89 503 442 160 279 513 511 585 162 763 418 361\n",
      " 152 262 238 679 388 663 382  29 772 368 283 586 577 517 370 628 569 399\n",
      " 384 341 271 564 403 589 102 608 228 159 769  39 681 300 224 171 651 432\n",
      " 325 649   1  69 306 345  53 578 597 253   8  37 423 667 243 626 324 509\n",
      " 677 570 598 532 664  13 587 151 662 150 405 310 576 683 636 240 552 606\n",
      " 734 580 754 231 331 761 426 337 410 178 434 572 539 692 198 747  44 659\n",
      " 134 685  65 333 705 454 691 637 235 241 745 675 696 411 232 615 620 590\n",
      " 137 122 518 558 312 716 614  42 407 582 293 709 647 522 103 579 595 239\n",
      " 527  54 698 492 568 339 496 657 379 773 743   0 680 612 302 554 392 267\n",
      " 205 118 223  41  71 109 266  83 473 482 338 278 755 756 544 741 381 780\n",
      " 654   7 555 710 374 559 296 281 153 396 467 480  96 507 738 117 233 708\n",
      " 712 146 618 744 765  48  82 534 461 464  21 363 779 506 330 563  50 436\n",
      " 274 458 376 255 655 767 188  99 367 485 187 652 722 268 349 444 402 316\n",
      " 519 575 327  74 594  33 471 272 313 562 771 326  27 447 369 412 476 633\n",
      " 400 128 212 113 320 335 352 413 409 318 613 702 294 166  19 720 707 106\n",
      "  75 192 298 625 481 601  80 383 475 378 695 347 491 107  20 261 303 723\n",
      " 169 758 104 630 666 616 465 533 441 401 206 674 317 764 358 477 549 653\n",
      " 735 753 521 230 778 309 726 124 216  32 690 776 215 682]\n",
      "rows to prune in layer 3 : 225\n",
      "[  0   5   7   9  10  18  20  21  22  23  24  28  31  32  35  36  37  38\n",
      "  39  40  43  46  48  51  52  54  58  62  63  64  65  66  67  68  69  70\n",
      "  71  73  76  78  80  81  85  88  90  91  96  97  98 100 101 102 103 107\n",
      " 109 110 116 117 121 122 123 124 125 128 129 130 132 134 136 137 140 141\n",
      " 142 144 145 149 150 152 155 157 160 161 163 164 166 168 170 174 175 176\n",
      " 177 178 180 186 189 198 199 200 201 202 205 207 208 211 214 216 217 218\n",
      " 219 224 226 228 230 233 235 236 238 239 241 242 243 246 247 250 251 253\n",
      " 254 255 258 260 261 262 265 267 269 274 280 282 283 284 285 287 288 289\n",
      " 290 291 294 295 296 298]\n",
      "[122  88 255 265 228  32  40  81  91 295 100  46 101  85 283 251 238  10\n",
      "   0 178 207 201 243 136 152  36 102 109  73 284   5 161 166 129 261 246\n",
      " 258  98 294 134 130 155 145  51 214 199 226 285 177 175 235 280 250 128\n",
      "  64 224  63  54 150 141 287 125 290 208  68  43  52 247  97 254 217  38\n",
      " 260  18 289 107  76  20 142 160 239 121 137 274 219  66 236 262  22 241\n",
      " 218  62 216  67 124  69  35 149 174  28 116 202 168 242 298  71 230 282\n",
      " 186 132 233 198  78  39   9  90  96  24  58 291 163 110  48 269 176 123\n",
      "  70 189 164 211  80 170  21 200  37 180 205  23 157 117 140  65 103 267\n",
      " 288  31 296 253   7 144]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 0  2  4  6  7 10 11 13 15 18 19 21 22 24 31 34 36 40 41 42 47 48 50 51\n",
      " 53 56 57 58 59 60 65 67 69 70 71 72 73 74 76 77 80 81 83 85 87 89 91 93\n",
      " 98 99]\n",
      "[ 4 73 13 50 56 48 15 91  0  7 11 60 18  6 93 51 89 47 53 40 57 80 31 42\n",
      " 69  2 34 87 22 58 81 24 98 65 76 10 74 85 77 72 41 67 36 21 70 99 59 19\n",
      " 71 83]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5574 - accuracy: 0.9215 - val_loss: 1.5256 - val_accuracy: 0.9452\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5141 - accuracy: 0.9545 - val_loss: 1.5169 - val_accuracy: 0.9505\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5053 - accuracy: 0.9614 - val_loss: 1.5091 - val_accuracy: 0.9563\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4995 - accuracy: 0.9664 - val_loss: 1.5091 - val_accuracy: 0.9546\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4959 - accuracy: 0.9692 - val_loss: 1.5063 - val_accuracy: 0.9588\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4930 - accuracy: 0.9720 - val_loss: 1.5041 - val_accuracy: 0.9591\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4907 - accuracy: 0.9734 - val_loss: 1.5028 - val_accuracy: 0.9597\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4889 - accuracy: 0.9752 - val_loss: 1.5020 - val_accuracy: 0.9608\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4875 - accuracy: 0.9767 - val_loss: 1.5020 - val_accuracy: 0.9609\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4858 - accuracy: 0.9783 - val_loss: 1.5017 - val_accuracy: 0.9601\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4846 - accuracy: 0.9793 - val_loss: 1.5029 - val_accuracy: 0.9593\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9800 - val_loss: 1.5015 - val_accuracy: 0.9612\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4825 - accuracy: 0.9807 - val_loss: 1.5008 - val_accuracy: 0.9612\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4814 - accuracy: 0.9819 - val_loss: 1.5027 - val_accuracy: 0.9591\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4809 - accuracy: 0.9821 - val_loss: 1.5013 - val_accuracy: 0.9596\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4801 - accuracy: 0.9830 - val_loss: 1.5003 - val_accuracy: 0.9615\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9830 - val_loss: 1.5009 - val_accuracy: 0.9602\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4791 - accuracy: 0.9837 - val_loss: 1.5002 - val_accuracy: 0.9617\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4784 - accuracy: 0.9845 - val_loss: 1.5012 - val_accuracy: 0.9611\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4778 - accuracy: 0.9849 - val_loss: 1.5009 - val_accuracy: 0.9606\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4772 - accuracy: 0.9853 - val_loss: 1.5003 - val_accuracy: 0.9615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [07:47, 167.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 3\n",
      "rows to prune in layer 0 : 686\n",
      "[  0   7  19  20  21  27  32  33  41  42  48  50  54  71  74  75  80  82\n",
      "  83  96  99 103 104 106 107 109 113 117 118 122 124 128 137 146 153 166\n",
      " 169 187 188 192 205 206 212 215 216 223 230 233 239 255 261 266 267 268\n",
      " 272 274 278 281 293 294 296 298 302 303 309 312 313 316 317 318 320 326\n",
      " 327 330 335 338 339 347 349 352 358 363 367 369 374 376 378 379 381 383\n",
      " 392 396 400 401 402 407 409 412 413 436 441 444 447 458 461 464 465 467\n",
      " 471 473 475 476 477 480 481 482 485 491 492 496 506 507 518 519 521 522\n",
      " 527 533 534 544 549 554 555 558 559 562 563 568 575 579 582 590 594 595\n",
      " 601 612 613 614 616 618 620 625 630 633 647 652 653 654 655 657 666 674\n",
      " 680 682 690 695 698 702 707 708 709 710 712 716 720 722 723 726 735 738\n",
      " 741 743 744 753 755 756 758 764 765 767 771 773 776 778 779 780]\n",
      "[206 764 612 709   0 339 338 690 755 758 630 779 267 335 266 726 413 402\n",
      " 698 616 212 767 223 614  71 106 680 744 464 780 544 710  48 187  41  27\n",
      " 613 555 230 482 491   7 352 447 103 309 496 507 702 274 475 666 407 107\n",
      " 765 471 723  54 533 480 188 378 741  32 562 522 549 347 376 349  19 476\n",
      " 492 554 590  42 558  82 192 278 461 625 653 298 215 518 392 316  75 559\n",
      " 778 117 169 205 712 318 753 113  20 722 579 618 109 358 568 458  80 137\n",
      " 302 655 682 381 674 281 647 320 396  96 735 633 595 216 272 436 485 467\n",
      " 313 776 657 601 707 473 327 363 708 412 118 296 481 654 367 128  50  33\n",
      " 575 303 409 261 293  21 326 444 652 534 756 166 104 506 563 401 233 582\n",
      " 521 695 317 294 374 369 383 268 771 519 124 400 239 620 312 477  99 146\n",
      " 773 441 527 743 720 255  74 738 330 716 122 465 594 153 379  83]\n",
      "rows to prune in layer 3 : 262\n",
      "[  7   9  20  21  22  23  24  28  31  35  37  39  48  58  62  65  66  67\n",
      "  69  70  71  76  78  80  90  96 103 107 110 116 117 121 123 124 132 137\n",
      " 140 142 144 149 157 160 163 164 168 170 174 176 180 186 189 198 200 202\n",
      " 205 211 216 218 219 230 233 236 239 241 242 253 262 267 269 274 282 288\n",
      " 291 296 298]\n",
      "[ 67 164 107 211 202 123 137 241 124 198 274 110  22  78  62 288   9 149\n",
      " 117   7  90  76 176 170 189  31 216  35 168  39 236 103 142 157 269  65\n",
      " 174 180 121 163 253 298  28 186 239 144  37 282 160  21  70  24  66 296\n",
      " 132  80 230 116 262  58  23 218  48 291 233  20 219 140 200 242  96  69\n",
      " 205  71 267]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 2 10 19 21 22 24 34 36 41 58 59 65 67 70 71 72 74 76 77 81 83 85 87 98\n",
      " 99]\n",
      "[71 24 41 34 58 76 74 22 10 21 83 87 65 77 85 72 67 81 59 98  2 36 19 70\n",
      " 99]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6705 - accuracy: 0.8144 - val_loss: 1.5950 - val_accuracy: 0.8780\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5807 - accuracy: 0.8924 - val_loss: 1.5713 - val_accuracy: 0.8982\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5647 - accuracy: 0.9061 - val_loss: 1.5621 - val_accuracy: 0.9061\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5565 - accuracy: 0.9122 - val_loss: 1.5563 - val_accuracy: 0.9104\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5509 - accuracy: 0.9169 - val_loss: 1.5513 - val_accuracy: 0.9150\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5473 - accuracy: 0.9200 - val_loss: 1.5503 - val_accuracy: 0.9158\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5442 - accuracy: 0.9228 - val_loss: 1.5476 - val_accuracy: 0.9177\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5415 - accuracy: 0.9253 - val_loss: 1.5476 - val_accuracy: 0.9173\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5395 - accuracy: 0.9268 - val_loss: 1.5459 - val_accuracy: 0.9186\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5374 - accuracy: 0.9282 - val_loss: 1.5433 - val_accuracy: 0.9197\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5359 - accuracy: 0.9298 - val_loss: 1.5424 - val_accuracy: 0.9222\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5344 - accuracy: 0.9313 - val_loss: 1.5425 - val_accuracy: 0.9208\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5328 - accuracy: 0.9330 - val_loss: 1.5397 - val_accuracy: 0.9237\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5317 - accuracy: 0.9338 - val_loss: 1.5403 - val_accuracy: 0.9238\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5308 - accuracy: 0.9345 - val_loss: 1.5394 - val_accuracy: 0.9241\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5296 - accuracy: 0.9359 - val_loss: 1.5386 - val_accuracy: 0.9251\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5286 - accuracy: 0.9368 - val_loss: 1.5373 - val_accuracy: 0.9261\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5280 - accuracy: 0.9369 - val_loss: 1.5373 - val_accuracy: 0.9269\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5268 - accuracy: 0.9382 - val_loss: 1.5382 - val_accuracy: 0.9245\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5260 - accuracy: 0.9391 - val_loss: 1.5381 - val_accuracy: 0.9249\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5252 - accuracy: 0.9397 - val_loss: 1.5364 - val_accuracy: 0.9263\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5249 - accuracy: 0.9396 - val_loss: 1.5370 - val_accuracy: 0.9263\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5239 - accuracy: 0.9405 - val_loss: 1.5372 - val_accuracy: 0.9260\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5235 - accuracy: 0.9409 - val_loss: 1.5364 - val_accuracy: 0.9269\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5231 - accuracy: 0.9413 - val_loss: 1.5367 - val_accuracy: 0.9266\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5225 - accuracy: 0.9413 - val_loss: 1.5348 - val_accuracy: 0.9278\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5220 - accuracy: 0.9423 - val_loss: 1.5357 - val_accuracy: 0.9268\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5213 - accuracy: 0.9427 - val_loss: 1.5345 - val_accuracy: 0.9279\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5205 - accuracy: 0.9436 - val_loss: 1.5352 - val_accuracy: 0.9278\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5205 - accuracy: 0.9434 - val_loss: 1.5358 - val_accuracy: 0.9261\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5201 - accuracy: 0.9438 - val_loss: 1.5357 - val_accuracy: 0.9262\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [10:34, 167.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 3\n",
      "rows to prune in layer 0 : 735\n",
      "[ 20  21  33  50  74  80  83  96  99 104 109 118 122 124 128 137 146 153\n",
      " 166 216 233 239 255 261 268 272 281 293 294 296 302 303 312 313 317 320\n",
      " 326 327 330 358 363 367 369 374 379 381 383 396 400 401 409 412 436 441\n",
      " 444 458 465 467 473 477 481 485 506 519 521 527 534 563 568 575 579 582\n",
      " 594 595 601 618 620 633 647 652 654 655 657 674 682 695 707 708 716 720\n",
      " 722 735 738 743 756 771 773 776]\n",
      "[369 216 707 473 506 367 296 534 146 647 743 381  33 137 294  96 401 708\n",
      " 358 383 477 153 563  20 595 104 652 293 313 756 485 122 601 396 467 303\n",
      " 582 735 409 412 465 722 255 657 716 312 166 261 302  50 527 327 124 109\n",
      " 594 268 773  99 374 330 579 776 633 233  80 771 317 441 738 379 654 618\n",
      "  74 239  83 326 272 720 568 363 655 320 695 519  21 400 128 575 521 682\n",
      " 620 458 118 436 674 281 444 481]\n",
      "rows to prune in layer 3 : 281\n",
      "[ 20  21  23  24  28  37  48  58  66  69  70  71  80  96 116 121 132 140\n",
      " 144 160 163 180 186 200 205 218 219 230 233 239 242 253 262 267 282 291\n",
      " 296 298]\n",
      "[282 144 262 267  71  20  66  28 205 218  37 180  69 291 219 160 163 121\n",
      "  58 253 296  48 186 230 242 140  70 116 233 200 132 298  24  96  80 239\n",
      "  23  21]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 2 19 36 59 65 67 70 72 77 81 85 98 99]\n",
      "[36 70 67 85 19  2 59 77 65 72 99 81 98]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9025 - accuracy: 0.5724 - val_loss: 1.7743 - val_accuracy: 0.6994\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7336 - accuracy: 0.7425 - val_loss: 1.7032 - val_accuracy: 0.7683\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6891 - accuracy: 0.7843 - val_loss: 1.6802 - val_accuracy: 0.7890\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6725 - accuracy: 0.7997 - val_loss: 1.6679 - val_accuracy: 0.8018\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6629 - accuracy: 0.8067 - val_loss: 1.6621 - val_accuracy: 0.8045\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6562 - accuracy: 0.8130 - val_loss: 1.6546 - val_accuracy: 0.8163\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6511 - accuracy: 0.8172 - val_loss: 1.6508 - val_accuracy: 0.8185\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6470 - accuracy: 0.8213 - val_loss: 1.6472 - val_accuracy: 0.8203\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6436 - accuracy: 0.8244 - val_loss: 1.6441 - val_accuracy: 0.8233\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6408 - accuracy: 0.8270 - val_loss: 1.6412 - val_accuracy: 0.8254\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6384 - accuracy: 0.8287 - val_loss: 1.6400 - val_accuracy: 0.8260\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6359 - accuracy: 0.8308 - val_loss: 1.6391 - val_accuracy: 0.8265\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6344 - accuracy: 0.8327 - val_loss: 1.6375 - val_accuracy: 0.8266\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6324 - accuracy: 0.8348 - val_loss: 1.6351 - val_accuracy: 0.8307\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6311 - accuracy: 0.8355 - val_loss: 1.6353 - val_accuracy: 0.8296\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6297 - accuracy: 0.8366 - val_loss: 1.6335 - val_accuracy: 0.8302\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6286 - accuracy: 0.8378 - val_loss: 1.6340 - val_accuracy: 0.8286\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6274 - accuracy: 0.8389 - val_loss: 1.6311 - val_accuracy: 0.8342\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6263 - accuracy: 0.8401 - val_loss: 1.6314 - val_accuracy: 0.8328\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6255 - accuracy: 0.8403 - val_loss: 1.6297 - val_accuracy: 0.8339\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6247 - accuracy: 0.8413 - val_loss: 1.6289 - val_accuracy: 0.8348\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6237 - accuracy: 0.8423 - val_loss: 1.6289 - val_accuracy: 0.8335\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6227 - accuracy: 0.8427 - val_loss: 1.6275 - val_accuracy: 0.8362\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6218 - accuracy: 0.8441 - val_loss: 1.6286 - val_accuracy: 0.8344\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6214 - accuracy: 0.8440 - val_loss: 1.6264 - val_accuracy: 0.8367\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6209 - accuracy: 0.8443 - val_loss: 1.6277 - val_accuracy: 0.8346\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6205 - accuracy: 0.8449 - val_loss: 1.6261 - val_accuracy: 0.8361\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6195 - accuracy: 0.8456 - val_loss: 1.6282 - val_accuracy: 0.8357\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6190 - accuracy: 0.8461 - val_loss: 1.6253 - val_accuracy: 0.8364\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6184 - accuracy: 0.8471 - val_loss: 1.6262 - val_accuracy: 0.8369\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6180 - accuracy: 0.8466 - val_loss: 1.6261 - val_accuracy: 0.8366\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6172 - accuracy: 0.8477 - val_loss: 1.6245 - val_accuracy: 0.8380\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6165 - accuracy: 0.8487 - val_loss: 1.6238 - val_accuracy: 0.8387\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6161 - accuracy: 0.8489 - val_loss: 1.6243 - val_accuracy: 0.8367\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6152 - accuracy: 0.8494 - val_loss: 1.6234 - val_accuracy: 0.8392\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6149 - accuracy: 0.8500 - val_loss: 1.6224 - val_accuracy: 0.8403\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6142 - accuracy: 0.8506 - val_loss: 1.6240 - val_accuracy: 0.8374\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6139 - accuracy: 0.8507 - val_loss: 1.6230 - val_accuracy: 0.8391\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6131 - accuracy: 0.8515 - val_loss: 1.6224 - val_accuracy: 0.8390\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [13:53, 177.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 3\n",
      "rows to prune in layer 0 : 759\n",
      "[ 21  50  74  80  83  99 109 118 124 128 233 239 268 272 281 317 320 326\n",
      " 327 330 363 374 379 400 436 441 444 458 481 519 521 527 568 575 579 594\n",
      " 618 620 633 654 655 674 682 695 720 738 771 773 776]\n",
      "[400 281 272 655 268 776 118 738 594  83 327 695 481 320 620 444 109  74\n",
      " 124 128 519 720 618 575 521 771 317 239 379 363 527 654 633 330  99  80\n",
      " 436 568 326 233 773  21  50 682 458 579 374 674 441]\n",
      "rows to prune in layer 3 : 290\n",
      "[ 21  23  24  48  70  80  96 116 132 140 186 200 230 233 239 242 253 296\n",
      " 298]\n",
      "[253  24 298 132 233 296 186  21 140 230  48 242 239 200  80  96  23 116\n",
      "  70]\n",
      "rows to prune in layer 6 : 96\n",
      "[59 65 72 77 81 98 99]\n",
      "[65 99 98 77 81 59 72]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0995 - accuracy: 0.3667 - val_loss: 2.0087 - val_accuracy: 0.4636\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9802 - accuracy: 0.4912 - val_loss: 1.9554 - val_accuracy: 0.5123\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9430 - accuracy: 0.5247 - val_loss: 1.9307 - val_accuracy: 0.5344\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9264 - accuracy: 0.5397 - val_loss: 1.9210 - val_accuracy: 0.5437\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9182 - accuracy: 0.5463 - val_loss: 1.9148 - val_accuracy: 0.5494\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9130 - accuracy: 0.5512 - val_loss: 1.9110 - val_accuracy: 0.5534\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9098 - accuracy: 0.5544 - val_loss: 1.9085 - val_accuracy: 0.5538\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9073 - accuracy: 0.5558 - val_loss: 1.9079 - val_accuracy: 0.5539\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9052 - accuracy: 0.5570 - val_loss: 1.9048 - val_accuracy: 0.5571\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9033 - accuracy: 0.5596 - val_loss: 1.9030 - val_accuracy: 0.5579\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9018 - accuracy: 0.5605 - val_loss: 1.9020 - val_accuracy: 0.5589\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9005 - accuracy: 0.5613 - val_loss: 1.9011 - val_accuracy: 0.5602\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8995 - accuracy: 0.5625 - val_loss: 1.8992 - val_accuracy: 0.5617\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8984 - accuracy: 0.5632 - val_loss: 1.8989 - val_accuracy: 0.5622\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8975 - accuracy: 0.5644 - val_loss: 1.8988 - val_accuracy: 0.5605\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8966 - accuracy: 0.5652 - val_loss: 1.8974 - val_accuracy: 0.5643\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8958 - accuracy: 0.5662 - val_loss: 1.8970 - val_accuracy: 0.5624\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8949 - accuracy: 0.5672 - val_loss: 1.8960 - val_accuracy: 0.5635\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8941 - accuracy: 0.5680 - val_loss: 1.8957 - val_accuracy: 0.5646\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8934 - accuracy: 0.5686 - val_loss: 1.8945 - val_accuracy: 0.5654\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8928 - accuracy: 0.5687 - val_loss: 1.8934 - val_accuracy: 0.5675\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8919 - accuracy: 0.5699 - val_loss: 1.8929 - val_accuracy: 0.5667\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8911 - accuracy: 0.5707 - val_loss: 1.8924 - val_accuracy: 0.5677\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8905 - accuracy: 0.5706 - val_loss: 1.8924 - val_accuracy: 0.5681\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8899 - accuracy: 0.5720 - val_loss: 1.8913 - val_accuracy: 0.5679\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8891 - accuracy: 0.5724 - val_loss: 1.8905 - val_accuracy: 0.5693\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8887 - accuracy: 0.5729 - val_loss: 1.8900 - val_accuracy: 0.5710\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8883 - accuracy: 0.5735 - val_loss: 1.8896 - val_accuracy: 0.5716\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8876 - accuracy: 0.5740 - val_loss: 1.8887 - val_accuracy: 0.5713\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8870 - accuracy: 0.5751 - val_loss: 1.8879 - val_accuracy: 0.5720\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8862 - accuracy: 0.5754 - val_loss: 1.8877 - val_accuracy: 0.5728\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8859 - accuracy: 0.5758 - val_loss: 1.8878 - val_accuracy: 0.5712\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8854 - accuracy: 0.5764 - val_loss: 1.8869 - val_accuracy: 0.5731\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8849 - accuracy: 0.5769 - val_loss: 1.8862 - val_accuracy: 0.5735\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8847 - accuracy: 0.5771 - val_loss: 1.8853 - val_accuracy: 0.5743\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8844 - accuracy: 0.5771 - val_loss: 1.8859 - val_accuracy: 0.5756\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8838 - accuracy: 0.5779 - val_loss: 1.8851 - val_accuracy: 0.5737\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8836 - accuracy: 0.5779 - val_loss: 1.8846 - val_accuracy: 0.5763\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8834 - accuracy: 0.5780 - val_loss: 1.8842 - val_accuracy: 0.5774\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8830 - accuracy: 0.5784 - val_loss: 1.8843 - val_accuracy: 0.5761\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8828 - accuracy: 0.5787 - val_loss: 1.8833 - val_accuracy: 0.5773\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8826 - accuracy: 0.5786 - val_loss: 1.8832 - val_accuracy: 0.5769\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8822 - accuracy: 0.5789 - val_loss: 1.8829 - val_accuracy: 0.5762\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8822 - accuracy: 0.5789 - val_loss: 1.8829 - val_accuracy: 0.5778\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8818 - accuracy: 0.5794 - val_loss: 1.8832 - val_accuracy: 0.5769\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8816 - accuracy: 0.5793 - val_loss: 1.8828 - val_accuracy: 0.5771\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8815 - accuracy: 0.5794 - val_loss: 1.8823 - val_accuracy: 0.5765\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8813 - accuracy: 0.5800 - val_loss: 1.8822 - val_accuracy: 0.5772\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8811 - accuracy: 0.5799 - val_loss: 1.8834 - val_accuracy: 0.5773\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8809 - accuracy: 0.5801 - val_loss: 1.8827 - val_accuracy: 0.5767\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8806 - accuracy: 0.5803 - val_loss: 1.8820 - val_accuracy: 0.5792\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8806 - accuracy: 0.5804 - val_loss: 1.8823 - val_accuracy: 0.5776\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8804 - accuracy: 0.5803 - val_loss: 1.8816 - val_accuracy: 0.5781\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8802 - accuracy: 0.5808 - val_loss: 1.8826 - val_accuracy: 0.5782\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8800 - accuracy: 0.5814 - val_loss: 1.8825 - val_accuracy: 0.5765\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8802 - accuracy: 0.5806 - val_loss: 1.8817 - val_accuracy: 0.5776\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [17:28, 188.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 3\n",
      "rows to prune in layer 0 : 771\n",
      "[ 21  50  80  99 233 239 317 326 330 363 374 379 436 441 458 521 527 568\n",
      " 579 633 654 674 682 771 773]\n",
      "[374 521 330  21 682 568 773 379 317 579 326 654 363 633  50 527 458 441\n",
      "  99  80 239 436 674 771 233]\n",
      "rows to prune in layer 3 : 295\n",
      "[ 23  48  70  80  96 116 200 230 239 242]\n",
      "[230  96  80 239  70  48  23 116 200 242]\n",
      "rows to prune in layer 6 : 98\n",
      "[59 72 77 81]\n",
      "[77 59 81 72]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2122 - accuracy: 0.2459 - val_loss: 2.1977 - val_accuracy: 0.2580\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1980 - accuracy: 0.2555 - val_loss: 2.1905 - val_accuracy: 0.2630\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1921 - accuracy: 0.2584 - val_loss: 2.1850 - val_accuracy: 0.2641\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1667 - accuracy: 0.2792 - val_loss: 2.1459 - val_accuracy: 0.3049\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1472 - accuracy: 0.3054 - val_loss: 2.1374 - val_accuracy: 0.3168\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1409 - accuracy: 0.3112 - val_loss: 2.1332 - val_accuracy: 0.3207\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1373 - accuracy: 0.3142 - val_loss: 2.1300 - val_accuracy: 0.3228\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1344 - accuracy: 0.3168 - val_loss: 2.1270 - val_accuracy: 0.3309\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1318 - accuracy: 0.3259 - val_loss: 2.1245 - val_accuracy: 0.3382\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1297 - accuracy: 0.3305 - val_loss: 2.1225 - val_accuracy: 0.3376\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1277 - accuracy: 0.3321 - val_loss: 2.1205 - val_accuracy: 0.3402\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1255 - accuracy: 0.3348 - val_loss: 2.1176 - val_accuracy: 0.3430\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1232 - accuracy: 0.3361 - val_loss: 2.1156 - val_accuracy: 0.3455\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1213 - accuracy: 0.3379 - val_loss: 2.1146 - val_accuracy: 0.3451\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1197 - accuracy: 0.3390 - val_loss: 2.1129 - val_accuracy: 0.3455\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1186 - accuracy: 0.3397 - val_loss: 2.1117 - val_accuracy: 0.3473\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1177 - accuracy: 0.3407 - val_loss: 2.1106 - val_accuracy: 0.3485\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1168 - accuracy: 0.3410 - val_loss: 2.1097 - val_accuracy: 0.3506\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1162 - accuracy: 0.3407 - val_loss: 2.1092 - val_accuracy: 0.3491\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1156 - accuracy: 0.3418 - val_loss: 2.1085 - val_accuracy: 0.3484\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1150 - accuracy: 0.3426 - val_loss: 2.1075 - val_accuracy: 0.3512\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1146 - accuracy: 0.3431 - val_loss: 2.1076 - val_accuracy: 0.3487\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1141 - accuracy: 0.3432 - val_loss: 2.1069 - val_accuracy: 0.3525\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1137 - accuracy: 0.3435 - val_loss: 2.1062 - val_accuracy: 0.3506\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1134 - accuracy: 0.3433 - val_loss: 2.1058 - val_accuracy: 0.3508\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1129 - accuracy: 0.3447 - val_loss: 2.1058 - val_accuracy: 0.3516\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1127 - accuracy: 0.3446 - val_loss: 2.1059 - val_accuracy: 0.3490\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1123 - accuracy: 0.3447 - val_loss: 2.1049 - val_accuracy: 0.3517\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1120 - accuracy: 0.3449 - val_loss: 2.1051 - val_accuracy: 0.3519\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1117 - accuracy: 0.3451 - val_loss: 2.1046 - val_accuracy: 0.3539\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1115 - accuracy: 0.3453 - val_loss: 2.1048 - val_accuracy: 0.3514\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1113 - accuracy: 0.3451 - val_loss: 2.1036 - val_accuracy: 0.3533\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1110 - accuracy: 0.3452 - val_loss: 2.1041 - val_accuracy: 0.3515\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1107 - accuracy: 0.3458 - val_loss: 2.1033 - val_accuracy: 0.3555\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1106 - accuracy: 0.3459 - val_loss: 2.1035 - val_accuracy: 0.3560\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1103 - accuracy: 0.3451 - val_loss: 2.1030 - val_accuracy: 0.3527\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1100 - accuracy: 0.3459 - val_loss: 2.1023 - val_accuracy: 0.3536\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1097 - accuracy: 0.3460 - val_loss: 2.1018 - val_accuracy: 0.3553\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1094 - accuracy: 0.3460 - val_loss: 2.1026 - val_accuracy: 0.3528\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1091 - accuracy: 0.3470 - val_loss: 2.1012 - val_accuracy: 0.3545\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1088 - accuracy: 0.3463 - val_loss: 2.1011 - val_accuracy: 0.3554\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1084 - accuracy: 0.3467 - val_loss: 2.1009 - val_accuracy: 0.3537\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1081 - accuracy: 0.3464 - val_loss: 2.0999 - val_accuracy: 0.3557\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1078 - accuracy: 0.3467 - val_loss: 2.0999 - val_accuracy: 0.3571\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1075 - accuracy: 0.3466 - val_loss: 2.0993 - val_accuracy: 0.3564\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1072 - accuracy: 0.3470 - val_loss: 2.0990 - val_accuracy: 0.3563\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1068 - accuracy: 0.3474 - val_loss: 2.0987 - val_accuracy: 0.3538\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1065 - accuracy: 0.3480 - val_loss: 2.0982 - val_accuracy: 0.3551\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1061 - accuracy: 0.3490 - val_loss: 2.0977 - val_accuracy: 0.3572\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1058 - accuracy: 0.3489 - val_loss: 2.0979 - val_accuracy: 0.3576\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1057 - accuracy: 0.3497 - val_loss: 2.0970 - val_accuracy: 0.3594\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1053 - accuracy: 0.3498 - val_loss: 2.0974 - val_accuracy: 0.3569\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1051 - accuracy: 0.3505 - val_loss: 2.0964 - val_accuracy: 0.3613\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1048 - accuracy: 0.3508 - val_loss: 2.0959 - val_accuracy: 0.3634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1046 - accuracy: 0.3519 - val_loss: 2.0961 - val_accuracy: 0.3623\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1043 - accuracy: 0.3521 - val_loss: 2.0957 - val_accuracy: 0.3602\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1040 - accuracy: 0.3531 - val_loss: 2.0958 - val_accuracy: 0.3598\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1039 - accuracy: 0.3528 - val_loss: 2.0957 - val_accuracy: 0.3605\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1036 - accuracy: 0.3532 - val_loss: 2.0957 - val_accuracy: 0.3593\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1035 - accuracy: 0.3538 - val_loss: 2.0952 - val_accuracy: 0.3613\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1033 - accuracy: 0.3543 - val_loss: 2.0953 - val_accuracy: 0.3617\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1033 - accuracy: 0.3542 - val_loss: 2.0948 - val_accuracy: 0.3631\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1029 - accuracy: 0.3548 - val_loss: 2.0949 - val_accuracy: 0.3620\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1029 - accuracy: 0.3547 - val_loss: 2.0948 - val_accuracy: 0.3591\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1028 - accuracy: 0.3546 - val_loss: 2.0944 - val_accuracy: 0.3615\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1025 - accuracy: 0.3550 - val_loss: 2.0956 - val_accuracy: 0.3576\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1025 - accuracy: 0.3550 - val_loss: 2.0943 - val_accuracy: 0.3630\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1024 - accuracy: 0.3550 - val_loss: 2.0942 - val_accuracy: 0.3623\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1022 - accuracy: 0.3548 - val_loss: 2.0937 - val_accuracy: 0.3626\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1019 - accuracy: 0.3554 - val_loss: 2.0934 - val_accuracy: 0.3643\n",
      "Epoch 71/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1020 - accuracy: 0.3556 - val_loss: 2.0933 - val_accuracy: 0.3649\n",
      "Epoch 72/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1019 - accuracy: 0.3555 - val_loss: 2.0934 - val_accuracy: 0.3635\n",
      "Epoch 73/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1018 - accuracy: 0.3559 - val_loss: 2.0929 - val_accuracy: 0.3652\n",
      "Epoch 74/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1015 - accuracy: 0.3565 - val_loss: 2.0931 - val_accuracy: 0.3640\n",
      "Epoch 75/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1015 - accuracy: 0.3564 - val_loss: 2.0933 - val_accuracy: 0.3637\n",
      "Epoch 76/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1015 - accuracy: 0.3564 - val_loss: 2.0928 - val_accuracy: 0.3642\n",
      "Epoch 77/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1013 - accuracy: 0.3564 - val_loss: 2.0932 - val_accuracy: 0.3630\n",
      "Epoch 78/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1012 - accuracy: 0.3563 - val_loss: 2.0926 - val_accuracy: 0.3673\n",
      "Epoch 79/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1012 - accuracy: 0.3564 - val_loss: 2.0927 - val_accuracy: 0.3656\n",
      "Epoch 80/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1011 - accuracy: 0.3560 - val_loss: 2.0925 - val_accuracy: 0.3654\n",
      "Epoch 81/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1011 - accuracy: 0.3566 - val_loss: 2.0926 - val_accuracy: 0.3638\n",
      "Epoch 82/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1009 - accuracy: 0.3568 - val_loss: 2.0923 - val_accuracy: 0.3667\n",
      "Epoch 83/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1008 - accuracy: 0.3566 - val_loss: 2.0923 - val_accuracy: 0.3649\n",
      "Epoch 84/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1007 - accuracy: 0.3573 - val_loss: 2.0928 - val_accuracy: 0.3650\n",
      "Epoch 85/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1007 - accuracy: 0.3571 - val_loss: 2.0921 - val_accuracy: 0.3663\n",
      "Epoch 86/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1007 - accuracy: 0.3576 - val_loss: 2.0922 - val_accuracy: 0.3679\n",
      "Epoch 87/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1006 - accuracy: 0.3575 - val_loss: 2.0918 - val_accuracy: 0.3672\n",
      "Epoch 88/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1005 - accuracy: 0.3569 - val_loss: 2.0919 - val_accuracy: 0.3651\n",
      "Epoch 89/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1005 - accuracy: 0.3571 - val_loss: 2.0930 - val_accuracy: 0.3629\n",
      "Epoch 90/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1004 - accuracy: 0.3576 - val_loss: 2.0917 - val_accuracy: 0.3660\n",
      "Epoch 91/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1002 - accuracy: 0.3580 - val_loss: 2.0916 - val_accuracy: 0.3681\n",
      "Epoch 92/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1001 - accuracy: 0.3578 - val_loss: 2.0924 - val_accuracy: 0.3641\n",
      "Epoch 93/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1001 - accuracy: 0.3578 - val_loss: 2.0917 - val_accuracy: 0.3659\n",
      "Epoch 94/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1001 - accuracy: 0.3579 - val_loss: 2.0937 - val_accuracy: 0.3626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [21:35, 185.10s/it]\u001b[A\n",
      " 40%|████      | 4/10 [1:16:08<1:55:08, 1151.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5748 - accuracy: 0.8990 - val_loss: 1.5228 - val_accuracy: 0.9411\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 4\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[264 417  36 458   6 258 775  48  77 327 756 257 733 624 406 673 674 362\n",
      " 165 502 411 431  13  80 556 188 307 682 429 139 160 629 612 728 508 122\n",
      "  31 535 575 441 692 150 748 617  94 246 419 316 301 738 667  76 181 457\n",
      " 338 594 453 132 224 559   8 322 750 308 323 100 140 198 152 376 111 526\n",
      " 393 286 636 201 248 163  27 432 633 511 387 524 600 774 533 739 690 669\n",
      "  15 206  64 492 154  23 207 133 473  58 769 409 745 415 352 423 129 702\n",
      " 606 155 685 675 223 668 309  51 596 213 765 330 240  45 466 193 389 211\n",
      " 452 586 277 168 147 757 663 768 577 481 192 268 709 405 339 520 580 176\n",
      " 621 547 552  38 529 416 555 486 474 752 199 724 640  62 380 347 245  54\n",
      " 506 306 497 696 186 723 110 249 605 394  65 537  52 468  24 616 311 732\n",
      " 104 190 127 117 410 588 135 252 671 447 744 476 469   4 566 558 119 434\n",
      " 705 395 341 460 619 182  37 209 465 136 350 515 571 319 329 210 444 568\n",
      " 112 191 420 628 158 639 540  46 551 170 461 574  92 276 397 433  79 336\n",
      " 521 711 542 353 782  47 483 678 759  97 641 130 676 777  35  33 438 287\n",
      " 729 217 414 712 519 118  85 357 364 656 218 156 205  42 720 404 426 463\n",
      " 477 281 231  29 755 770 113 219 771 369 742 500 503  32 576 216 767 719\n",
      " 425 495 225  93 247 280 661 131 424 534  84 174 265 305 430 103 603 578\n",
      " 573 427 743 121 620  20  30 592 312 736 565 501 279  82 169 167 214 725\n",
      " 569 664 178 665 454  59 230 713  28  81 762 303 238 278 345 764 384 164\n",
      " 436 227 317 472 491 304 443 459 527 418 658 708   1 722 647 548 525 274\n",
      " 698 403 499  71 626 334 106 735 490 128 623 766 229 173 688 648 731 275\n",
      " 550 196 400 589 650 320 488 652 375 754 609 382  10 689 314 734 760 325\n",
      "  22 651 467 315 344 514 299 220 747 614 598 585 487 686 235  63 343 707\n",
      " 518 391 373 570 549 761 361 545 753 310  55 289  88 482  98 355 399 564\n",
      " 553 634   5 716  19 226 631  68 142 485 368 451  91 644 498 302 687 381\n",
      " 528  75 513 632 221 212  73  95 608 697 313 583 610 233 371 116 530 377\n",
      " 187  83 622 706 746 243  56 751 475 659   2 680 149 254 175  25  90 326\n",
      " 615 244 517   0  17 388  40 144  57 333 646 386  12 691 643 321 448 332\n",
      " 721 349 654 496 251 288  43 666  60  61 162 295 642 700 780 177 241 379\n",
      " 684 704 421 392 356 509 126 260 581 627 657 284 141 776 587 538 478 737\n",
      " 412 662 613  34 324  72  26 714 124 239 655  53 464 358 593 455 259 772\n",
      " 645 340 710 354  96 197 262  87 618 203 115 445 171 718 749 407 134 346\n",
      " 601 390  21  67 715 638 563 635 137 557 763 660  14 493 532 695 480 105\n",
      " 285   7 462 591 579 366 328  86 270 516 232   3 161 184 539  41 172 546\n",
      " 138 401 727 335 446 372  74 590 143 778 234 109 779 145 562 758 413 428\n",
      " 561 383 180 157 267 544 693 653  16 781 398 489 437 450 255 741 283 604\n",
      " 773 269 370 200 672 471 726  44 256 783 554 195 440 494 185 602 272 505\n",
      " 296 351 101 541 536 294 298 599  89 701 297  50 630 291 449 108 681 582\n",
      " 504 290 522 189 107 470 300 510 740  11 607 363 318 677 120 730 208  99\n",
      " 584 215 153 202 717 331 572  69 442 151 699 408 484 703 637 543 367   9\n",
      " 567 236 271 512 222 123 625 683 595 694 342 114 435 531  49 523 102 166\n",
      " 159 611 125  70 385 649 670 237 679 146 194 337 183 292 378 359 293  78\n",
      " 360 242 560 374 348 365 273  39 204 179 261  66 422 253 597 282 250 439\n",
      " 228 507 402 263 148 266 456 396  18 479]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[147 235  90 116 109 142 230  49 128 127 126 229 225 202 111 161 134 146\n",
      " 213 278  37 199 219 269 275 214 223 259 118 209 107 257 184 252 165  77\n",
      " 247  68 162  29  45  84 265 138 129 220  89 286 279 217 201  28  86 114\n",
      "  38 198  48 176  47  71  55 251  76  27 249  36 272  79 255 260  53  59\n",
      "  58 297 295 170 169 121  62 246  44  87 293 200 124 157 211 276  60 294\n",
      " 152  35 131 140  57  52 194 273 266   1  88  72 182   3 290 282 103 113\n",
      " 185 241 244 263 172 168 135 174 141  81 158 237 216 163 110 143  96 281\n",
      " 280 137  56 123 148 250  25 261 187 248 181 108  32 288  80   0  11 164\n",
      " 171 277 226   4 139 207 122 287  30 215   5 283 106 167 177 130 145 228\n",
      " 156  51  69 234 120 102 233 173 262  61  40 206 151 268   7 274 195 298\n",
      " 191 243 292 270 136 253  94 101  73 210  92  97  24 296  82  54 231 221\n",
      " 208 190 224  95 192 258 218  42 189  65   2 203 166 144 245 256  10   9\n",
      "  50 222   6  39 240  93  85 150 284 119 204 100  41 183 205  21 212   8\n",
      " 125 236 193  66  22 197  91 239 196 271  78 112 132  26  63 159 232  74\n",
      " 186  15  14  83 242  43 149 291  75 179 264  20 238 115  70 267  46  31\n",
      "  64 285 133  18 104 105 254 160  16  23  13  98  33 155 178  67  12 153\n",
      " 180  34 227 299 175 154  99 117  19 289 188  17]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[35 28 25 93 12 52 56 59 82 14 80 42 63 11 95 86 66 60 64 33 30 81 71  8\n",
      " 87 68 48 26 41 72  0 70 18 79  7 36  1 61  5 57 31 94 62 39 43 38 77 13\n",
      " 98 90 65 84 24 29 19 69  4 34  3 55 99 89 83 67 17 97  6 21 40 10 58  2\n",
      " 88 96 73 76 23 50 27 51 85 91  9 32 16 78 47 49 20 46 74 22 92 75 44 37\n",
      " 53 15 45 54]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5127 - accuracy: 0.9508 - val_loss: 1.5073 - val_accuracy: 0.9549\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4994 - accuracy: 0.9632 - val_loss: 1.4982 - val_accuracy: 0.9641\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4925 - accuracy: 0.9697 - val_loss: 1.4934 - val_accuracy: 0.9687\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4877 - accuracy: 0.9744 - val_loss: 1.4967 - val_accuracy: 0.9663\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4848 - accuracy: 0.9768 - val_loss: 1.4879 - val_accuracy: 0.9735\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4809 - accuracy: 0.9810 - val_loss: 1.4885 - val_accuracy: 0.9732\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4794 - accuracy: 0.9822 - val_loss: 1.4929 - val_accuracy: 0.9687\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4780 - accuracy: 0.9837 - val_loss: 1.4883 - val_accuracy: 0.9731\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:53, 113.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 4\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[452 363 165 111 716 644 135 567 477 530 185 744 675  93 599 208 487 307\n",
      " 346  80  53 611 783 698 610 663 697 424 582  84 126 304 727 397 612 303\n",
      " 380 557 366 478 259  24 310 539 628  70 234 471 521 642 232  92 450 765\n",
      " 688  26 400 472 248 758 509 258 202 171 662 677 237 444  27 125 767 753\n",
      " 112 588   3  81 731 180 723 128 607 668  46 633  30 406 518 782 169 183\n",
      " 709  35 309 272  95 142  51 773  77 498 370 420 689 390 358 295 618 333\n",
      " 494  38 291 659   0 553 533 389  54 290 105 278 750 546  23 218 445 157\n",
      " 634 647 641 747 463 164   5 265 732 457 774 586 101 486 122 120  86 683\n",
      "  25 549 284 559 260 113  90 161 720 511  94 287 562 181 416 613 705 532\n",
      " 247 233 323 769 603 570 279 211 249 143 405 149 433 489 133 596 222 692\n",
      " 104 136 428 395 746  21 292 170 499  18 741 286 385 749 778 762 503 512\n",
      " 755 264 147 470 231 331 367 439 538 473 179 664 722 210 474 517 140 482\n",
      " 341 669 151 332 379 362 172 177 609 593 766 476 505 195 246 302 327 436\n",
      " 674 387 751 667 455 106 571  65 563 630 480 288 329 515 465 661 713 540\n",
      " 454  68 519 434  89 430 525 620 640 159 245 205 681  42 485 336 686 213\n",
      " 429 639 764 635   4 730 154 203 508 382 117 531 577  13 543 427 468 342\n",
      " 752 413 497 236 410 682 280 384 685 545 700 404 348  34  57 458  99 638\n",
      " 510 267 393 257 114 343 565  61 739  47  45 772 182  39 330 412 107 360\n",
      " 520 483 281  67 141 238 502 736 220 602 743 717  44 359 296 275 718 355\n",
      " 269 460 551 522 345 648 623 230 590 707 226 484 587 548 595 353 711 569\n",
      " 144 451 589 198 534 591   8 337 580 442 124 215 204 760 109 690 523 318\n",
      "  59 459 695 146 322  19 775 155 239 542  91 605 244 575 616  78 361 768\n",
      " 253 491 398  63 392 255 250 666 493 383 568 703  50 693 306 461 687 166\n",
      " 708 289  28 283  64  69 369  10 729 781 488 134 702 621 118 130 178 740\n",
      " 121 356 779 632  15 469 554 529  11 712 192 300 316 670 552 340 347  17\n",
      " 627 578 339 191 440 475 266 560 148 216 581 673 462 334 745 626 550 655\n",
      " 223 214 381 501 138 409 513  66 301 285  72  76 175 748 110 564 495 572\n",
      " 156 207  75  85 496 372 386 299 123 629 614 770 526 710 421 651 625 293\n",
      " 738 242 352 443 544  79  58 219  37 282 115 733 504 224 162 742 418 197\n",
      " 556 320 678 221 600 535 649 701 153 704 199 139 273 617 737 643 298 228\n",
      " 414 150 432 119 313 592 349 176 163 631 715 479 338 217 425 658 637 200\n",
      " 763 464 776 374 268 145 100 699 243 173 583 311 251 598 537 102 657 108\n",
      " 344  73  12 507 735 373 656 584 601 438 357 351 676 453 721  36 132 408\n",
      " 435 447 415 271 585   1 423 103 573 335 371 240 241  31 354 714  29   7\n",
      " 728 256 305 317 270 645 574 492 757   9 189 671 536  49 394 254 528 734\n",
      " 490 761 561 277 411 646 604  41 652 516 196 756 308 168 724 274 325 719\n",
      "  32 321 696 691   2 449 314 225 152 437 158 187 558 319 315 364 235 706\n",
      "  43 194 514  98 368 206 650 466 174 402  40 441  87 326   6 524 129 771\n",
      "  97 137 636 547  96  71 684 422 377 391 294 365  74  14  82 328 378 227\n",
      "  52 350 500 456 403 399 276 446 184  55  20 579 324  22 566 431 725 188\n",
      " 376 615 665  88 780  48  60 680 653 229 261 201 672 555 375 481 388 193\n",
      " 622  56 312 263 186 419 131 597 726 576 624 190 407 594 160 694 606 127\n",
      " 209 297 506 608 401 759  33 252  16 619  83 654 417 679 262 777 212 527\n",
      " 426 396 467 541 116 448  62 660 754 167]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[230  76 142 154 217 260  91  41 205 268 193  54 165   5  63 130 159 292\n",
      "  21  56  13 198  25 136 104 280 223 225  26 210 203 111   4 275 209 244\n",
      " 220 139   2 226 178  93 113  85 239 232 272 253 277 242 181  66  89 212\n",
      " 107 283 182 251 145 161  90 127  61 279 270  92  34 185 168 124  67  79\n",
      "  51 213 166 141  57  42 174 176 296 255 162 180 172 164 265 171 169 170\n",
      " 254 106 189 108 101 153  46 259 267 116  11  74 192  19 184 246 295 105\n",
      "  88 282 238  10  18 132 146 175  73  81 102 191 100  20 122  99 187  12\n",
      "  28  15 155 245 222   3  84 163 110  64 200   8 250 236 287 285  47  31\n",
      " 240 188 237 156 256 215 218  45 144 112 134  55 117  40 133 149  38  24\n",
      "  82 183 123  70 273 274 289 109 129 228 125   6   0   1 248 262  35  60\n",
      "  14 281 137  48  94  59 143 235 290  69 219 284 258 158  50  62 196   9\n",
      " 202  77  16  29 294  65 211 278  53  72 298 221 121 206 199  49 152 288\n",
      "  36 208 147 114  22  83  52  39 207 247  68 119  78 241 140  23 167 264\n",
      " 243 115 135 252 261  75  37 150 204  96 179 103  27 194 231 263  87 257\n",
      " 233  98 216   7 131 201 276  58 120  97 177 227  17  44  43  71 224 128\n",
      " 118 197 291 190 151 234  86  30 269 173 160 138 157 148 186 293 286  95\n",
      "  33  80 126 229 249 214 299 297 271  32 266 195]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[60 40 80 34 97 71 82  8 46 23 16 58 44 57 31 45 51 49 75 36 12 70 72  4\n",
      " 53 83 19 91 42 38 10 18 24 92 26  9 98 47 50 55 17 22 63 86 77 84 43 41\n",
      " 33  6 68 96 67 28 81 78 14 20 54 11 69 15  1 52 65 88 59 85 76 62 61 90\n",
      " 30 32 48  3  2 99 21 37 13 89 93  0 95 66 87 74 56 35 39 27 79 25  7 64\n",
      " 29  5 94 73]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5104 - accuracy: 0.9622 - val_loss: 1.4998 - val_accuracy: 0.9665\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4906 - accuracy: 0.9747 - val_loss: 1.4943 - val_accuracy: 0.9702\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4846 - accuracy: 0.9796 - val_loss: 1.4910 - val_accuracy: 0.9718\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4812 - accuracy: 0.9827 - val_loss: 1.4891 - val_accuracy: 0.9734\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4789 - accuracy: 0.9841 - val_loss: 1.4885 - val_accuracy: 0.9732\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4768 - accuracy: 0.9858 - val_loss: 1.4879 - val_accuracy: 0.9742\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4756 - accuracy: 0.9868 - val_loss: 1.4891 - val_accuracy: 0.9736\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4744 - accuracy: 0.9879 - val_loss: 1.4883 - val_accuracy: 0.9731\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4733 - accuracy: 0.9890 - val_loss: 1.4864 - val_accuracy: 0.9757\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4725 - accuracy: 0.9895 - val_loss: 1.4879 - val_accuracy: 0.9728\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4724 - accuracy: 0.9898 - val_loss: 1.4863 - val_accuracy: 0.9755\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4716 - accuracy: 0.9902 - val_loss: 1.4869 - val_accuracy: 0.9751\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4709 - accuracy: 0.9909 - val_loss: 1.4862 - val_accuracy: 0.9754\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9914 - val_loss: 1.4884 - val_accuracy: 0.9735\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4702 - accuracy: 0.9915 - val_loss: 1.4886 - val_accuracy: 0.9739\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4697 - accuracy: 0.9920 - val_loss: 1.4869 - val_accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:01, 117.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 4\n",
      "rows to prune in layer 0 : 588\n",
      "[  1   2   6   7   9  10  11  12  14  15  16  17  20  22  28  29  31  32\n",
      "  33  36  37  40  41  43  48  49  50  52  55  56  58  60  62  63  64  66\n",
      "  69  71  72  73  74  75  76  78  79  82  83  85  87  88  96  97  98 100\n",
      " 102 103 108 110 115 116 118 119 121 123 127 129 130 131 132 134 137 138\n",
      " 139 145 148 150 152 153 156 158 160 162 163 166 167 168 173 174 175 176\n",
      " 178 184 186 187 188 189 190 191 192 193 194 196 197 199 200 201 206 207\n",
      " 209 212 214 216 217 219 221 223 224 225 227 228 229 235 240 241 242 243\n",
      " 250 251 252 253 254 255 256 261 262 263 266 268 270 271 273 274 276 277\n",
      " 282 283 285 289 293 294 297 298 299 300 301 305 306 308 311 312 313 314\n",
      " 315 316 317 319 320 321 324 325 326 328 334 335 338 339 340 344 347 349\n",
      " 350 351 352 354 356 357 361 364 365 368 369 371 372 373 374 375 376 377\n",
      " 378 381 383 386 388 391 392 394 396 398 399 401 402 403 407 408 409 411\n",
      " 414 415 417 418 419 421 422 423 425 426 431 432 435 437 438 440 441 443\n",
      " 446 447 448 449 453 456 461 462 464 466 467 469 475 479 481 488 490 491\n",
      " 492 493 495 496 500 501 504 506 507 513 514 516 524 526 527 528 529 535\n",
      " 536 537 541 544 547 550 552 554 555 556 558 560 561 564 566 568 572 573\n",
      " 574 576 578 579 581 583 584 585 592 594 597 598 600 601 604 606 608 614\n",
      " 615 616 617 619 621 622 624 625 626 627 629 631 632 636 637 643 645 646\n",
      " 649 650 651 652 653 654 655 656 657 658 660 665 666 670 671 672 673 676\n",
      " 678 679 680 684 687 691 693 694 696 699 701 702 703 704 706 708 710 712\n",
      " 714 715 719 721 724 725 726 728 729 733 734 735 737 738 740 742 745 748\n",
      " 754 756 757 759 761 763 768 770 771 776 777 779 780 781]\n",
      "[714 168 186   9  83 435 598 425 779 754 481 251  88 552 263   2 594 206\n",
      " 710 254 624  69 637  96 703 655 167 300 196 381 535 671  71 771  40 768\n",
      " 600 227 289 676 131 601 411 432 228 190 734 524 419 461 733 299 326 513\n",
      " 145 200 130  85 507 121 585 438 349 354 737 715 529 158 636 712 780 423\n",
      " 371 352 305 378 759 223 347 160 229  98 606 666  55 680 650 516 316 118\n",
      " 315 665 313 398 583 615 527 328 724 466 568 298  20 670 626 584 649 422\n",
      " 528 757 116 152 356 285 735 449 148 701 574  50  73 189 399 240 268 592\n",
      " 617 706 537 652 456 631 581  52  56 756 490 386  12 573  97 656 678 217\n",
      " 453 708 402 679 176 579  72 645 339 373 262  14 426 699 253 102 738 123\n",
      " 314  29  78 137 541 691 388  41  31 191 728  28 108 643 219 719 250 464\n",
      " 544 407 166 441 306 193  16 129 496 391 365 763  36  82 207 440 311 396\n",
      " 646 500 256 448 550 619 197 297 277 431 178  17  22 320 702  76   1 555\n",
      " 201 770 271 660 704 335 748 252 576 403 344 392 604 214 294 781  49  60\n",
      " 616 273 526  79 421  66 192  15 321 479 283 558 729  11 627 401 188 443\n",
      " 745 317  63 243 672 241 334  33 312 163 173 536 242 221 469 547 225 597\n",
      " 777 566 103 492 721 368 608 622 357 696  43 462 625 369 156 324 578 504\n",
      "  10 372 491 418 187 621 274 560 224 127  62 255 338 417 726 361 340 572\n",
      " 194 475 654 514 115 501 209 561 174 319 132 437 554 629 761 488 614 409\n",
      " 446 150   6 658 467 651 175 495 276 408 632 119 415   7  58 776  75 153\n",
      "  37 375 184  64 383 376 301 653 350 394 216 556 293 261 447 282  74 266\n",
      " 374 364  48 351  32 740 414 694 506 742 308 564 134 684 100 162 377 110\n",
      " 693 212 657 235 138 687 673  87 325 725 270 139 199 493]\n",
      "rows to prune in layer 3 : 225\n",
      "[  0   1   6   7   9  14  16  17  22  23  24  27  29  30  32  33  35  36\n",
      "  37  38  39  40  43  44  45  48  49  50  52  53  55  58  59  60  62  65\n",
      "  68  69  70  71  72  75  77  78  80  82  83  86  87  94  95  96  97  98\n",
      " 103 109 112 114 115 117 118 119 120 121 123 125 126 128 129 131 133 134\n",
      " 135 137 138 140 143 144 147 148 149 150 151 152 157 158 160 167 173 177\n",
      " 179 183 186 190 194 195 196 197 199 201 202 204 206 207 208 211 214 216\n",
      " 218 219 221 224 227 228 229 231 233 234 235 241 243 247 248 249 252 257\n",
      " 258 261 262 263 264 266 269 271 273 274 276 278 281 284 286 288 289 290\n",
      " 291 293 294 297 298 299]\n",
      "[248  53 115 286  27 224 143 218 233 126 289 281 206  65  38 273 262   9\n",
      " 135 150  30  58 228 134 247  37  45  43 195 197 293 151  78 129 196  83\n",
      " 290 297 133 243   6  29 261 219 199 147  36 227 241  86  17 183 264  52\n",
      "   7  39  50 231 299  48  69 114  55 179 144 138  87 298  35 252 131  68\n",
      "  44 177 202 117 194  71  77 125 291 269  95  82 152 167 140 249  32 119\n",
      " 207 158 284 137 208 216  49 211 258  14  16 112  94 274  59  98 201 263\n",
      "   1   0 121  24 276  23 103  40 229 271  80 204 257 186 278 120  75  22\n",
      " 266 214 160 109 221  96 288 157  70  97  33 294 190 148 123 149 173 118\n",
      "  62 128 234  60  72 235]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 0  1  2  3  5  7 11 13 14 15 20 21 25 27 28 29 30 32 35 37 39 48 52 54\n",
      " 56 59 61 62 64 65 66 67 68 69 73 74 76 78 79 81 85 87 88 89 90 93 94 95\n",
      " 96 99]\n",
      "[73 35 68 11 54 79 30  0 20 90 95 29 59 39  5 74 87  2 15 25 48 62 69 96\n",
      " 93 14 99 27 94 85 66 78 81 64  7  3 76 32 89 88 52 67 65 28 13 37 56 21\n",
      " 61  1]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5501 - accuracy: 0.9288 - val_loss: 1.5184 - val_accuracy: 0.9512\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5099 - accuracy: 0.9588 - val_loss: 1.5135 - val_accuracy: 0.9524\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5021 - accuracy: 0.9648 - val_loss: 1.5074 - val_accuracy: 0.9568\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4970 - accuracy: 0.9692 - val_loss: 1.5038 - val_accuracy: 0.9608\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4936 - accuracy: 0.9719 - val_loss: 1.5016 - val_accuracy: 0.9627\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4911 - accuracy: 0.9736 - val_loss: 1.5013 - val_accuracy: 0.9620\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4890 - accuracy: 0.9759 - val_loss: 1.5023 - val_accuracy: 0.9608\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4872 - accuracy: 0.9772 - val_loss: 1.4997 - val_accuracy: 0.9631\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4859 - accuracy: 0.9782 - val_loss: 1.4998 - val_accuracy: 0.9629\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4843 - accuracy: 0.9797 - val_loss: 1.4983 - val_accuracy: 0.9640\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.9804 - val_loss: 1.4977 - val_accuracy: 0.9654\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4823 - accuracy: 0.9811 - val_loss: 1.4964 - val_accuracy: 0.9664\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4815 - accuracy: 0.9819 - val_loss: 1.4976 - val_accuracy: 0.9653\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4806 - accuracy: 0.9826 - val_loss: 1.4964 - val_accuracy: 0.9657\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9832 - val_loss: 1.4983 - val_accuracy: 0.9640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:12, 121.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 4\n",
      "rows to prune in layer 0 : 686\n",
      "[  1   6   7  10  11  15  17  22  32  33  37  43  48  49  58  60  62  63\n",
      "  64  66  74  75  76  79  87 100 103 110 115 119 127 132 134 138 139 150\n",
      " 153 156 162 163 173 174 175 178 184 187 188 192 194 197 199 201 209 212\n",
      " 214 216 221 224 225 235 241 242 243 252 255 256 261 266 270 271 273 274\n",
      " 276 277 282 283 293 294 297 301 308 311 312 317 319 320 321 324 325 334\n",
      " 335 338 340 344 350 351 357 361 364 368 369 372 374 375 376 377 383 392\n",
      " 394 396 401 403 408 409 414 415 417 418 421 431 437 443 446 447 448 462\n",
      " 467 469 475 479 488 491 492 493 495 500 501 504 506 514 526 536 547 550\n",
      " 554 555 556 558 560 561 564 566 572 576 578 597 604 608 614 616 619 621\n",
      " 622 625 627 629 632 646 651 653 654 657 658 660 672 673 684 687 693 694\n",
      " 696 702 704 721 725 726 729 740 742 745 748 761 770 776 777 781]\n",
      "[243 340 197 469 660 448 283 725 276 658 368 651 745 431 572 100 392 702\n",
      " 214 684 173 536 479 729 696 225 418 224 282 376  15 320 673 317 187  60\n",
      " 319 558 506 235 401 394 351 153 514   7 547 657 294 261 687 396   6 632\n",
      " 150 377 501 110 437 403 301 409 619 119 194 324  76  17 184 566 271 321\n",
      "  74  10 770  32 369 139 616  79 408 242 704 325 361 500 604 293 374 270\n",
      " 672 156 297 446 127 646  33 311 201  75 578 467 622 132 504 554 335 415\n",
      " 608 372 312 274 241 560 334 526 740 350 209 357 475 273 555 726  66 417\n",
      " 103 414 134 192  62  22 561 138 344 629 163 761 162 375 781 266 221  37\n",
      "  49 597 748 491 621 178 308 653 564 492 488 625 550 255 199 383 576 614\n",
      " 421 462 443 115  87 495 277 694  64 256 212 556 777 742  11 175 693  48\n",
      " 364  58 338 776 188 627 654   1  43 721 174 493 447 252  63 216]\n",
      "rows to prune in layer 3 : 262\n",
      "[  0   1  14  16  22  23  24  32  33  40  49  59  60  62  70  71  72  75\n",
      "  77  80  82  94  95  96  97  98 103 109 112 117 118 119 120 121 123 125\n",
      " 128 137 140 148 149 152 157 158 160 167 173 186 190 194 201 204 207 208\n",
      " 211 214 216 221 229 234 235 249 257 258 263 266 269 271 274 276 278 284\n",
      " 288 291 294]\n",
      "[ 80  33 278  70   1 216  94 221  59 229 234 266 294  22  96 112  24 211\n",
      "  60 158   0 128  62 121 123  95 291  71  72 258 249 148 125  14 117 190\n",
      " 137 152 204  23 201 194 276  75 140 263 274 214 149 208 235  98 160 284\n",
      "  82  40  16 207 120 271 288 119 257  32 118 109 269 186  77 167  97 157\n",
      " 173 103  49]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 1  3  7 13 14 21 27 28 32 37 52 56 61 64 65 66 67 76 78 81 85 88 89 94\n",
      " 99]\n",
      "[56 66 21 67 88 37 85  7 76 99 78 13 32  1 61 94 27 64 65 52 81 28 89  3\n",
      " 14]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7366 - accuracy: 0.7525 - val_loss: 1.6425 - val_accuracy: 0.8334\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6318 - accuracy: 0.8448 - val_loss: 1.6184 - val_accuracy: 0.8542\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6128 - accuracy: 0.8602 - val_loss: 1.6075 - val_accuracy: 0.8613\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6024 - accuracy: 0.8680 - val_loss: 1.6001 - val_accuracy: 0.8675\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5949 - accuracy: 0.8755 - val_loss: 1.5946 - val_accuracy: 0.8746\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5901 - accuracy: 0.8790 - val_loss: 1.5918 - val_accuracy: 0.8730\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5862 - accuracy: 0.8820 - val_loss: 1.5890 - val_accuracy: 0.8760\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5826 - accuracy: 0.8852 - val_loss: 1.5870 - val_accuracy: 0.8783\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5798 - accuracy: 0.8879 - val_loss: 1.5828 - val_accuracy: 0.8827\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5775 - accuracy: 0.8892 - val_loss: 1.5825 - val_accuracy: 0.8833\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5755 - accuracy: 0.8909 - val_loss: 1.5833 - val_accuracy: 0.8824\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5735 - accuracy: 0.8931 - val_loss: 1.5778 - val_accuracy: 0.8864\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5720 - accuracy: 0.8944 - val_loss: 1.5787 - val_accuracy: 0.8853\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5706 - accuracy: 0.8954 - val_loss: 1.5778 - val_accuracy: 0.8864\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5691 - accuracy: 0.8971 - val_loss: 1.5782 - val_accuracy: 0.8858\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5678 - accuracy: 0.8978 - val_loss: 1.5791 - val_accuracy: 0.8838\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5667 - accuracy: 0.8986 - val_loss: 1.5761 - val_accuracy: 0.8859\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5654 - accuracy: 0.9007 - val_loss: 1.5763 - val_accuracy: 0.8875\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5647 - accuracy: 0.9005 - val_loss: 1.5755 - val_accuracy: 0.8870\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5635 - accuracy: 0.9020 - val_loss: 1.5756 - val_accuracy: 0.8875\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5628 - accuracy: 0.9026 - val_loss: 1.5744 - val_accuracy: 0.8886\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5624 - accuracy: 0.9022 - val_loss: 1.5760 - val_accuracy: 0.8857\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5618 - accuracy: 0.9033 - val_loss: 1.5727 - val_accuracy: 0.8905\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5610 - accuracy: 0.9035 - val_loss: 1.5734 - val_accuracy: 0.8883\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5600 - accuracy: 0.9051 - val_loss: 1.5740 - val_accuracy: 0.8869\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5595 - accuracy: 0.9055 - val_loss: 1.5720 - val_accuracy: 0.8903\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5590 - accuracy: 0.9061 - val_loss: 1.5735 - val_accuracy: 0.8886\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5581 - accuracy: 0.9066 - val_loss: 1.5738 - val_accuracy: 0.8890\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5576 - accuracy: 0.9069 - val_loss: 1.5741 - val_accuracy: 0.8870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [08:56, 134.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 4\n",
      "rows to prune in layer 0 : 735\n",
      "[  1  11  22  37  43  48  49  58  62  63  64  66  75  87 103 115 132 134\n",
      " 138 162 163 174 175 178 188 192 199 201 209 212 216 221 241 252 255 256\n",
      " 266 273 274 277 308 312 334 335 338 344 350 357 364 372 375 383 414 415\n",
      " 417 421 443 447 462 467 475 488 491 492 493 495 504 526 550 554 555 556\n",
      " 560 561 564 576 578 597 608 614 621 622 625 627 629 653 654 693 694 721\n",
      " 726 740 742 748 761 776 777 781]\n",
      "[ 43 578 493 174 556 561 221 627 209 462 364 163 443 338 266  22 495 492\n",
      "  62 622 447 375 776 621 629 564 256 274 372 103 134 334 357 526 312 115\n",
      "  63 608  75 138 597 383  37 277 252 175 467 625 576 414 273 421 693 188\n",
      " 162 350 344  11 653  64 742  49 555 554 761  87 201 199 504 132 721 781\n",
      " 560 216 726 192 740 614 241   1 335 550 415 654 178 748 212 488 417  58\n",
      " 694 777 475  66 308 491 255  48]\n",
      "rows to prune in layer 3 : 281\n",
      "[ 16  23  32  40  49  75  77  82  97  98 103 109 118 119 120 140 149 152\n",
      " 157 160 167 173 186 194 201 204 207 208 214 235 257 263 269 271 274 276\n",
      " 284 288]\n",
      "[269  16 194 149 173 288 214  32 118  75  49 207  40 103  98 257 119 235\n",
      " 160  82 201  77 208 284 263 274 186 152 276 140  23 109  97 271 167 157\n",
      " 204 120]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 1  3 14 27 28 32 52 61 64 65 81 89 94]\n",
      "[65  3 52 81  1 64 27 14 94 61 32 89 28]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9828 - accuracy: 0.4945 - val_loss: 1.8569 - val_accuracy: 0.6277\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8273 - accuracy: 0.6531 - val_loss: 1.8019 - val_accuracy: 0.6743\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7933 - accuracy: 0.6797 - val_loss: 1.7812 - val_accuracy: 0.6890\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7789 - accuracy: 0.6908 - val_loss: 1.7716 - val_accuracy: 0.6969\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7698 - accuracy: 0.6992 - val_loss: 1.7653 - val_accuracy: 0.7023\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7636 - accuracy: 0.7055 - val_loss: 1.7611 - val_accuracy: 0.7043\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7591 - accuracy: 0.7085 - val_loss: 1.7571 - val_accuracy: 0.7075\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7551 - accuracy: 0.7130 - val_loss: 1.7550 - val_accuracy: 0.7099\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7515 - accuracy: 0.7161 - val_loss: 1.7506 - val_accuracy: 0.7135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7487 - accuracy: 0.7184 - val_loss: 1.7488 - val_accuracy: 0.7168\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7464 - accuracy: 0.7197 - val_loss: 1.7460 - val_accuracy: 0.7182\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7440 - accuracy: 0.7226 - val_loss: 1.7454 - val_accuracy: 0.7194\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7422 - accuracy: 0.7245 - val_loss: 1.7434 - val_accuracy: 0.7206\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7405 - accuracy: 0.7258 - val_loss: 1.7416 - val_accuracy: 0.7222\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7390 - accuracy: 0.7273 - val_loss: 1.7400 - val_accuracy: 0.7226\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7376 - accuracy: 0.7278 - val_loss: 1.7412 - val_accuracy: 0.7218\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7365 - accuracy: 0.7284 - val_loss: 1.7387 - val_accuracy: 0.7248\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7353 - accuracy: 0.7299 - val_loss: 1.7367 - val_accuracy: 0.7259\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7342 - accuracy: 0.7312 - val_loss: 1.7359 - val_accuracy: 0.7285\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7332 - accuracy: 0.7322 - val_loss: 1.7366 - val_accuracy: 0.7269\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7321 - accuracy: 0.7328 - val_loss: 1.7343 - val_accuracy: 0.7286\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7316 - accuracy: 0.7331 - val_loss: 1.7341 - val_accuracy: 0.7275\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7303 - accuracy: 0.7337 - val_loss: 1.7330 - val_accuracy: 0.7310\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7297 - accuracy: 0.7345 - val_loss: 1.7328 - val_accuracy: 0.7309\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7290 - accuracy: 0.7357 - val_loss: 1.7315 - val_accuracy: 0.7316\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7286 - accuracy: 0.7359 - val_loss: 1.7321 - val_accuracy: 0.7288\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7276 - accuracy: 0.7367 - val_loss: 1.7321 - val_accuracy: 0.7302\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7273 - accuracy: 0.7371 - val_loss: 1.7314 - val_accuracy: 0.7286\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7264 - accuracy: 0.7378 - val_loss: 1.7312 - val_accuracy: 0.7319\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7261 - accuracy: 0.7382 - val_loss: 1.7307 - val_accuracy: 0.7304\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7256 - accuracy: 0.7391 - val_loss: 1.7310 - val_accuracy: 0.7315\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7253 - accuracy: 0.7380 - val_loss: 1.7306 - val_accuracy: 0.7305\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7248 - accuracy: 0.7398 - val_loss: 1.7304 - val_accuracy: 0.7321\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7244 - accuracy: 0.7401 - val_loss: 1.7284 - val_accuracy: 0.7337\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7240 - accuracy: 0.7406 - val_loss: 1.7283 - val_accuracy: 0.7324\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7237 - accuracy: 0.7399 - val_loss: 1.7288 - val_accuracy: 0.7301\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7233 - accuracy: 0.7400 - val_loss: 1.7289 - val_accuracy: 0.7307\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7229 - accuracy: 0.7405 - val_loss: 1.7281 - val_accuracy: 0.7329\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7228 - accuracy: 0.7413 - val_loss: 1.7274 - val_accuracy: 0.7340\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7225 - accuracy: 0.7411 - val_loss: 1.7272 - val_accuracy: 0.7336\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7217 - accuracy: 0.7420 - val_loss: 1.7273 - val_accuracy: 0.7349\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7217 - accuracy: 0.7416 - val_loss: 1.7281 - val_accuracy: 0.7329\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7214 - accuracy: 0.7426 - val_loss: 1.7263 - val_accuracy: 0.7349\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7210 - accuracy: 0.7425 - val_loss: 1.7283 - val_accuracy: 0.7321\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7211 - accuracy: 0.7420 - val_loss: 1.7263 - val_accuracy: 0.7348\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7205 - accuracy: 0.7429 - val_loss: 1.7263 - val_accuracy: 0.7342\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7202 - accuracy: 0.7436 - val_loss: 1.7251 - val_accuracy: 0.7355\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7201 - accuracy: 0.7438 - val_loss: 1.7262 - val_accuracy: 0.7359\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7198 - accuracy: 0.7440 - val_loss: 1.7247 - val_accuracy: 0.7351\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7194 - accuracy: 0.7447 - val_loss: 1.7256 - val_accuracy: 0.7345\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7191 - accuracy: 0.7446 - val_loss: 1.7254 - val_accuracy: 0.7356\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7190 - accuracy: 0.7445 - val_loss: 1.7257 - val_accuracy: 0.7368\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:15, 153.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 4\n",
      "rows to prune in layer 0 : 759\n",
      "[  1  11  48  49  58  64  66  87 132 162 178 188 192 199 201 212 216 241\n",
      " 255 273 308 335 344 350 414 415 417 421 475 488 491 504 550 554 555 560\n",
      " 614 653 654 693 694 721 726 740 742 748 761 777 781]\n",
      "[201 162 781   1 273  49 421 132 550 488 350 555  48 417 748 742 414 475\n",
      " 614 491 192 212 694 693 216 199 721 560 653 308 726  58 554 344 740  66\n",
      "  11  87  64 188 178 335 415 777 504 241 255 654 761]\n",
      "rows to prune in layer 3 : 290\n",
      "[ 23  77  82  97 109 120 140 152 157 167 186 201 204 208 263 271 274 276\n",
      " 284]\n",
      "[274  77 271 167 157 186 120 140 284  97 152 263 276  82  23 208 204 109\n",
      " 201]\n",
      "rows to prune in layer 6 : 96\n",
      "[14 27 28 32 61 89 94]\n",
      "[14 61 89 94 28 27 32]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2365 - accuracy: 0.2118 - val_loss: 2.2047 - val_accuracy: 0.2384\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1632 - accuracy: 0.2975 - val_loss: 2.1370 - val_accuracy: 0.3233\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1164 - accuracy: 0.3486 - val_loss: 2.1021 - val_accuracy: 0.3627\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1012 - accuracy: 0.3634 - val_loss: 2.0914 - val_accuracy: 0.3750\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0927 - accuracy: 0.3708 - val_loss: 2.0860 - val_accuracy: 0.3747\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0881 - accuracy: 0.3734 - val_loss: 2.0823 - val_accuracy: 0.3802\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0851 - accuracy: 0.3753 - val_loss: 2.0797 - val_accuracy: 0.3812\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0830 - accuracy: 0.3778 - val_loss: 2.0784 - val_accuracy: 0.3826\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0812 - accuracy: 0.3785 - val_loss: 2.0788 - val_accuracy: 0.3812\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0801 - accuracy: 0.3794 - val_loss: 2.0767 - val_accuracy: 0.3817\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0790 - accuracy: 0.3805 - val_loss: 2.0762 - val_accuracy: 0.3822\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0780 - accuracy: 0.3814 - val_loss: 2.0755 - val_accuracy: 0.3847\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0773 - accuracy: 0.3817 - val_loss: 2.0743 - val_accuracy: 0.3844\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0765 - accuracy: 0.3826 - val_loss: 2.0741 - val_accuracy: 0.3855\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0757 - accuracy: 0.3838 - val_loss: 2.0731 - val_accuracy: 0.3851\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0750 - accuracy: 0.3838 - val_loss: 2.0728 - val_accuracy: 0.3848\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0742 - accuracy: 0.3846 - val_loss: 2.0717 - val_accuracy: 0.3872\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0737 - accuracy: 0.3844 - val_loss: 2.0727 - val_accuracy: 0.3853\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0729 - accuracy: 0.3859 - val_loss: 2.0724 - val_accuracy: 0.3861\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0723 - accuracy: 0.3862 - val_loss: 2.0713 - val_accuracy: 0.3876\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0717 - accuracy: 0.3865 - val_loss: 2.0708 - val_accuracy: 0.3875\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0712 - accuracy: 0.3867 - val_loss: 2.0695 - val_accuracy: 0.3891\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0708 - accuracy: 0.3875 - val_loss: 2.0695 - val_accuracy: 0.3892\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0702 - accuracy: 0.3885 - val_loss: 2.0698 - val_accuracy: 0.3890\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0696 - accuracy: 0.3888 - val_loss: 2.0701 - val_accuracy: 0.3866\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0692 - accuracy: 0.3887 - val_loss: 2.0688 - val_accuracy: 0.3901\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0687 - accuracy: 0.3893 - val_loss: 2.0691 - val_accuracy: 0.3902\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0683 - accuracy: 0.3896 - val_loss: 2.0694 - val_accuracy: 0.3874\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0681 - accuracy: 0.3898 - val_loss: 2.0695 - val_accuracy: 0.3876\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [14:13, 143.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 4\n",
      "rows to prune in layer 0 : 771\n",
      "[ 11  58  64  66  87 178 188 199 216 241 255 308 335 344 415 504 554 560\n",
      " 653 654 721 726 740 761 777]\n",
      "[344 653  58  87 721 504 308 255 726 199 188 178 241  66 654 415 560 761\n",
      " 216 554 335  64 777  11 740]\n",
      "rows to prune in layer 3 : 295\n",
      "[ 23  82  97 109 152 201 204 208 263 276]\n",
      "[204  97 201 276  82 109 208  23 152 263]\n",
      "rows to prune in layer 6 : 98\n",
      "[27 28 32 94]\n",
      "[94 32 28 27]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2907 - accuracy: 0.1592 - val_loss: 2.2616 - val_accuracy: 0.1745\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2374 - accuracy: 0.2011 - val_loss: 2.2224 - val_accuracy: 0.2369\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2095 - accuracy: 0.2508 - val_loss: 2.1981 - val_accuracy: 0.2584\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1920 - accuracy: 0.2703 - val_loss: 2.1823 - val_accuracy: 0.2826\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1814 - accuracy: 0.2812 - val_loss: 2.1723 - val_accuracy: 0.2939\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1747 - accuracy: 0.2858 - val_loss: 2.1674 - val_accuracy: 0.2973\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1716 - accuracy: 0.2872 - val_loss: 2.1646 - val_accuracy: 0.2966\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1692 - accuracy: 0.2888 - val_loss: 2.1626 - val_accuracy: 0.2989\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1675 - accuracy: 0.2894 - val_loss: 2.1612 - val_accuracy: 0.2991\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1661 - accuracy: 0.2905 - val_loss: 2.1600 - val_accuracy: 0.2989\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1649 - accuracy: 0.2912 - val_loss: 2.1587 - val_accuracy: 0.3015\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1641 - accuracy: 0.2923 - val_loss: 2.1578 - val_accuracy: 0.3010\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1632 - accuracy: 0.2928 - val_loss: 2.1580 - val_accuracy: 0.3029\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1626 - accuracy: 0.2939 - val_loss: 2.1572 - val_accuracy: 0.3023\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1620 - accuracy: 0.2943 - val_loss: 2.1564 - val_accuracy: 0.3014\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1614 - accuracy: 0.2949 - val_loss: 2.1559 - val_accuracy: 0.3023\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1610 - accuracy: 0.2956 - val_loss: 2.1552 - val_accuracy: 0.3022\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1606 - accuracy: 0.2956 - val_loss: 2.1547 - val_accuracy: 0.3037\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1602 - accuracy: 0.2962 - val_loss: 2.1547 - val_accuracy: 0.3026\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1598 - accuracy: 0.2969 - val_loss: 2.1540 - val_accuracy: 0.3033\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1594 - accuracy: 0.2972 - val_loss: 2.1541 - val_accuracy: 0.3037\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1592 - accuracy: 0.2971 - val_loss: 2.1534 - val_accuracy: 0.3038\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1588 - accuracy: 0.2980 - val_loss: 2.1540 - val_accuracy: 0.3019\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1586 - accuracy: 0.2980 - val_loss: 2.1532 - val_accuracy: 0.3047\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1583 - accuracy: 0.2980 - val_loss: 2.1527 - val_accuracy: 0.3034\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1579 - accuracy: 0.2982 - val_loss: 2.1528 - val_accuracy: 0.3045\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1577 - accuracy: 0.2993 - val_loss: 2.1525 - val_accuracy: 0.3020\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1574 - accuracy: 0.2989 - val_loss: 2.1524 - val_accuracy: 0.3038\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1571 - accuracy: 0.2990 - val_loss: 2.1520 - val_accuracy: 0.3031\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1569 - accuracy: 0.2994 - val_loss: 2.1515 - val_accuracy: 0.3044\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1566 - accuracy: 0.3003 - val_loss: 2.1510 - val_accuracy: 0.3047\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1564 - accuracy: 0.2998 - val_loss: 2.1513 - val_accuracy: 0.3049\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1563 - accuracy: 0.2998 - val_loss: 2.1506 - val_accuracy: 0.3061\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1559 - accuracy: 0.3006 - val_loss: 2.1504 - val_accuracy: 0.3060\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1558 - accuracy: 0.3002 - val_loss: 2.1508 - val_accuracy: 0.3065\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1555 - accuracy: 0.3005 - val_loss: 2.1509 - val_accuracy: 0.3062\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1553 - accuracy: 0.3007 - val_loss: 2.1499 - val_accuracy: 0.3063\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1551 - accuracy: 0.3010 - val_loss: 2.1496 - val_accuracy: 0.3072\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1549 - accuracy: 0.3011 - val_loss: 2.1491 - val_accuracy: 0.3080\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1546 - accuracy: 0.3014 - val_loss: 2.1489 - val_accuracy: 0.3076\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1545 - accuracy: 0.3017 - val_loss: 2.1487 - val_accuracy: 0.3089\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1543 - accuracy: 0.3024 - val_loss: 2.1489 - val_accuracy: 0.3096\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1540 - accuracy: 0.3033 - val_loss: 2.1477 - val_accuracy: 0.3103\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1538 - accuracy: 0.3035 - val_loss: 2.1486 - val_accuracy: 0.3096\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1536 - accuracy: 0.3041 - val_loss: 2.1480 - val_accuracy: 0.3099\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1536 - accuracy: 0.3038 - val_loss: 2.1475 - val_accuracy: 0.3105\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1533 - accuracy: 0.3046 - val_loss: 2.1469 - val_accuracy: 0.3113\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1531 - accuracy: 0.3052 - val_loss: 2.1477 - val_accuracy: 0.3144\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1529 - accuracy: 0.3060 - val_loss: 2.1477 - val_accuracy: 0.3120\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1528 - accuracy: 0.3064 - val_loss: 2.1471 - val_accuracy: 0.3153\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [16:43, 143.32s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [1:32:53<1:32:18, 1107.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5752 - accuracy: 0.8979 - val_loss: 1.5183 - val_accuracy: 0.9457\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 5\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[336 100 780 757 648 751 268  29 761 421 565 122 456 408 330 767 167 433\n",
      "  44  40 709 781 248 207 310 269 542 442 154 364 113 334  15 458 230 133\n",
      " 335 632 344 423 628  36 593 356 177 121 555 182 683  26 243 740 707 506\n",
      " 348 397 426 468 581 663 572 171 234 467 534 529 574 117 478 658 735 132\n",
      "  48 415 146 286 783 576 196 476   2 326 367 198 544  16 444 431 261 507\n",
      " 610 406  68 479 346 435 473 570 520 162 351 391 226 360 638 491 665 112\n",
      "  82 395   9 416 453 324 738 686  20 108 439 190 549 466 102 172 617 527\n",
      " 325 149 726 237 678 145 424  76 292 774 130 769 472 664   5 375 174 733\n",
      " 349 499  42 443 209 512 245 553 673 274 483 693 450 518 745 194 498 159\n",
      " 747 272 414  52 223 440 436 296 568 401 359  50 312 255 771 352 278 754\n",
      " 419 134 557 772 141 270 256 672 714 583 452 502 755 706 510 526 338 155\n",
      " 717 142 287 461 725   1 606  90 517 153 776 160 650 619 115 684 144 729\n",
      " 337 180 366  12 199 765 390 238 210 449 448 654 150 496 490 475 704 412\n",
      " 566 422 474 333 138 105 126 716 484 170   4 341 700 317 613 441 625 469\n",
      " 736 513 393 279 457 201 575 522 519 708 460  61 670  65 616 732 221 253\n",
      " 187 525 770 368 782 611 129 259 465 614 107 110 682 580  77 470 697 744\n",
      " 541 195 644 621 766 251 702  47 674 676 779 543 383 720 148 515  83 489\n",
      " 396 331  60 282 189 247  27 698  37 118 492 425  17 537 305 106 462 222\n",
      "  87 246 589 637 668 573  19  45 241 373 532 116 592 119 288 206 584  96\n",
      " 655  49  99 620 109 188 602 354 152  97 681 596 236 622 728 147 303 773\n",
      " 535 285 175 151 762 605 297  39  55 748 332 630   0 263 695 203 763 711\n",
      " 389 680  91 280 675 350 724 601 294 249 163 120 125 229 445 530 388 551\n",
      " 402 777 313 677 600 216 629 604  30 329 276 271  28  58 235 140 455 124\n",
      " 750 212 477 387 639 647 718 546 275 386 265 703  88 184  73 742 564 778\n",
      "  70 485 262 758 434 266 339 731 480 659  13 233 538 539 459 381  23 127\n",
      "  78 536 432 257 319  63 626 267 559 612 301 531 211  41 587 254 316 204\n",
      "  67 166 687 627 342 220 699  59 715 208  34 752 200 653 599 590 411 231\n",
      "  72   8  84 756  35 547 143 428 642 340 158 413 164 500 550 358 727 567\n",
      " 554  32 437 355  64 370 454 464 694 320 548 607 131  66 318 556 701 173\n",
      "  92 685 514  71 311 191 571 258 384 487 524 205 586   3 385 179 438 165\n",
      " 181 635 741 493 298 398 273 407 228 471 447 169 240 504 631 137 239 192\n",
      " 652 281 197 689 377 293 328 369 178  62  93 315 521 135 667  89 392 688\n",
      " 743 307 306  69 343 753  86 365 232 721 662 304 283 560 382 516 295  24\n",
      " 250 321 374 505  80 420 722 327  79 760 103 193  85 508 394 409 540 712\n",
      " 156 645 363 218 322 290 511 202 299  11  18 488 696 345 679 641 588 595\n",
      " 746 361 376 405 463  31 597 111 561 136  10 634  51 217 219 651 446 400\n",
      " 497 598 705 615 309 277 528 666 252 646  22 594 661 410 186 494 552  74\n",
      "   7 579 347  14 417 482 378 713 128 775 691 690 264 759 430 183  81 737\n",
      " 545 640 669  33 562 503 161 643 495 585 429 114 768 371 486 591 104 284\n",
      " 660 577 260  53  54 608 308 618 224 563  57  75 509 730 353 558 372 214\n",
      " 656 723 578 213 523 609 380  25 314 300 603 624  94 227  56  43 176 215\n",
      " 657 362 185 379 157 692 242 582  98 671 764 139  95 501 323 481 168 403\n",
      " 739 649 101   6 633  21 123 291 533 749 289 569 734 451 418 404  38 623\n",
      " 719 399 710  46 636 357 225 302 244 427]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[127 202 170  67 232 105  43 205 183 293  88  33 299 231  53  25   8  50\n",
      " 182 152 122   9 282   0 123 195 159  94 248 283 241 212 259 124  75 110\n",
      " 172 165  72 244  85 278 291 295   5  10  14 213 197  16 186 162  66 120\n",
      " 233 111 139 247 102 229 214  17 298 114  28  86 169 253  99  31 194 190\n",
      "  77 147  18 228   6  96 269 151 107  64 216 158  24  81 268 297 274 238\n",
      "  11 289 175 157  79  44 257  51 140 207 103 279 217 100  47 206  38 249\n",
      "   7 284 222 226  19 292 203 135  87 210 131 230 196 174  80 208 266   3\n",
      " 104 177 137 256 199 115 148  59 219 117 181  76   1 296  62 240 121  22\n",
      " 225  90 191 161 184  13  12  98 171 116 243 176  84 156  71 134  74  36\n",
      "  35 198 275  60 201 185 209 173  58  49 204 254  91 145 237 118 160 143\n",
      " 265 287 271 242 141 126 154 264  42 227  55 168 200 106 193 286 234 119\n",
      " 260 261 136  46 128  39 246  32 285 163 218 142  97  56 188 150 252 255\n",
      "  78 267 153 245  27 277  68  37  69 187 108 192 125  23  70  63 112  48\n",
      " 133 215 130 251  29 155 179  20 221 144  45 149 178 290  30 236  21  52\n",
      "   4 167  54 288  40 101 294 262  15  89  95 129 211 189 276 263 273  34\n",
      " 258 239 281 250  82 166 180   2 223  41  93  73 272  83  61 220 270 132\n",
      "  26  57 109 235 224 164  92 138 113 280 146  65]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[21 14 54 73 78 28 19 25 23 87 83 50 24 72 59 18 32 43 77 70 94 49 48 33\n",
      " 62 91 38 88 40 99 71 53 37  4 68 76  3 81 90 46 63 34 97 44 95  5 36 92\n",
      " 64 55 58 22 42 57 51 15 65  2 39 20 35 13 41 26 31 82  9 16  7 66 96  8\n",
      " 56 47 10 52 11 67 75 27 12 80 93 17 86 79 61 74 84 29  1  6 85 89 30 69\n",
      " 60 45  0 98]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5114 - accuracy: 0.9522 - val_loss: 1.5082 - val_accuracy: 0.9553\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4983 - accuracy: 0.9645 - val_loss: 1.5000 - val_accuracy: 0.9622\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4910 - accuracy: 0.9711 - val_loss: 1.4955 - val_accuracy: 0.9660\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4859 - accuracy: 0.9761 - val_loss: 1.4931 - val_accuracy: 0.9685\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4831 - accuracy: 0.9789 - val_loss: 1.4875 - val_accuracy: 0.9739\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4810 - accuracy: 0.9807 - val_loss: 1.4898 - val_accuracy: 0.9720\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4789 - accuracy: 0.9829 - val_loss: 1.4857 - val_accuracy: 0.9752\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4775 - accuracy: 0.9841 - val_loss: 1.4847 - val_accuracy: 0.9767\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4760 - accuracy: 0.9855 - val_loss: 1.4857 - val_accuracy: 0.9746\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4753 - accuracy: 0.9862 - val_loss: 1.4857 - val_accuracy: 0.9759\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4750 - accuracy: 0.9864 - val_loss: 1.4833 - val_accuracy: 0.9779\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4736 - accuracy: 0.9875 - val_loss: 1.4851 - val_accuracy: 0.9765\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4732 - accuracy: 0.9881 - val_loss: 1.4824 - val_accuracy: 0.9783\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4728 - accuracy: 0.9885 - val_loss: 1.4843 - val_accuracy: 0.9774\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4723 - accuracy: 0.9888 - val_loss: 1.4852 - val_accuracy: 0.9758\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4728 - accuracy: 0.9884 - val_loss: 1.4838 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:42, 162.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 5\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[266 125 732 440 454 241 487 210 237 389 590 322 475 777 576 173 249 778\n",
      "  13 166 506 598 387 463 549  65 639 535 375 773 468 543 244  43 279 212\n",
      " 253 750 390 388 175 278 383 418 675 359 681 345 646 531  88 355 599   0\n",
      " 480 271 633 561 292 486 441 348 326 696 395 156 208 449 600 106  44 588\n",
      "  20 654 718 167 641 188 758 114  26 706 219  54 136 325 614 415 417   6\n",
      " 426 688 211 450 736 719 408 174 265 270 197 711 272  74  31 329 444 169\n",
      " 556 740 733 757 351 215 194 507 762 112 435 754 343 669 242 519 715 604\n",
      " 459  55 618 761  71 532 617 193 466 201  10  62 553 216 783 644 493 632\n",
      " 630  38  57 180 207 455 695 699 509 638  73  21 283 573 485 640 473 772\n",
      " 684 402 222  34 133  72 470 209 427 336 771 123 328 726 138 257 379   9\n",
      " 274 636 739  41  85 662 514 524 116 520 643 529 302 341 110 504 425  69\n",
      " 560 567 766 689 606 540 245 368 119 113 424 547 605 187 679 702 362  83\n",
      " 581 346 373 626 511 264 101 709  60 544   2 625 713 262 746 352 710 179\n",
      " 224 622 163 445 673 635 503 517 741 263 609  81 539 384 508 269 502  29\n",
      " 171 541 492  96 760 522   1 613 616 764 678 665 422 372 423 776 151 285\n",
      " 717 252  39 497 327 146 204 394 725 708 705 366 122 562 585 577 289 545\n",
      " 775 464 460 238 357  84 200  27 416 568 228 510 178 141 526 304 381  82\n",
      " 767 428 597 782 714 319  58 382 680 565 685 199 339 235 721  12 369 275\n",
      " 314 291 700 360 437  25 312 774 307  35  91 367  28 295 571 421 333 478\n",
      " 686 660 259 768 300 516 525 744 664 499 203 458  18 190 738 765 268 340\n",
      " 559 298  95 159 413 168 147  67 505 183  78 518 218 534 287 586 321  76\n",
      " 482 623 233 181 385  94 698 591 690  99 672 476 661 602 723 563 293 722\n",
      " 728 142  92 404 185 683  15 134 196  70 645 297 619 140 579 494 570 692\n",
      " 429 400 647 523 578 477 176 536 310 521 566  98 162 153 443 658 648 656\n",
      " 438 330 256 727 230 653 552 580 569 338 374 720  80 564 182 342 551 419\n",
      " 755 491 659 198 393 170 745   3 364 250 442 584 294 601 145  17 751 554\n",
      " 100 358  46 759 149 495 707 735 108 469 693 306 248 471 432 575 515 316\n",
      " 674 172 537 280 378 239  23 318 315 582 474 500 769 501 483 261 406 361\n",
      " 205 313  37  89 137  36 749 781 231 371 195 161  42 530 612 132 512 102\n",
      " 548 589 128 642 344  14  52 243 301 624 779 528  49 546 457 308 603 663\n",
      " 380 399 213 621 542 288 452 409 317 743 334 192 481 370 148 377 144 129\n",
      " 687 627 396 667 668 247  16 354 472  24 550 703  40 694 610 737   4 157\n",
      "  79 126 254 770 637 120 365 608 124 131 286 592 780 332 281  87 353 533\n",
      " 631 260 467 186 513 232 391 410 165 118 164 484 655  86 117 666 651 335\n",
      " 155 607 557 111 704  30 650 676 555 331 446 226 742 255 398 682  75 305\n",
      " 109 311 611 202 439 405 251 701 127 323 593  50  47 303 234  53 277 628\n",
      " 104 433 596  51 229 320 620 734 121 436 649 350 763 412 496 214 284 290\n",
      " 324  45 258 189 191 465 724  77 143  64 448 583 431 574  97 538 697 712\n",
      " 752 594 135 184 401 376 671 420 489 349 652 107 236  68  32 462 595 273\n",
      " 729 558   8 488 716 456 105 220 479 240 403 356 434 527 747 453 223 276\n",
      " 587 158  22 691 753 309  19  56 227  48 282 407 103 629 677 392 139 461\n",
      " 217  33  61 221 130  90 498  66  59 225  11 490 411 730 299 363 756   7\n",
      " 267 115  93 670 414 246 386 152 451  63 160 430 347 634 731 296 337 206\n",
      " 150 657   5 154 447 177 572 615 397 748]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[292  42 134 124 253  97  78 115  69  20 216 108 144 166  71 248 273 132\n",
      " 106 123 246  79  76 291 237 209 263 285 193 142 197  44 183  99 218 272\n",
      " 188 195  57  58 288 170 169 133 126 251  14 279 211 257 231 181 282 212\n",
      " 215 299   5 138   0 245 252 161  25 121 178 135 111  67 116 120 298  62\n",
      " 217  16 238 224 117  15  91  35  29  12 201 283 260  38 225  46 258 255\n",
      "  68   3 119  81  63  95  73 198 159   2 130 293 228 148  31 184 259  92\n",
      " 274 149  48 194 204  23 243 185 242 179 128 164 100 107 294 220  43  39\n",
      " 158 289  54  28 109 247 207 200 153 125  30 180  61  74  87 191 261 271\n",
      " 256  64  83  17  86 287 254 103 141   6  34 284  65 229 110  41  37  11\n",
      "  52 113 152  77 227 286 151 173  59 214 168  66  19 235 296 264 206 221\n",
      " 136 269 249 205 276 127 187  24 163 105 210 155 199  45  94 226 140 172\n",
      "  21 239  72   1 165 112  36 171  33 160 236 177 154  18 278  89   8  96\n",
      "  32 219 104 297 234 139 262 137   9 270 240  10  84 114 190  40 232 202\n",
      "  98  93 268  90  22 233 101 192 230 203 146  56 222  13  47  51  49  55\n",
      " 189 156 213 295 280 147 174  85 267 244 175 196   4 129  80  75 265 145\n",
      "  82 290 143 277 167  27 162   7 275 122 266  60 157  88 241  26 150  53\n",
      " 208 131 281 186 102 223 118  50  70 176 182 250]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[67 97 27  7 51 83 63 34 15 43 88 99 77 79 28 94 82 22 16  8  5 59  4 89\n",
      " 87 33 17 96 70 85 32 73 58 42 75 37 31 65  3 21 10 90 69 53 78 55 46 38\n",
      " 98 49 86 50 68 61 44 18 23  1  0  2 14 13 74 66 62 24  6 41 57 47 80 12\n",
      " 54 11 40 36 29 84 92 30 45 19 39 95 71 56  9 64 91 48 35 60 52 26 72 76\n",
      " 93 81 25 20]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4955 - accuracy: 0.9723 - val_loss: 1.4928 - val_accuracy: 0.9714\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4817 - accuracy: 0.9829 - val_loss: 1.4896 - val_accuracy: 0.9739\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4775 - accuracy: 0.9857 - val_loss: 1.4898 - val_accuracy: 0.9742\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9875 - val_loss: 1.4887 - val_accuracy: 0.9741\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4735 - accuracy: 0.9892 - val_loss: 1.4877 - val_accuracy: 0.9747\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4718 - accuracy: 0.9907 - val_loss: 1.4870 - val_accuracy: 0.9747\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4717 - accuracy: 0.9905 - val_loss: 1.4860 - val_accuracy: 0.9762\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4711 - accuracy: 0.9910 - val_loss: 1.4851 - val_accuracy: 0.9776\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4698 - accuracy: 0.9922 - val_loss: 1.4839 - val_accuracy: 0.9782\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4695 - accuracy: 0.9926 - val_loss: 1.4852 - val_accuracy: 0.9765\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4699 - accuracy: 0.9920 - val_loss: 1.4876 - val_accuracy: 0.9739\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4692 - accuracy: 0.9926 - val_loss: 1.4842 - val_accuracy: 0.9777\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:44, 150.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 5\n",
      "rows to prune in layer 0 : 588\n",
      "[  3   4   5   7   8  11  14  15  16  17  19  22  23  24  30  32  33  36\n",
      "  37  40  42  45  46  47  48  49  50  51  52  53  56  59  61  63  64  66\n",
      "  68  70  75  77  79  80  86  87  89  90  92  93  97  98 100 102 103 104\n",
      " 105 107 108 109 111 115 117 118 120 121 124 126 127 128 129 130 131 132\n",
      " 134 135 137 139 140 142 143 144 145 148 149 150 152 153 154 155 157 158\n",
      " 160 161 162 164 165 170 172 176 177 182 184 185 186 189 191 192 195 196\n",
      " 198 202 205 206 213 214 217 220 221 223 225 226 227 229 230 231 232 234\n",
      " 236 239 240 243 246 247 248 250 251 254 255 256 258 260 261 267 273 276\n",
      " 277 280 281 282 284 286 288 290 293 294 296 297 299 301 303 305 306 308\n",
      " 309 310 311 313 315 316 317 318 320 323 324 330 331 332 334 335 337 338\n",
      " 342 344 347 349 350 353 354 356 358 361 363 364 365 370 371 374 376 377\n",
      " 378 380 386 391 392 393 396 397 398 399 400 401 403 404 405 406 407 409\n",
      " 410 411 412 414 419 420 429 430 431 432 433 434 436 438 439 442 443 446\n",
      " 447 448 451 452 453 456 457 461 462 465 467 469 471 472 474 477 479 481\n",
      " 483 484 488 489 490 491 494 495 496 498 500 501 512 513 515 521 523 527\n",
      " 528 530 533 536 537 538 542 546 548 550 551 552 554 555 557 558 563 564\n",
      " 566 569 570 572 574 575 578 579 580 582 583 584 587 589 592 593 594 595\n",
      " 596 601 603 607 608 610 611 612 615 619 620 621 624 627 628 629 631 634\n",
      " 637 642 645 647 648 649 650 651 652 653 655 656 657 658 659 663 666 667\n",
      " 668 670 671 674 676 677 682 683 687 691 692 693 694 697 701 703 704 707\n",
      " 712 716 720 722 723 724 727 728 729 730 731 734 735 737 742 743 745 747\n",
      " 748 749 751 752 753 755 756 759 763 769 770 779 780 781]\n",
      "[306 737 704 436 412 280 332 578 288  51 282 404 589 399 409 442  90 411\n",
      " 477 121 293  75  98 676  11 548 192 668 365 214 126  77  64 752 315 144\n",
      " 347 550  46 536 692 137 621 405 330 281 513 748 115 294 743 247 749 595\n",
      " 594 527 500 642 691 276  36 143 551 628 456 142 161 356 158  80 227 102\n",
      " 364 667 258 770 670 431 451 496   8 587 386 196 414 479 593 189 198 469\n",
      " 170 358 611 671 763 202 720 624 331 747 528 596 769 299 697 490 229 575\n",
      " 653  52 149 542  93 255 148 579 729 615 607 129 512 246 674 537  48 631\n",
      " 220  37 461 261 316 318 619 153 380 338 521 176 730 243  53 683 707 397\n",
      " 217  16 309 712 472 378 398 172 165 557 530 260 323 753 574 755 564 582\n",
      " 656   3 320  47 780 226 286  33 400 658   7 555 488 185 483 655 434 515\n",
      " 410 467 566 457 231 186 313 637 160 308 558 439 206 182 666 734 396 647\n",
      " 465 443 374 124 592 334 701 370 703 724 140 649 393 391 448 105  56 452\n",
      " 267 118 132 583 230 127 612 601 205 134 433  24  92 432 277  59 296 735\n",
      " 377 240 392 225 131  63 501  70 297 335   4 645  15 538  30 239  89 256\n",
      " 324 109 155 407 657 584 250 759 350  42 273 751 251 354 419 570 284 498\n",
      " 471 603 363  40 145 353 552  50 337 572  23 430 195 154 371 104 634 162\n",
      " 130 438  86 254 107 489  45  97 742 651 234 610 221 349 722 523 756  32\n",
      " 128 100 484 290 223 727 659  79 716 152 677 184  19 546 491 554 111 627\n",
      " 608 150 652 629  87 117 533 108 648 248   5 650 620 232 135 447 494 311\n",
      "  61 474  17 481 164 157 569 420 120 693  14  66 682 731 663 462 694 342\n",
      " 495 361 401 723 177 139 305 301 303 344  49 781 429 376 453 213 103 191\n",
      " 406 779 687  68 745 310  22 403 580 317 728 446 563 236]\n",
      "rows to prune in layer 3 : 225\n",
      "[  1   4   6   7   8   9  10  11  13  18  19  21  22  24  26  27  32  33\n",
      "  34  36  37  40  41  45  47  49  50  51  52  53  55  56  59  60  65  66\n",
      "  70  72  75  77  80  82  84  85  88  89  90  93  94  96  98 101 102 103\n",
      " 104 105 110 112 113 114 118 122 127 129 131 136 137 139 140 141 143 145\n",
      " 146 147 150 151 152 154 155 156 157 160 162 163 165 167 168 171 172 173\n",
      " 174 175 176 177 182 186 187 189 190 192 196 199 202 203 205 206 208 210\n",
      " 213 214 219 221 222 223 226 227 229 230 232 233 234 235 236 239 240 241\n",
      " 244 249 250 254 262 264 265 266 267 268 269 270 275 276 277 278 280 281\n",
      " 284 286 290 295 296 297]\n",
      "[140 143 156 202 227 155  10 280  98 174  52 173  55 104 226 160 275 262\n",
      "  21   1  94  75 157 269 114  26 122  47  36 206 177 254 137 244 265 172\n",
      " 196  19  89 233  37 295  40 281  33 150 152 147 270  88 239  60  49  90\n",
      "  53  51  13 235   7 199 297 186 267 250 171 141 240  84 146 131  59  22\n",
      " 145 234 189  77 229 277 221 162 163 210 110 236  11 219 167 187  85 284\n",
      "  45  56 168 241 103 249 290 230 203 154 118 276 113 268 286   4  27 136\n",
      " 208 222 205  70 165  66 190  32  18 127 264   9 139 223   6 176  24  34\n",
      " 232 175  50  93 102 266 213 151  65 105  96 182  80  82 192   8  41 278\n",
      " 112 101 214 129 296  72]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 0  1  2  6  9 11 12 13 14 18 19 20 23 24 25 26 29 30 35 36 39 40 41 44\n",
      " 45 47 48 50 52 54 56 57 60 61 62 64 66 68 71 72 74 76 80 81 84 86 91 92\n",
      " 93 95]\n",
      "[84 26 62  6 47 76 19 24 13 60 50 81 56 25 68 30 93 20 44  1 36 40 57 71\n",
      " 95 91 35 18 12 54 45  9 64 72 23 11 29 52 61 14 48 92 80 86  0 74 39 66\n",
      " 41  2]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5627 - accuracy: 0.9174 - val_loss: 1.5306 - val_accuracy: 0.9394\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5198 - accuracy: 0.9504 - val_loss: 1.5202 - val_accuracy: 0.9467\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5100 - accuracy: 0.9575 - val_loss: 1.5134 - val_accuracy: 0.9532\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5044 - accuracy: 0.9624 - val_loss: 1.5109 - val_accuracy: 0.9534\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5002 - accuracy: 0.9658 - val_loss: 1.5087 - val_accuracy: 0.9551\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4969 - accuracy: 0.9685 - val_loss: 1.5048 - val_accuracy: 0.9583\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4944 - accuracy: 0.9706 - val_loss: 1.5039 - val_accuracy: 0.9595\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4923 - accuracy: 0.9723 - val_loss: 1.5041 - val_accuracy: 0.9585\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4905 - accuracy: 0.9739 - val_loss: 1.5033 - val_accuracy: 0.9594\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4888 - accuracy: 0.9755 - val_loss: 1.5021 - val_accuracy: 0.9608\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4876 - accuracy: 0.9765 - val_loss: 1.5029 - val_accuracy: 0.9590\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4866 - accuracy: 0.9772 - val_loss: 1.5027 - val_accuracy: 0.9596\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4856 - accuracy: 0.9780 - val_loss: 1.5027 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:53, 143.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 5\n",
      "rows to prune in layer 0 : 686\n",
      "[  4   5  14  15  17  19  22  23  24  30  32  40  42  45  49  50  56  59\n",
      "  61  63  66  68  70  79  86  87  89  92  97 100 103 104 105 107 108 109\n",
      " 111 117 118 120 124 127 128 130 131 132 134 135 139 140 145 150 152 154\n",
      " 155 157 162 164 177 184 191 195 205 213 221 223 225 230 232 234 236 239\n",
      " 240 248 250 251 254 256 267 273 277 284 290 296 297 301 303 305 310 311\n",
      " 317 324 334 335 337 342 344 349 350 353 354 361 363 370 371 374 376 377\n",
      " 391 392 393 396 401 403 406 407 419 420 429 430 432 433 438 443 446 447\n",
      " 448 452 453 462 465 471 474 481 484 489 491 494 495 498 501 523 533 538\n",
      " 546 552 554 563 569 570 572 580 583 584 592 601 603 608 610 612 620 627\n",
      " 629 634 645 647 648 649 650 651 652 657 659 663 677 682 687 693 694 701\n",
      " 703 716 722 723 724 727 728 731 735 742 745 751 756 759 779 781]\n",
      "[205 230  15 164 572  68 742 234 501  14 756 583  45  30 396 498  70 256\n",
      " 494 546 716  87 484 177 349 392 446 448 132 277 659 335 393 781 127 489\n",
      "  63 361 130 491 406 134 254 334 552 645 433 443 438 569 324 135 462  92\n",
      " 297 403 620 391 250  50 751 634 342  56 223   5 584 225 354 305  40 652\n",
      " 363 474 301 657 310 570 251 419 284 724 452 465 371 120 303 563 350 677\n",
      " 111 447 533 240 131 401 108 374 221  66 104 429 337 150 608  23 694  97\n",
      " 649 107 648  17  79 290  59 353 236 109 124 103 430 610 723 213 650 117\n",
      " 731 155 538   4 239 191 273  19 432 759 779 663 195 687 232 693 317 735\n",
      " 592 603 745 139 296 377 154 728 157 248 311 370 601 128 647 682  24 145\n",
      " 580 554 376  49 481  32 267 152  22  89 703 420 471 701 722 495 140  61\n",
      " 407 100 651 184 105 118 162 344 453  42  86 612 629 627 727 523]\n",
      "rows to prune in layer 3 : 262\n",
      "[  4   6   8   9  11  18  24  27  32  34  41  45  50  56  65  66  70  72\n",
      "  77  80  82  85  93  96 101 102 103 105 110 112 113 118 127 129 136 139\n",
      " 151 154 162 163 165 167 168 175 176 182 187 190 192 203 205 208 210 213\n",
      " 214 219 221 222 223 229 230 232 236 241 249 264 266 268 276 277 278 284\n",
      " 286 290 296]\n",
      "[162 290 182 210 176 221 163  93  50  11  77 236   6 154 213  45 230 127\n",
      "  70  27  80  72 296 192 113  66 203 222 277   9 286  56 118 264   4 241\n",
      "  34 110  82  24 175 165 101 249 139  18 232   8 284 102  41 190  65 205\n",
      " 276 112 208 214 187 278 103 151 219 167  96  85 136 168 129 229  32 266\n",
      " 223 268 105]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 0  2  9 11 12 14 18 23 29 35 39 41 45 48 52 54 61 64 66 72 74 80 86 91\n",
      " 92]\n",
      "[64 86 35 80  2 54 61 45 12 39 29 66 48  0 72  9 92 14 74 23 11 91 18 41\n",
      " 52]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7119 - accuracy: 0.7862 - val_loss: 1.6203 - val_accuracy: 0.8584\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6106 - accuracy: 0.8661 - val_loss: 1.5941 - val_accuracy: 0.8789\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5929 - accuracy: 0.8796 - val_loss: 1.5825 - val_accuracy: 0.8888\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5836 - accuracy: 0.8870 - val_loss: 1.5763 - val_accuracy: 0.8930\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5766 - accuracy: 0.8922 - val_loss: 1.5722 - val_accuracy: 0.8961\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5718 - accuracy: 0.8968 - val_loss: 1.5682 - val_accuracy: 0.8986\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5682 - accuracy: 0.8998 - val_loss: 1.5658 - val_accuracy: 0.9012\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5646 - accuracy: 0.9031 - val_loss: 1.5631 - val_accuracy: 0.9037\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5622 - accuracy: 0.9049 - val_loss: 1.5626 - val_accuracy: 0.9022\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5596 - accuracy: 0.9073 - val_loss: 1.5594 - val_accuracy: 0.9068\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5577 - accuracy: 0.9089 - val_loss: 1.5594 - val_accuracy: 0.9052\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5559 - accuracy: 0.9105 - val_loss: 1.5587 - val_accuracy: 0.9082\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5539 - accuracy: 0.9123 - val_loss: 1.5580 - val_accuracy: 0.9061\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5530 - accuracy: 0.9132 - val_loss: 1.5588 - val_accuracy: 0.9043\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5515 - accuracy: 0.9149 - val_loss: 1.5585 - val_accuracy: 0.9051\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5502 - accuracy: 0.9156 - val_loss: 1.5565 - val_accuracy: 0.9076\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5490 - accuracy: 0.9166 - val_loss: 1.5562 - val_accuracy: 0.9077\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5480 - accuracy: 0.9173 - val_loss: 1.5573 - val_accuracy: 0.9057\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5469 - accuracy: 0.9184 - val_loss: 1.5545 - val_accuracy: 0.9096\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5461 - accuracy: 0.9196 - val_loss: 1.5554 - val_accuracy: 0.9064\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5449 - accuracy: 0.9201 - val_loss: 1.5531 - val_accuracy: 0.9095\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5443 - accuracy: 0.9211 - val_loss: 1.5540 - val_accuracy: 0.9098\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5438 - accuracy: 0.9210 - val_loss: 1.5523 - val_accuracy: 0.9101\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5433 - accuracy: 0.9216 - val_loss: 1.5528 - val_accuracy: 0.9108\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5423 - accuracy: 0.9227 - val_loss: 1.5528 - val_accuracy: 0.9089\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5418 - accuracy: 0.9229 - val_loss: 1.5514 - val_accuracy: 0.9110\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5411 - accuracy: 0.9238 - val_loss: 1.5526 - val_accuracy: 0.9093\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5405 - accuracy: 0.9240 - val_loss: 1.5524 - val_accuracy: 0.9099\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5403 - accuracy: 0.9245 - val_loss: 1.5521 - val_accuracy: 0.9098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:50, 153.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 5\n",
      "rows to prune in layer 0 : 735\n",
      "[  4  17  19  22  23  24  32  42  49  59  61  66  79  86  89  97 100 103\n",
      " 104 105 107 109 117 118 124 128 139 140 145 150 152 154 155 157 162 184\n",
      " 191 195 213 221 232 236 239 248 267 273 290 296 311 317 337 344 353 370\n",
      " 376 377 407 420 429 430 432 453 471 481 495 523 538 554 580 592 601 603\n",
      " 608 610 612 627 629 647 648 649 650 651 663 682 687 693 694 701 703 722\n",
      " 723 727 728 731 735 745 759 779]\n",
      "[104 213 139 152 107 554 610 430 759 731 377 629 407  49 703 155 481 150\n",
      " 471  17 162 429   4 523 612 376 647 580 124 687  86 663  42 236  61 353\n",
      " 682  59 145 296 317  97 290 232 779 592 453 273 105  23 157  22 693  32\n",
      " 649 267 420  79 650 627 722 248 128 694 495 239 195 100 538 221  24 745\n",
      " 154 728 140 735 651 109 701 432 118 191 603 344 184  66  19 337 117 103\n",
      " 648 727 723 370 608 601 311  89]\n",
      "rows to prune in layer 3 : 281\n",
      "[  8  18  24  32  41  65  82  85  96 101 102 103 105 110 112 129 136 139\n",
      " 151 165 167 168 175 187 190 205 208 214 219 223 229 232 249 266 268 276\n",
      " 278 284]\n",
      "[168 112 105  18  96  85 205 101  24 165 102 284 110 232  32 276  41 175\n",
      " 249 167 208 223 129  82 136 151 229 139  65   8 214 219 268 103 190 278\n",
      " 266 187]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 0  9 11 14 18 23 41 48 52 72 74 91 92]\n",
      "[74 52 48 41  9 91 18 14 11 23 92 72  0]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9870 - accuracy: 0.4879 - val_loss: 1.9011 - val_accuracy: 0.5737\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8828 - accuracy: 0.5881 - val_loss: 1.8740 - val_accuracy: 0.5934\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8529 - accuracy: 0.6162 - val_loss: 1.8389 - val_accuracy: 0.6317\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8253 - accuracy: 0.6426 - val_loss: 1.8221 - val_accuracy: 0.6435\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8122 - accuracy: 0.6542 - val_loss: 1.8121 - val_accuracy: 0.6539\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8042 - accuracy: 0.6614 - val_loss: 1.8050 - val_accuracy: 0.6594\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7981 - accuracy: 0.6676 - val_loss: 1.7998 - val_accuracy: 0.6629\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7929 - accuracy: 0.6725 - val_loss: 1.7963 - val_accuracy: 0.6661\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7810 - accuracy: 0.6862 - val_loss: 1.7722 - val_accuracy: 0.6960\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7618 - accuracy: 0.7056 - val_loss: 1.7613 - val_accuracy: 0.7075\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7541 - accuracy: 0.7126 - val_loss: 1.7565 - val_accuracy: 0.7087\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7497 - accuracy: 0.7168 - val_loss: 1.7524 - val_accuracy: 0.7127\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7463 - accuracy: 0.7194 - val_loss: 1.7505 - val_accuracy: 0.7155\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7436 - accuracy: 0.7227 - val_loss: 1.7485 - val_accuracy: 0.7179\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7415 - accuracy: 0.7239 - val_loss: 1.7459 - val_accuracy: 0.7172\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7393 - accuracy: 0.7266 - val_loss: 1.7444 - val_accuracy: 0.7197\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7376 - accuracy: 0.7283 - val_loss: 1.7429 - val_accuracy: 0.7202\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7361 - accuracy: 0.7297 - val_loss: 1.7441 - val_accuracy: 0.7179\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7347 - accuracy: 0.7305 - val_loss: 1.7393 - val_accuracy: 0.7255\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7332 - accuracy: 0.7317 - val_loss: 1.7409 - val_accuracy: 0.7223\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7319 - accuracy: 0.7328 - val_loss: 1.7386 - val_accuracy: 0.7258\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7309 - accuracy: 0.7341 - val_loss: 1.7396 - val_accuracy: 0.7241\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7296 - accuracy: 0.7352 - val_loss: 1.7378 - val_accuracy: 0.7239\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7290 - accuracy: 0.7352 - val_loss: 1.7359 - val_accuracy: 0.7274\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7279 - accuracy: 0.7367 - val_loss: 1.7370 - val_accuracy: 0.7245\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7271 - accuracy: 0.7375 - val_loss: 1.7361 - val_accuracy: 0.7264\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7263 - accuracy: 0.7379 - val_loss: 1.7352 - val_accuracy: 0.7268\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7256 - accuracy: 0.7384 - val_loss: 1.7344 - val_accuracy: 0.7284\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7247 - accuracy: 0.7398 - val_loss: 1.7340 - val_accuracy: 0.7289\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7242 - accuracy: 0.7399 - val_loss: 1.7336 - val_accuracy: 0.7291\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7236 - accuracy: 0.7402 - val_loss: 1.7336 - val_accuracy: 0.7285\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7229 - accuracy: 0.7416 - val_loss: 1.7316 - val_accuracy: 0.7314\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7225 - accuracy: 0.7417 - val_loss: 1.7326 - val_accuracy: 0.7268\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7220 - accuracy: 0.7416 - val_loss: 1.7318 - val_accuracy: 0.7297\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7213 - accuracy: 0.7425 - val_loss: 1.7327 - val_accuracy: 0.7287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:35, 157.06s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 5\n",
      "rows to prune in layer 0 : 759\n",
      "[ 19  22  23  24  32  66  79  89 100 103 109 117 118 128 140 154 157 184\n",
      " 191 195 221 239 248 267 311 337 344 370 420 432 495 538 601 603 608 627\n",
      " 648 649 650 651 693 694 701 722 723 727 728 735 745]\n",
      "[103 432 651 701 117 601 239 722 140 184 420 649 745  89 495 311 100 694\n",
      " 154 221 648 538 723 693 650 248 727 728  66 370  24 191 157  19  32 109\n",
      " 267 128 735 195 608  22  79 344 603 118 337 627  23]\n",
      "rows to prune in layer 3 : 290\n",
      "[  8  65  82 103 129 136 139 151 167 187 190 208 214 219 223 229 266 268\n",
      " 278]\n",
      "[ 65 187 266 268 167 278 103 219 151 208 190 223 214   8  82 229 136 139\n",
      " 129]\n",
      "rows to prune in layer 6 : 96\n",
      "[ 0 11 14 18 23 72 92]\n",
      "[92 14 11 18 23 72  0]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2231 - accuracy: 0.2280 - val_loss: 2.1851 - val_accuracy: 0.2755\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1619 - accuracy: 0.2950 - val_loss: 2.1282 - val_accuracy: 0.3282\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1143 - accuracy: 0.3462 - val_loss: 2.1174 - val_accuracy: 0.3403\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1043 - accuracy: 0.3557 - val_loss: 2.0961 - val_accuracy: 0.3630\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0833 - accuracy: 0.3797 - val_loss: 2.0822 - val_accuracy: 0.3790\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0753 - accuracy: 0.3854 - val_loss: 2.0761 - val_accuracy: 0.3821\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0693 - accuracy: 0.3923 - val_loss: 2.0693 - val_accuracy: 0.3949\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0639 - accuracy: 0.3996 - val_loss: 2.0649 - val_accuracy: 0.3962\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0598 - accuracy: 0.4036 - val_loss: 2.0631 - val_accuracy: 0.3998\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0569 - accuracy: 0.4053 - val_loss: 2.0606 - val_accuracy: 0.3991\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0535 - accuracy: 0.4086 - val_loss: 2.0587 - val_accuracy: 0.4027\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0512 - accuracy: 0.4106 - val_loss: 2.0566 - val_accuracy: 0.4033\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0488 - accuracy: 0.4130 - val_loss: 2.0541 - val_accuracy: 0.4067\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0470 - accuracy: 0.4142 - val_loss: 2.0531 - val_accuracy: 0.4057\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0447 - accuracy: 0.4158 - val_loss: 2.0494 - val_accuracy: 0.4095\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0426 - accuracy: 0.4182 - val_loss: 2.0465 - val_accuracy: 0.4136\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0408 - accuracy: 0.4193 - val_loss: 2.0460 - val_accuracy: 0.4130\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0396 - accuracy: 0.4193 - val_loss: 2.0474 - val_accuracy: 0.4109\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0385 - accuracy: 0.4214 - val_loss: 2.0441 - val_accuracy: 0.4164\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0374 - accuracy: 0.4223 - val_loss: 2.0417 - val_accuracy: 0.4170\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0362 - accuracy: 0.4230 - val_loss: 2.0397 - val_accuracy: 0.4186\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0349 - accuracy: 0.4243 - val_loss: 2.0372 - val_accuracy: 0.4223\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0341 - accuracy: 0.4255 - val_loss: 2.0372 - val_accuracy: 0.4219\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0328 - accuracy: 0.4263 - val_loss: 2.0359 - val_accuracy: 0.4239\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0321 - accuracy: 0.4266 - val_loss: 2.0344 - val_accuracy: 0.4254\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0313 - accuracy: 0.4284 - val_loss: 2.0340 - val_accuracy: 0.4239\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0305 - accuracy: 0.4287 - val_loss: 2.0339 - val_accuracy: 0.4247\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0298 - accuracy: 0.4285 - val_loss: 2.0331 - val_accuracy: 0.4256\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0293 - accuracy: 0.4294 - val_loss: 2.0323 - val_accuracy: 0.4264\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0288 - accuracy: 0.4302 - val_loss: 2.0314 - val_accuracy: 0.4277\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0280 - accuracy: 0.4308 - val_loss: 2.0312 - val_accuracy: 0.4305\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0274 - accuracy: 0.4318 - val_loss: 2.0309 - val_accuracy: 0.4274\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0272 - accuracy: 0.4314 - val_loss: 2.0307 - val_accuracy: 0.4287\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0267 - accuracy: 0.4317 - val_loss: 2.0287 - val_accuracy: 0.4295\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0266 - accuracy: 0.4316 - val_loss: 2.0297 - val_accuracy: 0.4281\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0260 - accuracy: 0.4326 - val_loss: 2.0310 - val_accuracy: 0.4290\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0256 - accuracy: 0.4327 - val_loss: 2.0296 - val_accuracy: 0.4289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [15:24, 160.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 5\n",
      "rows to prune in layer 0 : 771\n",
      "[ 19  22  23  24  32  66  79 109 118 128 157 191 195 248 267 337 344 370\n",
      " 603 608 627 650 727 728 735]\n",
      "[603  19 118 627  23 337 344 248 728 608  32 128 191 650 267 195 109 727\n",
      " 157  79 370  22  24  66 735]\n",
      "rows to prune in layer 3 : 295\n",
      "[  8  82 129 136 139 190 208 214 223 229]\n",
      "[214 139  82 136 229   8 208 129 223 190]\n",
      "rows to prune in layer 6 : 98\n",
      "[ 0 18 23 72]\n",
      "[23  0 72 18]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2722 - accuracy: 0.1633 - val_loss: 2.2500 - val_accuracy: 0.1891\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2332 - accuracy: 0.2055 - val_loss: 2.2241 - val_accuracy: 0.2133\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2196 - accuracy: 0.2397 - val_loss: 2.2140 - val_accuracy: 0.2452\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2128 - accuracy: 0.2471 - val_loss: 2.2069 - val_accuracy: 0.2471\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2035 - accuracy: 0.2658 - val_loss: 2.1967 - val_accuracy: 0.2694\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1947 - accuracy: 0.2711 - val_loss: 2.1896 - val_accuracy: 0.2719\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1894 - accuracy: 0.2747 - val_loss: 2.1854 - val_accuracy: 0.2706\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1859 - accuracy: 0.2722 - val_loss: 2.1818 - val_accuracy: 0.2777\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1810 - accuracy: 0.2766 - val_loss: 2.1782 - val_accuracy: 0.2810\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1784 - accuracy: 0.2809 - val_loss: 2.1763 - val_accuracy: 0.2754\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1767 - accuracy: 0.2804 - val_loss: 2.1751 - val_accuracy: 0.2754\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1749 - accuracy: 0.2828 - val_loss: 2.1737 - val_accuracy: 0.2923\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1733 - accuracy: 0.2821 - val_loss: 2.1726 - val_accuracy: 0.2900\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1716 - accuracy: 0.2851 - val_loss: 2.1710 - val_accuracy: 0.2908\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1702 - accuracy: 0.2850 - val_loss: 2.1700 - val_accuracy: 0.2933\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1690 - accuracy: 0.2853 - val_loss: 2.1689 - val_accuracy: 0.2922\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1680 - accuracy: 0.2869 - val_loss: 2.1683 - val_accuracy: 0.2871\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1672 - accuracy: 0.2873 - val_loss: 2.1676 - val_accuracy: 0.2950\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1666 - accuracy: 0.2876 - val_loss: 2.1669 - val_accuracy: 0.2944\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1659 - accuracy: 0.2878 - val_loss: 2.1663 - val_accuracy: 0.2951\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1655 - accuracy: 0.2867 - val_loss: 2.1656 - val_accuracy: 0.2954\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1651 - accuracy: 0.2884 - val_loss: 2.1655 - val_accuracy: 0.2949\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1645 - accuracy: 0.2889 - val_loss: 2.1650 - val_accuracy: 0.2937\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1642 - accuracy: 0.2886 - val_loss: 2.1647 - val_accuracy: 0.2962\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1638 - accuracy: 0.2887 - val_loss: 2.1646 - val_accuracy: 0.2894\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1635 - accuracy: 0.2896 - val_loss: 2.1641 - val_accuracy: 0.2960\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1632 - accuracy: 0.2896 - val_loss: 2.1637 - val_accuracy: 0.2949\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1628 - accuracy: 0.2905 - val_loss: 2.1635 - val_accuracy: 0.2958\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1625 - accuracy: 0.2905 - val_loss: 2.1636 - val_accuracy: 0.2962\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1622 - accuracy: 0.2903 - val_loss: 2.1642 - val_accuracy: 0.2949\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1620 - accuracy: 0.2897 - val_loss: 2.1630 - val_accuracy: 0.2939\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1616 - accuracy: 0.2908 - val_loss: 2.1624 - val_accuracy: 0.2974\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1611 - accuracy: 0.2916 - val_loss: 2.1625 - val_accuracy: 0.2968\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1607 - accuracy: 0.2917 - val_loss: 2.1615 - val_accuracy: 0.2969\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1604 - accuracy: 0.2919 - val_loss: 2.1619 - val_accuracy: 0.2951\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1604 - accuracy: 0.2912 - val_loss: 2.1611 - val_accuracy: 0.2944\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1601 - accuracy: 0.2920 - val_loss: 2.1608 - val_accuracy: 0.2957\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1597 - accuracy: 0.2923 - val_loss: 2.1612 - val_accuracy: 0.2967\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1592 - accuracy: 0.2922 - val_loss: 2.1603 - val_accuracy: 0.2905\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1586 - accuracy: 0.2932 - val_loss: 2.1587 - val_accuracy: 0.3013\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1578 - accuracy: 0.2943 - val_loss: 2.1577 - val_accuracy: 0.2977\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1572 - accuracy: 0.2948 - val_loss: 2.1572 - val_accuracy: 0.3010\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1568 - accuracy: 0.2947 - val_loss: 2.1569 - val_accuracy: 0.2994\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1565 - accuracy: 0.2944 - val_loss: 2.1565 - val_accuracy: 0.3013\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1562 - accuracy: 0.2948 - val_loss: 2.1563 - val_accuracy: 0.3014\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1558 - accuracy: 0.2955 - val_loss: 2.1557 - val_accuracy: 0.3011\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1552 - accuracy: 0.2974 - val_loss: 2.1553 - val_accuracy: 0.3025\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1547 - accuracy: 0.2985 - val_loss: 2.1548 - val_accuracy: 0.3039\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1542 - accuracy: 0.2992 - val_loss: 2.1549 - val_accuracy: 0.3034\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1538 - accuracy: 0.2998 - val_loss: 2.1543 - val_accuracy: 0.3048\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1532 - accuracy: 0.3005 - val_loss: 2.1546 - val_accuracy: 0.3057\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1531 - accuracy: 0.3002 - val_loss: 2.1542 - val_accuracy: 0.3044\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1528 - accuracy: 0.3003 - val_loss: 2.1544 - val_accuracy: 0.3030\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1526 - accuracy: 0.3000 - val_loss: 2.1534 - val_accuracy: 0.3042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1523 - accuracy: 0.3006 - val_loss: 2.1535 - val_accuracy: 0.3032\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1521 - accuracy: 0.3004 - val_loss: 2.1540 - val_accuracy: 0.3039\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1519 - accuracy: 0.3009 - val_loss: 2.1531 - val_accuracy: 0.3047\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1520 - accuracy: 0.3005 - val_loss: 2.1531 - val_accuracy: 0.3045\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1516 - accuracy: 0.3012 - val_loss: 2.1529 - val_accuracy: 0.3032\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1515 - accuracy: 0.3005 - val_loss: 2.1525 - val_accuracy: 0.3030\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1514 - accuracy: 0.3014 - val_loss: 2.1527 - val_accuracy: 0.3063\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1512 - accuracy: 0.3018 - val_loss: 2.1527 - val_accuracy: 0.3039\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1511 - accuracy: 0.3008 - val_loss: 2.1524 - val_accuracy: 0.3057\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1511 - accuracy: 0.3011 - val_loss: 2.1521 - val_accuracy: 0.3044\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1511 - accuracy: 0.3011 - val_loss: 2.1530 - val_accuracy: 0.3066\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1508 - accuracy: 0.3013 - val_loss: 2.1516 - val_accuracy: 0.3054\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1505 - accuracy: 0.3016 - val_loss: 2.1514 - val_accuracy: 0.3052\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1507 - accuracy: 0.3016 - val_loss: 2.1517 - val_accuracy: 0.3066\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1504 - accuracy: 0.3012 - val_loss: 2.1514 - val_accuracy: 0.3053\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1504 - accuracy: 0.3024 - val_loss: 2.1523 - val_accuracy: 0.3065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [18:31, 158.78s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [1:51:27<1:13:57, 1109.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5792 - accuracy: 0.8960 - val_loss: 1.5205 - val_accuracy: 0.9443\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 6\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[340 547 463 406 658 530  63 247 134 774 465 210 437 715 393 257 721 526\n",
      " 543 701 235 149  87 396 283 695 427 574 420  26 242 363 362 422 497 192\n",
      "  91 332  40 220 312 555 141 375 769 407 677 746 237 125 101 727 389 303\n",
      " 379 651 627 415 609 601 519 367 219 202  13 703 246 343 612 436 502 243\n",
      "  59 423  47 663 768 451 705 592 458 711 572  70 771 460 761 358  60 586\n",
      " 249 689 482 386 596 693 286 522  53   3  86 480 424 327   0 576 318 378\n",
      " 561 690 602 185  49 591 395 661 208 259 659 588  24 338 714 579 467 688\n",
      " 682 191 431 128 239 545 747 226 385  51 158 292 366 696 218  30 494  88\n",
      " 148 488 381 409 539 231 311 599 619 121 578 642 605 730 510 117 736 598\n",
      " 419  65  69  12 595 321 265 650 310 178 400 492 757 672 686 507 329 552\n",
      " 759   9 644  17 258 342 593 607 745 557 319 293 368  25 382 285 495 284\n",
      " 782 564 156 755 733 429 611 617 355 103 314 762 567  95 779 614 604 277\n",
      " 668 130 626 205 479 391 720 665 621  38 697  27  72 352 474 615 749 377\n",
      " 449 280 120 240 269 298  31  96 397 302  94 620 481 542 108 216 341 675\n",
      "  77 533  76 198  20 207  10 489 336 633 304 641 669 476  42 177 432 296\n",
      " 223 135 560 398 360 447 417 175 652 122 671 450 217 685 182 466 780 282\n",
      " 472 335  33 289  37 691 221 387 756 523 583 616 418 777 713 410 209 674\n",
      " 228 383 487 559 698 434 323  66   5  78 230 183 142 136 550 707 294 546\n",
      " 348 503 473 236 624 189  64 628 295 428 425 376 439  35 204 660 625 238\n",
      " 349 388 129  15 699 151 724 477   4 245 163 471 339 635 528 622 687 581\n",
      " 772 734  55 532 535 288 613 524 634 138 113 670 504 320 110 679  34 726\n",
      " 770 584 573   1 662 490  97 333 306 255 608 300 499 594 316 500 709 743\n",
      " 186 719 505 252 531 442 299  90 783 725 700 119 776 405 667 683 444  92\n",
      " 454 433 655 421  82 160 229 401 215 647  68 233 248 752 742 456 315 356\n",
      " 648 717 146 392 266  57 563 187 775 404 664 374 225 737 740 273 271 250\n",
      "   7 518  19 781 380 262 263 309 331 244 171 411 731 529 326  99 162 371\n",
      "   6 180  54 618  14   8 413 435 166 537 241 657 636 394 184 443 256 729\n",
      " 153 253  58 575 462 637 270 203 631 167 525 373 234 159 140 773  89 538\n",
      " 173 157 150 196 172 452 632 414  28 448 516 307 580 197 127 654 102 493\n",
      " 354 131 100 457 322 334 328 520 750 629 453 190 694 224 646 174 137 214\n",
      " 478 305 206 213 154 445 297 317 748 165 491 521 515 764 188 544 565 164\n",
      " 606 549 511 345 678  11 548 570 566 741 568 199 281 287 212 353 139 402\n",
      " 426  74 344 438 571 610 723 145 105 116 152 168  36 430  21 264 227 666\n",
      " 589  81 330  93 200 112 194 758  23 361 496 751  46 390 313  50 440 441\n",
      " 351  73 527 778 484 267 169 708 201 274  44  62 718 514 554 109   2 254\n",
      " 569 739 673 744 459 469 597 124 585 308 324 412 104 767  71 486 372 684\n",
      "  75 115 738  83 536  43 498 702  67 577 732 653 193 512 275 114 279 384\n",
      "  29  39  18 551  52 638 722  61 475 232 765 541 692 357 370 587 268 147\n",
      "  22 639 222 347 508 553 623  56 534 276 408 681 144 261 106  98 753 446\n",
      " 600 291 710 123 301 369 676 176 517 278 562 337 716 656 485  41 509 346\n",
      " 645 754  48 107  80 161 211 170 556 763 649 181 706 272 364  32 603 290\n",
      " 251 506  45 513 399 760 501 735 483 260 540 643 640 133 582 416 468 712\n",
      "  84 195 455 132 590 118  16 461 365 143 470 464 350 155 403  79 766 126\n",
      " 704 728 179  85 558 680 359 630 111 325]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[143  54 174  19 265  29 134 107  43 177 248  10 158 203 218  96 283   1\n",
      " 133 199 189   2 125  78   5  44 159 207 142 181 254 282 242 186  12  59\n",
      " 224  76  77 264 149 243 152  58 172   3 236 155 126 209 102 252  72 217\n",
      " 150 229 261 195 124 266 287 240 170  63 245 127  15 176 233 115 280 162\n",
      " 295  70  64  98 167 119  80  23 121  81 184  67  24 160  41 272 157  82\n",
      " 156  91 179 139   9 105 151  36 286 299   0 182 135 259 257 116 225  93\n",
      "  71  49  50 103  74   8 204 113 138 268  51  73  48 230 256 173  55 214\n",
      "  61 275  57 232 132  45 109  79 297 226 163  88 144 101 298  52 161  28\n",
      "  84 262 175 210  32 104 100  34 140 193  20 168  25 196  62  21 285 137\n",
      "  68 198 228  94  22  16 296 141  33 294 169 110 117 289 166  47 253 122\n",
      " 271 263  69  89 201 120 281  31 118 291 212 114 260  86 244 258 165  38\n",
      " 234 219 171 191 227  37 270 136  26 112  85  13 197 192 279 106  65 145\n",
      "  75 131 277 235  53 221  27 129 237 183 276 216 190 146  40 269 288 180\n",
      " 200 205 278  14 241 148  92  66 292 255  39 273  97  11 284  83   6 111\n",
      " 215 154 223 108 247 130 274  46 202 267 185 293 147 239  42 178 211 251\n",
      " 220 123   7 249 231 153  56   4  87  90 290 188 164  18 246  99  17  95\n",
      " 213  60  30 128 222 206 208 250  35 238 187 194]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[59 15 39 97 75  6 36 55 42 16 50 79 60 76 20 81 51  1 43 98 99 80 46 14\n",
      "  0 88  9 35 18 57 85 95 56 91 41 92 11 63 28 62 87 73 96 53 64 94  3 67\n",
      " 27 52 38 40 93 78 71 69 29 22  4 65  7 83 21 89 77 47 48 54 26 19 10 72\n",
      " 84 82 12 13 45 33  8  2 61 17 23  5 44 25 24 58 49 74 32 90 70 31 34 37\n",
      " 86 66 30 68]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5119 - accuracy: 0.9523 - val_loss: 1.5057 - val_accuracy: 0.9576\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4982 - accuracy: 0.9644 - val_loss: 1.4961 - val_accuracy: 0.9661\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4912 - accuracy: 0.9713 - val_loss: 1.4916 - val_accuracy: 0.9715\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4864 - accuracy: 0.9760 - val_loss: 1.4877 - val_accuracy: 0.9744\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4836 - accuracy: 0.9782 - val_loss: 1.4871 - val_accuracy: 0.9744\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4810 - accuracy: 0.9807 - val_loss: 1.4900 - val_accuracy: 0.9717\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4793 - accuracy: 0.9822 - val_loss: 1.4860 - val_accuracy: 0.9753\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4773 - accuracy: 0.9842 - val_loss: 1.4846 - val_accuracy: 0.9775\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4759 - accuracy: 0.9856 - val_loss: 1.4876 - val_accuracy: 0.9743\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4755 - accuracy: 0.9860 - val_loss: 1.4853 - val_accuracy: 0.9757\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4752 - accuracy: 0.9860 - val_loss: 1.4852 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:20, 140.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 6\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[238 225 735 406 642 439 537 168 424 269  55 325 380 624 743  89 727  71\n",
      " 316 702 379 780 271 273 211 747 410 477 661 268 132 590 317  79 284 499\n",
      " 286 321 207 149 722 386 472 459 777 395 300  38 365 651 167 692 258 708\n",
      " 108 141  90 510 603 715 620 127 526 266 123 147 701 299 353 573 721 201\n",
      " 689 471 343 402 686 709 484 457  18  15 562 718 524 744 446 697 242 536\n",
      " 383 749 713 548 233 199 416 614 567 179 231 585 600  72 366 409 137 544\n",
      " 349  82 435 110 124 723 503 275 182 673 564 659 511 102  85  57  34 582\n",
      "  67 688 707 691 778 303  68 188 162 197 449 563 253 551 398 148 250  21\n",
      " 771 274 746 191 362 491 229 357 671 194 634 430 530 729 151 607 568 586\n",
      " 327 752 121 429 674 630 348 236 157 304 670 106 623 403  46  31 497 554\n",
      "  77  74 177 405  12 245 198 629  76 666 350 588 725 561 368 377 367 566\n",
      " 461 116 296 264  45 378 393 408 222 294 355 347 156  75 454 730 760 309\n",
      "   8 700  59  42 677   3 155 104  73 574 703 552 509 581 427  36 234 487\n",
      " 732 516 169 636 113 417  91  84 485 766 762 209 589 717 699 129 385 210\n",
      " 645 687 474 237 684 438 575 647 290 413 609  66 221   0 475 523 458 737\n",
      " 420 103 434 667 272 394 270 289 506  26 364 292 352 315 592 313 412 514\n",
      " 734 500 587 428 627 782 332 719 447 644 407 665 637 119 512 115 144 166\n",
      " 277   2 307 576 641 604 597 276 218 556 602 733 329 404 617  48 646 387\n",
      " 161 328 493 418 359 517 189 330 442 376 658 281 399  40 243 660 384 400\n",
      " 230 326 105 196 469 764 344 287 252 534 525 706 170 572 558 736 741 769\n",
      " 337 215 577 704 437 676 314 738 480 593  41  39 109 391 180 361  81  20\n",
      " 128  53 356 571 308 401 285 612 482 443 206 519 134 768 656 501 310 135\n",
      " 508  87 606 112 759 672 175 173  14 502  17 254 217   5 301  98  96 195\n",
      " 117 185 580   1 423 616 601 779 463 440 160 711  11 611 397 125 553 323\n",
      " 248 336 345 190 358 465 131 494 655  63 244 529  70 339 455 341 226 282\n",
      "  92   9  44 419  62 227 724 145 628 101 370 293 260 265 610 694 466 331\n",
      " 643 631 100 256 390 414 579 213 181 176 333 633 448 153 193 159  65 251\n",
      "  56  86 598 716  29 202 298  37  50 415 513 462 720  24 322 756 763 133\n",
      " 421 369 774 683 208 599 754  99 240 255 204 473 649 541 371 120 140 320\n",
      " 783 648 615  49 372  16 540 464 622 657 531  25 745 542 742  51 396 267\n",
      " 432  80 559 698 452 761 241 216 263 470 118  58 678 755 342 535 445 669\n",
      " 775 389 306 478 235 150 257 638 520  52 626 187 107 203 664 205 246 498\n",
      " 441 375   4 360 613 363 668 436 305  35 490 388 654 224 178 138  95 625\n",
      " 765  83 280 740 549 639 751 183  54  23 319 232 453  33   7 111 259  60\n",
      " 565 318 714 533 158 163 142 767 488 757 690  97 569 739 776  28 422  93\n",
      " 527 444 338 489 279 650 594 750 278 262 640 334 680 682 753 504 186 152\n",
      " 693 758 557 220 164 130 496 381 346  69 632 136 483 560 481  64 748 114\n",
      " 426 291 635 283 460 543 324  19 171 451 122  78 539 223 146 425 515 450\n",
      " 139 595 695 214 662 621  32 522 486  22 126 675  47 431 297 492 247 311\n",
      " 239 228 143 521 570 685 154 696 476 731 679 495 773 302 652  30 165 591\n",
      " 781 583 705 411  61 726 681 312 184 261 605 505 351 518 507 547 619 456\n",
      "  43 335 712 295 584 618 728 608 538 219 374 354 532 249  27  10 550 528\n",
      " 467 172 373 288 174 433  13 200  94   6 382 392  88 555 545 212 663 468\n",
      " 653 770 546 340 479 710 596 772 578 192]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[138  30 289 203 191 105 204 226 208 153 179   9  96 112 123  46 274  45\n",
      " 150  39 131   2 286 252  89  53 115  21   5 227 224 126 248 181  40 234\n",
      "  69 282  41 130 187 137 125 163 148 177 225  64 277  71  99 280  42 182\n",
      " 166 214 124  23 268  59 184 159  81  28 215  82 178  73  36 158   6  43\n",
      "  37 228 223 157  57 238 202 144  78 154 176 283 133  16 219  98 135  93\n",
      " 120 264 134  95  27 111  20  70 209  48  86 251 297 139 108 155 241   7\n",
      " 128 198 201 261  67 109 118 218 180 233 192 260 213  17  60 132 237 216\n",
      "  19 200 230 267  24  35  62 221 147 220 291 281 102 287 173  85  72 152\n",
      " 298 292 142  92 249  79 146 188   1 127 168 217 207  88 156 161 231  14\n",
      "  12  52  55 140 162 172 262 174   8 136  56 263  10  13 143 205 258 271\n",
      "  97 269  32 222 160  83 242  63  66 114  77 265 171 257  25  50 243 145\n",
      " 244  61 273  44 119 276 293 275 101  15  29  75 246 254 116  76 235 236\n",
      " 170 189 122 100  49  47 107 278   3 272  58  68 196  18 210 232 167   0\n",
      "   4 103 290 129 190  51  26 255  11  54 279 284 110 240  94 199 141 185\n",
      " 256 195 211 250  22 194 149 247 164  74 121  90 270 165 229 259 212 285\n",
      " 266 294 299  87  38 113 169  91 151 186 296 253 245 288 175  31  84 197\n",
      "  33 104  65 295 183 239 206  80 117 193  34 106]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[51 23 16 49 15 21 28 54 75 14 69 91 58 77 13  9 52 10 96 50 32 17  1 79\n",
      " 24 73 72 33 76 39 83 86 31 87  3 97 78 82  6 26 68 43 20 36 35 40 66 95\n",
      "  0 63 25 99 48  8 92 37 38 65 27 90 60 41 44 62 85 19 80 29 71 34  2 89\n",
      " 53 57 61 84 30 88 74 59 94 47 12 11 93 81 56 64  7  5 42 22 70  4 45 55\n",
      " 67 98 18 46]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5118 - accuracy: 0.9602 - val_loss: 1.5005 - val_accuracy: 0.9658\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4892 - accuracy: 0.9761 - val_loss: 1.4963 - val_accuracy: 0.9671\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.9812 - val_loss: 1.4937 - val_accuracy: 0.9694\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4799 - accuracy: 0.9839 - val_loss: 1.4913 - val_accuracy: 0.9708\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4777 - accuracy: 0.9854 - val_loss: 1.4897 - val_accuracy: 0.9731\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4761 - accuracy: 0.9869 - val_loss: 1.4884 - val_accuracy: 0.9737\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4746 - accuracy: 0.9880 - val_loss: 1.4954 - val_accuracy: 0.9661\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4742 - accuracy: 0.9881 - val_loss: 1.4886 - val_accuracy: 0.9736\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4735 - accuracy: 0.9887 - val_loss: 1.4874 - val_accuracy: 0.9733\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4720 - accuracy: 0.9900 - val_loss: 1.4903 - val_accuracy: 0.9708\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.9902 - val_loss: 1.4912 - val_accuracy: 0.9705\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4711 - accuracy: 0.9910 - val_loss: 1.4868 - val_accuracy: 0.9744\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4707 - accuracy: 0.9912 - val_loss: 1.4873 - val_accuracy: 0.9745\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4707 - accuracy: 0.9911 - val_loss: 1.4868 - val_accuracy: 0.9750\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4700 - accuracy: 0.9916 - val_loss: 1.4861 - val_accuracy: 0.9759\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4699 - accuracy: 0.9918 - val_loss: 1.4860 - val_accuracy: 0.9754\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4694 - accuracy: 0.9923 - val_loss: 1.4861 - val_accuracy: 0.9758\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4696 - accuracy: 0.9921 - val_loss: 1.4887 - val_accuracy: 0.9729\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4691 - accuracy: 0.9924 - val_loss: 1.4915 - val_accuracy: 0.9698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:48, 142.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 6\n",
      "rows to prune in layer 0 : 588\n",
      "[  1   4   5   6   7   9  10  11  13  14  16  17  19  22  23  24  25  27\n",
      "  28  29  30  32  33  35  37  43  44  47  49  50  51  52  54  56  58  60\n",
      "  61  62  63  64  65  69  70  78  80  83  86  87  88  92  93  94  95  96\n",
      "  97  98  99 100 101 107 111 112 114 117 118 120 122 125 126 130 131 133\n",
      " 135 136 138 139 140 142 143 145 146 150 152 153 154 158 159 160 163 164\n",
      " 165 171 172 173 174 175 176 178 181 183 184 185 186 187 190 192 193 195\n",
      " 200 202 203 204 205 208 212 213 214 216 217 219 220 223 224 226 227 228\n",
      " 232 235 239 240 241 244 246 247 248 249 251 254 255 256 257 259 260 261\n",
      " 262 263 265 267 278 279 280 282 283 288 291 293 295 297 298 301 302 305\n",
      " 306 310 311 312 318 319 320 322 323 324 331 333 334 335 336 338 339 340\n",
      " 341 342 345 346 351 354 358 360 363 369 370 371 372 373 374 375 381 382\n",
      " 388 389 390 392 396 397 411 414 415 419 421 422 423 425 426 431 432 433\n",
      " 436 440 441 444 445 448 450 451 452 453 455 456 460 462 463 464 465 466\n",
      " 467 468 470 473 476 478 479 481 483 486 488 489 490 492 494 495 496 498\n",
      " 501 502 504 505 507 508 513 515 518 520 521 522 527 528 529 531 532 533\n",
      " 535 538 539 540 541 542 543 545 546 547 549 550 553 555 557 559 560 565\n",
      " 569 570 578 579 580 583 584 591 594 595 596 598 599 601 605 606 608 610\n",
      " 611 613 615 616 618 619 621 622 625 626 628 631 632 633 635 638 639 640\n",
      " 643 648 649 650 652 653 654 655 656 657 662 663 664 668 669 672 675 678\n",
      " 679 680 681 682 683 685 690 693 694 695 696 698 705 710 711 712 714 716\n",
      " 720 724 726 728 731 739 740 742 745 748 750 751 753 754 755 756 757 758\n",
      " 759 761 763 765 767 770 772 773 774 775 776 779 781 783]\n",
      "[570 610 628 547 580 254 158 363 152 373  27 779 422  28 436 605  80  11\n",
      " 301  61 712 535 262  98 171  14 249 195 154 711 598 354 507 678 346 302\n",
      "  96 757 553 550 742 463 448 255 652 396 334  88 425 601 248 781   7 470\n",
      " 421 382 773 663 186 232 358 142 640 310  92 335 656 490 655 648 135 208\n",
      " 381 559 716 319 145 244 515 342 726 653  23 489 583 111  44 220 414 107\n",
      " 260 278 731 267 340 241 122 714 181 389  50 193 100 505 513 498 138 200\n",
      " 219 748 527 528 174 529 685 783 440 638 283 720 312 336 555 681 705 481\n",
      "  16 306 146  52 247  62  99 293 521  22 456 532 608 139   4 682  25 488\n",
      " 594 214 496 633  37 486 543 675 546 192 282 763 280 751 101 761 371  51\n",
      " 217 184 419 758 112 445 621 323 518 473  58 298 159 339 772  94 693 131\n",
      " 185 531 619   1 694  17 452   9  35 611  49 311 426 431 432 165 569   5\n",
      " 318 423 595 476 320 415 455 538 596 444 542 369 679 183 724 322 205 175\n",
      " 288 504 745 539 767 143 467 533 549 635 114 259 625 178  64 187 261 578\n",
      " 698 522  43  60 216 240 153 136 615 372 173 657 333  10 710 770 622  87\n",
      " 213 246 451 464 345 224 450 754 616  32 190  56 150 669 397 683 130 297\n",
      " 755 338 662 351 160 305 664 750 374 680  47 668 774 172 212 176 341  33\n",
      " 545  83 501 753  54 466 599 618 613  19 672 433 370 560 520 756  93 775\n",
      "  63  86 696 631  30 125 541 606 591 163 508 468  70 375 118 392 557 441\n",
      " 265 492 654 639 390 120  95 117 632 324 740 453 411 739 626 256 226 203\n",
      " 388  78 291 164 133 584 140 494 540  65 579 126 690   6 331 235 649 462\n",
      " 257 263 295 202 227  24 765 239 223  13 643 695 460 204 479 728 228 502\n",
      " 495 759 360 478 776 279 483 465  29  97 650 565 251  69]\n",
      "rows to prune in layer 3 : 225\n",
      "[  0   1   3   4   8  10  11  12  13  14  15  18  22  25  26  29  31  32\n",
      "  33  34  38  44  47  49  50  51  52  54  55  56  58  61  63  65  66  68\n",
      "  74  75  76  77  80  83  84  87  88  90  91  94  97 100 101 103 104 106\n",
      " 107 110 113 114 116 117 119 121 122 127 129 136 140 141 143 145 146 149\n",
      " 151 156 160 161 162 164 165 167 168 169 170 171 172 174 175 183 185 186\n",
      " 188 189 190 193 194 195 196 197 199 205 206 207 210 211 212 217 222 229\n",
      " 231 232 235 236 239 240 242 243 244 245 246 247 250 253 254 255 256 257\n",
      " 258 259 262 263 265 266 269 270 271 272 273 275 276 278 279 284 285 288\n",
      " 290 293 294 295 296 299]\n",
      "[232 256  87  91 288  83  33 101 129 250  54  94  52  63 116 113 272  26\n",
      " 170   1 212   8 240 269  97 186 151  32 174 271 114  10 149  44 244 222\n",
      " 285  58 294 107 276 296 146  49 262  66 106  14 121  75 193 295 266 185\n",
      " 293 160 239 273 275 270 110 231  51  55 235 141 165 103 183 190 255 207\n",
      " 171 167 143 243 265 136  34  11 197 161 254 247 217 122 164 175 119  22\n",
      " 263   4  84 145  77 278 168  25 194 196  88  31  80   3 290 259 246  50\n",
      "  90 199  12 189 195 258 104 253  13 205 210 172 169  56  47 229 236 140\n",
      " 279 245 257 206  38 100  65 188 156 242 284  76 117 127  18  29  68   0\n",
      " 211  74 299  15 162  61]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 2  4  5  7  8 11 12 18 19 22 25 27 29 30 34 37 38 41 42 44 45 46 47 48\n",
      " 53 55 56 57 59 60 61 62 64 65 67 70 71 74 80 81 84 85 88 89 90 92 93 94\n",
      " 98 99]\n",
      "[88 55  4 45 42 62  7 56 11 94 85 44 48 80 99  8 27 92 93 60 25 81 30 98\n",
      " 47 37 18 57 67 34  5 89 12 74 61 64 90 65  2 46 19 41 59 53 71 70 84 29\n",
      " 22 38]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5695 - accuracy: 0.9114 - val_loss: 1.5344 - val_accuracy: 0.9361\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5204 - accuracy: 0.9490 - val_loss: 1.5229 - val_accuracy: 0.9443\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5110 - accuracy: 0.9563 - val_loss: 1.5170 - val_accuracy: 0.9485\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5048 - accuracy: 0.9615 - val_loss: 1.5137 - val_accuracy: 0.9502\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5005 - accuracy: 0.9655 - val_loss: 1.5135 - val_accuracy: 0.9507\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4976 - accuracy: 0.9675 - val_loss: 1.5097 - val_accuracy: 0.9550\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4950 - accuracy: 0.9705 - val_loss: 1.5094 - val_accuracy: 0.9545\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4931 - accuracy: 0.9718 - val_loss: 1.5086 - val_accuracy: 0.9553\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4916 - accuracy: 0.9728 - val_loss: 1.5072 - val_accuracy: 0.9555\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4896 - accuracy: 0.9750 - val_loss: 1.5066 - val_accuracy: 0.9562\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4886 - accuracy: 0.9752 - val_loss: 1.5055 - val_accuracy: 0.9574\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4873 - accuracy: 0.9768 - val_loss: 1.5054 - val_accuracy: 0.9582\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4858 - accuracy: 0.9782 - val_loss: 1.5052 - val_accuracy: 0.9577\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4852 - accuracy: 0.9785 - val_loss: 1.5040 - val_accuracy: 0.9585\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4840 - accuracy: 0.9797 - val_loss: 1.5054 - val_accuracy: 0.9577\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4838 - accuracy: 0.9797 - val_loss: 1.5050 - val_accuracy: 0.9573\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4825 - accuracy: 0.9810 - val_loss: 1.5043 - val_accuracy: 0.9577\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:58, 138.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 6\n",
      "rows to prune in layer 0 : 686\n",
      "[  5   6  10  13  19  24  29  30  32  33  43  47  54  56  60  63  64  65\n",
      "  69  70  78  83  86  87  93  95  97 114 117 118 120 125 126 130 133 136\n",
      " 140 143 150 153 160 163 164 172 173 175 176 178 183 187 190 202 203 204\n",
      " 205 212 213 216 223 224 226 227 228 235 239 240 246 251 256 257 259 261\n",
      " 263 265 279 288 291 295 297 305 318 320 322 324 331 333 338 341 345 351\n",
      " 360 369 370 372 374 375 388 390 392 397 411 415 423 433 441 444 450 451\n",
      " 453 455 460 462 464 465 466 467 468 476 478 479 483 492 494 495 501 502\n",
      " 504 508 520 522 533 538 539 540 541 542 545 549 557 560 565 569 578 579\n",
      " 584 591 595 596 599 606 613 615 616 618 622 625 626 631 632 635 639 643\n",
      " 649 650 654 657 662 664 668 669 672 679 680 683 690 695 696 698 710 724\n",
      " 728 739 740 745 750 753 754 755 756 759 765 767 770 774 775 776]\n",
      "[520 216 375 388 150 478 770  54 467 441 606 479 423  69 494 235  95 351\n",
      " 318 465 557 370 322 136 297 776 202 560 466 295 203 305 739 205 190 324\n",
      " 372 639 126 504  63 212 728 635  87 117 501 333 522 240  33 755  19 775\n",
      " 259 153 118 696 679 549 450 257 369 759  29  10 695 483 599  56 265 279\n",
      "  65  13 213  47 578 538  83 643 724  93  70 183  43 618 745  32 710 502\n",
      " 468 433  64 650 533 460 239 626 672 120 625 774 476 224 226 616 649  30\n",
      " 130 756 632 204 288 453  60 125 360 411 683 455 495 163   6 596 178 164\n",
      " 545 765 754 657 397   5 246 492 740 320 451 569 750 631 114 698 464 227\n",
      " 542 338 291  24 584 662  86 669 341 539 595 173 767 565 160 331 615 622\n",
      " 261 753 133 175 263 690 172 345 579 415 251 143 654 541 613 374 228 390\n",
      " 444 462 140 668 256  78 392 508 591 664 680 176 223 540  97 187]\n",
      "rows to prune in layer 3 : 262\n",
      "[  0   3   4  11  12  13  15  18  22  25  29  31  34  38  47  50  56  61\n",
      "  65  68  74  76  77  80  84  88  90 100 104 117 119 122 127 136 140 145\n",
      " 156 161 162 164 168 169 172 175 188 189 194 195 196 197 199 205 206 210\n",
      " 211 217 229 236 242 243 245 246 247 253 254 257 258 259 263 265 278 279\n",
      " 284 290 299]\n",
      "[253 122 299 265  31 195  61  13  12  88 290 211 104  34  47 242 168 243\n",
      " 278 217  50 169 206 175 188 263  22   4 145 189 127 119 245  90 172 194\n",
      " 247  74 236 254 279 197  84 100 140 284 156  68 162  38  18 136  77 246\n",
      " 257  56 259 258  11 199 117  65   3 196 205  25  15 161 229  76   0 164\n",
      "  80 210  29]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 2  5 12 18 19 22 29 34 37 38 41 46 53 57 59 61 64 65 67 70 71 74 84 89\n",
      " 90]\n",
      "[37  5 65 19 84 57 90 64 71 22 46 89 74 29 38 70 41 59 53 67  2 61 18 12\n",
      " 34]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7803 - accuracy: 0.7018 - val_loss: 1.6557 - val_accuracy: 0.8244\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6422 - accuracy: 0.8331 - val_loss: 1.6282 - val_accuracy: 0.8419\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6209 - accuracy: 0.8513 - val_loss: 1.6168 - val_accuracy: 0.8507\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6097 - accuracy: 0.8614 - val_loss: 1.6083 - val_accuracy: 0.8616\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6027 - accuracy: 0.8665 - val_loss: 1.6069 - val_accuracy: 0.8600\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5969 - accuracy: 0.8720 - val_loss: 1.6007 - val_accuracy: 0.8671\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5926 - accuracy: 0.8753 - val_loss: 1.5969 - val_accuracy: 0.8687\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5889 - accuracy: 0.8785 - val_loss: 1.5981 - val_accuracy: 0.8662\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5862 - accuracy: 0.8813 - val_loss: 1.5961 - val_accuracy: 0.8699\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5837 - accuracy: 0.8838 - val_loss: 1.5928 - val_accuracy: 0.8708\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5813 - accuracy: 0.8849 - val_loss: 1.5933 - val_accuracy: 0.8705\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5796 - accuracy: 0.8867 - val_loss: 1.5922 - val_accuracy: 0.8727\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5778 - accuracy: 0.8883 - val_loss: 1.5896 - val_accuracy: 0.8744\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5763 - accuracy: 0.8896 - val_loss: 1.5905 - val_accuracy: 0.8733\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5749 - accuracy: 0.8916 - val_loss: 1.5887 - val_accuracy: 0.8745\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5733 - accuracy: 0.8929 - val_loss: 1.5873 - val_accuracy: 0.8769\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5722 - accuracy: 0.8933 - val_loss: 1.5864 - val_accuracy: 0.8768\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5712 - accuracy: 0.8943 - val_loss: 1.5862 - val_accuracy: 0.8770\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5703 - accuracy: 0.8953 - val_loss: 1.5856 - val_accuracy: 0.8778\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5693 - accuracy: 0.8962 - val_loss: 1.5881 - val_accuracy: 0.8750\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5682 - accuracy: 0.8970 - val_loss: 1.5838 - val_accuracy: 0.8791\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5675 - accuracy: 0.8981 - val_loss: 1.5844 - val_accuracy: 0.8780\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5664 - accuracy: 0.8990 - val_loss: 1.5835 - val_accuracy: 0.8805\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5660 - accuracy: 0.8992 - val_loss: 1.5839 - val_accuracy: 0.8785\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5652 - accuracy: 0.9000 - val_loss: 1.5842 - val_accuracy: 0.8775\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5644 - accuracy: 0.9011 - val_loss: 1.5828 - val_accuracy: 0.8806\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5637 - accuracy: 0.9017 - val_loss: 1.5836 - val_accuracy: 0.8795\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5634 - accuracy: 0.9014 - val_loss: 1.5851 - val_accuracy: 0.8765\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5623 - accuracy: 0.9023 - val_loss: 1.5845 - val_accuracy: 0.8767\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:52, 149.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 6\n",
      "rows to prune in layer 0 : 735\n",
      "[  5   6  24  30  60  78  86  97 114 120 125 130 133 140 143 160 163 164\n",
      " 172 173 175 176 178 187 204 223 224 226 227 228 246 251 256 261 263 288\n",
      " 291 320 331 338 341 345 360 374 390 392 397 411 415 444 451 453 455 462\n",
      " 464 476 492 495 508 539 540 541 542 545 565 569 579 584 591 595 596 613\n",
      " 615 616 622 625 631 632 649 654 657 662 664 668 669 672 680 683 690 698\n",
      " 740 750 753 754 756 765 767 774]\n",
      "[291 455 683  60 204 178 508 698   5 545 133 453 654 774 226 596 130 360\n",
      " 595 540 164 753 668 392 172 542 569 492   6 649 341 632 256 125 579  24\n",
      " 584 565 765 411 224 223 664 451 288 261 669 397 476 175 331 680 173 754\n",
      " 338 625 187 591 690 415 228 622 246 657 176 756 767 163 616 120  86 539\n",
      " 662 631 464 672 390 143 345 263 750 160  78 227 613 615  97 251 444  30\n",
      " 740 374 495 140 114 541 462 320]\n",
      "rows to prune in layer 3 : 281\n",
      "[  0   3  11  15  18  25  29  38  56  65  68  74  76  77  80  84 100 117\n",
      " 136 140 156 161 162 164 196 197 199 205 210 229 236 246 254 257 258 259\n",
      " 279 284]\n",
      "[258 136 279 246  77 205  80 197 284 164  15  29 161  84  25 199 162 100\n",
      " 196  68 140 259  18  74   0  11 210  65  56  38 257 117 236  76 156 229\n",
      " 254   3]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 2 12 18 29 34 38 41 53 59 61 67 70 74]\n",
      "[12 70 41 59  2 67 61 18 38 29 74 34 53]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9199 - accuracy: 0.5581 - val_loss: 1.8444 - val_accuracy: 0.6296\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8259 - accuracy: 0.6467 - val_loss: 1.8100 - val_accuracy: 0.6599\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7765 - accuracy: 0.6963 - val_loss: 1.7602 - val_accuracy: 0.7113\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7478 - accuracy: 0.7234 - val_loss: 1.7454 - val_accuracy: 0.7231\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7374 - accuracy: 0.7318 - val_loss: 1.7376 - val_accuracy: 0.7310\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7304 - accuracy: 0.7388 - val_loss: 1.7332 - val_accuracy: 0.7348\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7252 - accuracy: 0.7434 - val_loss: 1.7294 - val_accuracy: 0.7399\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7214 - accuracy: 0.7460 - val_loss: 1.7246 - val_accuracy: 0.7423\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7179 - accuracy: 0.7493 - val_loss: 1.7227 - val_accuracy: 0.7408\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7155 - accuracy: 0.7524 - val_loss: 1.7195 - val_accuracy: 0.7451\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7130 - accuracy: 0.7544 - val_loss: 1.7171 - val_accuracy: 0.7475\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7109 - accuracy: 0.7557 - val_loss: 1.7163 - val_accuracy: 0.7490\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7089 - accuracy: 0.7574 - val_loss: 1.7158 - val_accuracy: 0.7470\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7072 - accuracy: 0.7596 - val_loss: 1.7129 - val_accuracy: 0.7515\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7055 - accuracy: 0.7612 - val_loss: 1.7120 - val_accuracy: 0.7548\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7041 - accuracy: 0.7617 - val_loss: 1.7103 - val_accuracy: 0.7541\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7028 - accuracy: 0.7630 - val_loss: 1.7073 - val_accuracy: 0.7592\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7015 - accuracy: 0.7650 - val_loss: 1.7071 - val_accuracy: 0.7576\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7004 - accuracy: 0.7655 - val_loss: 1.7062 - val_accuracy: 0.7574\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6991 - accuracy: 0.7677 - val_loss: 1.7066 - val_accuracy: 0.7581\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6981 - accuracy: 0.7678 - val_loss: 1.7043 - val_accuracy: 0.7609\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6975 - accuracy: 0.7680 - val_loss: 1.7041 - val_accuracy: 0.7592\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6964 - accuracy: 0.7692 - val_loss: 1.7036 - val_accuracy: 0.7592\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6956 - accuracy: 0.7699 - val_loss: 1.7021 - val_accuracy: 0.7620\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6950 - accuracy: 0.7703 - val_loss: 1.7022 - val_accuracy: 0.7598\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6943 - accuracy: 0.7708 - val_loss: 1.7013 - val_accuracy: 0.7617\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6936 - accuracy: 0.7713 - val_loss: 1.7002 - val_accuracy: 0.7645\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6930 - accuracy: 0.7725 - val_loss: 1.6990 - val_accuracy: 0.7643\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6924 - accuracy: 0.7733 - val_loss: 1.6989 - val_accuracy: 0.7640\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6917 - accuracy: 0.7733 - val_loss: 1.6988 - val_accuracy: 0.7663\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6911 - accuracy: 0.7739 - val_loss: 1.6989 - val_accuracy: 0.7637\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6907 - accuracy: 0.7740 - val_loss: 1.6986 - val_accuracy: 0.7641\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6905 - accuracy: 0.7733 - val_loss: 1.6981 - val_accuracy: 0.7647\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6899 - accuracy: 0.7747 - val_loss: 1.6984 - val_accuracy: 0.7640\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6891 - accuracy: 0.7757 - val_loss: 1.6977 - val_accuracy: 0.7643\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6887 - accuracy: 0.7753 - val_loss: 1.6986 - val_accuracy: 0.7636\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6888 - accuracy: 0.7759 - val_loss: 1.6972 - val_accuracy: 0.7646\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6882 - accuracy: 0.7765 - val_loss: 1.6966 - val_accuracy: 0.7649\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6877 - accuracy: 0.7769 - val_loss: 1.6975 - val_accuracy: 0.7632\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6875 - accuracy: 0.7763 - val_loss: 1.6956 - val_accuracy: 0.7654\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6873 - accuracy: 0.7767 - val_loss: 1.6952 - val_accuracy: 0.7669\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6871 - accuracy: 0.7770 - val_loss: 1.6954 - val_accuracy: 0.7663\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6865 - accuracy: 0.7775 - val_loss: 1.6947 - val_accuracy: 0.7675\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6864 - accuracy: 0.7777 - val_loss: 1.6948 - val_accuracy: 0.7667\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6862 - accuracy: 0.7781 - val_loss: 1.6959 - val_accuracy: 0.7639\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6860 - accuracy: 0.7777 - val_loss: 1.6952 - val_accuracy: 0.7659\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [13:32, 170.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 6\n",
      "rows to prune in layer 0 : 759\n",
      "[ 30  78  86  97 114 120 140 143 160 163 173 175 176 187 227 228 246 251\n",
      " 263 320 331 338 345 374 390 415 444 462 464 495 539 541 591 613 615 616\n",
      " 622 625 631 657 662 672 680 690 740 750 754 756 767]\n",
      "[187 338 143  30 444 541 415 160 464 539 750 320 615 662 263  78 591 740\n",
      " 631 246 390 754 657  86 227 690 672 462 140 680 767  97 120 176 345 331\n",
      " 251 622 175 374 228 625 114 173 613 495 616 756 163]\n",
      "rows to prune in layer 3 : 290\n",
      "[  0   3  11  18  38  56  65  68  74  76 117 140 156 210 229 236 254 257\n",
      " 259]\n",
      "[  3 259 236  38  76  18 140  56 257 117 229 254 156  74 210   0  11  65\n",
      "  68]\n",
      "rows to prune in layer 6 : 96\n",
      "[18 29 34 38 53 61 74]\n",
      "[38 61 74 34 18 29 53]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2270 - accuracy: 0.2300 - val_loss: 2.1844 - val_accuracy: 0.2791\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1759 - accuracy: 0.2839 - val_loss: 2.1639 - val_accuracy: 0.2949\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1674 - accuracy: 0.2895 - val_loss: 2.1599 - val_accuracy: 0.2967\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1635 - accuracy: 0.2928 - val_loss: 2.1563 - val_accuracy: 0.2990\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1534 - accuracy: 0.3035 - val_loss: 2.1440 - val_accuracy: 0.3119\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1095 - accuracy: 0.3543 - val_loss: 2.0853 - val_accuracy: 0.3769\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0857 - accuracy: 0.3751 - val_loss: 2.0770 - val_accuracy: 0.3830\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0805 - accuracy: 0.3801 - val_loss: 2.0716 - val_accuracy: 0.3897\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0654 - accuracy: 0.3954 - val_loss: 2.0226 - val_accuracy: 0.4406\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0217 - accuracy: 0.4408 - val_loss: 2.0084 - val_accuracy: 0.4548\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0118 - accuracy: 0.4514 - val_loss: 2.0011 - val_accuracy: 0.4607\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0078 - accuracy: 0.4550 - val_loss: 1.9981 - val_accuracy: 0.4622\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0050 - accuracy: 0.4566 - val_loss: 1.9949 - val_accuracy: 0.4650\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0010 - accuracy: 0.4597 - val_loss: 1.9894 - val_accuracy: 0.4709\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9975 - accuracy: 0.4638 - val_loss: 1.9868 - val_accuracy: 0.4726\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9959 - accuracy: 0.4647 - val_loss: 1.9860 - val_accuracy: 0.4745\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9945 - accuracy: 0.4664 - val_loss: 1.9847 - val_accuracy: 0.4760\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9933 - accuracy: 0.4675 - val_loss: 1.9849 - val_accuracy: 0.4759\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9923 - accuracy: 0.4684 - val_loss: 1.9831 - val_accuracy: 0.4776\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9916 - accuracy: 0.4685 - val_loss: 1.9824 - val_accuracy: 0.4779\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9906 - accuracy: 0.4696 - val_loss: 1.9816 - val_accuracy: 0.4782\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9899 - accuracy: 0.4703 - val_loss: 1.9808 - val_accuracy: 0.4805\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9890 - accuracy: 0.4719 - val_loss: 1.9807 - val_accuracy: 0.4799\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9882 - accuracy: 0.4724 - val_loss: 1.9798 - val_accuracy: 0.4808\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9879 - accuracy: 0.4734 - val_loss: 1.9795 - val_accuracy: 0.4807\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9873 - accuracy: 0.4739 - val_loss: 1.9790 - val_accuracy: 0.4826\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9867 - accuracy: 0.4744 - val_loss: 1.9783 - val_accuracy: 0.4817\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9863 - accuracy: 0.4743 - val_loss: 1.9783 - val_accuracy: 0.4830\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9860 - accuracy: 0.4747 - val_loss: 1.9782 - val_accuracy: 0.4828\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9856 - accuracy: 0.4748 - val_loss: 1.9770 - val_accuracy: 0.4826\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9852 - accuracy: 0.4746 - val_loss: 1.9772 - val_accuracy: 0.4833\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9850 - accuracy: 0.4758 - val_loss: 1.9769 - val_accuracy: 0.4833\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9844 - accuracy: 0.4762 - val_loss: 1.9763 - val_accuracy: 0.4841\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9841 - accuracy: 0.4762 - val_loss: 1.9769 - val_accuracy: 0.4829\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9840 - accuracy: 0.4761 - val_loss: 1.9761 - val_accuracy: 0.4843\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9837 - accuracy: 0.4767 - val_loss: 1.9763 - val_accuracy: 0.4831\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9833 - accuracy: 0.4770 - val_loss: 1.9752 - val_accuracy: 0.4849\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9829 - accuracy: 0.4776 - val_loss: 1.9763 - val_accuracy: 0.4824\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9828 - accuracy: 0.4772 - val_loss: 1.9756 - val_accuracy: 0.4838\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9826 - accuracy: 0.4776 - val_loss: 1.9756 - val_accuracy: 0.4846\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [16:27, 171.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 6\n",
      "rows to prune in layer 0 : 771\n",
      "[ 97 114 120 140 163 173 175 176 227 228 251 331 345 374 462 495 613 616\n",
      " 622 625 672 680 690 756 767]\n",
      "[622 616 163 756 120 767 690  97 251 613 374 625 228 227 345 173 672 176\n",
      " 462 114 331 140 175 680 495]\n",
      "rows to prune in layer 3 : 295\n",
      "[  0  11  65  68  74 117 156 210 229 254]\n",
      "[ 74 117 210   0 254  68  65 229 156  11]\n",
      "rows to prune in layer 6 : 98\n",
      "[18 29 34 53]\n",
      "[53 34 18 29]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2225 - accuracy: 0.2655 - val_loss: 2.1947 - val_accuracy: 0.2916\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1898 - accuracy: 0.2892 - val_loss: 2.1789 - val_accuracy: 0.2964\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1794 - accuracy: 0.2905 - val_loss: 2.1705 - val_accuracy: 0.2972\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1728 - accuracy: 0.2919 - val_loss: 2.1641 - val_accuracy: 0.2992\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1681 - accuracy: 0.2935 - val_loss: 2.1594 - val_accuracy: 0.3010\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1630 - accuracy: 0.2943 - val_loss: 2.1532 - val_accuracy: 0.3046\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1556 - accuracy: 0.2991 - val_loss: 2.1481 - val_accuracy: 0.3092\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1500 - accuracy: 0.3050 - val_loss: 2.1421 - val_accuracy: 0.3111\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1440 - accuracy: 0.3075 - val_loss: 2.1375 - val_accuracy: 0.3189\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1372 - accuracy: 0.3181 - val_loss: 2.1273 - val_accuracy: 0.3303\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1308 - accuracy: 0.3235 - val_loss: 2.1243 - val_accuracy: 0.3339\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1279 - accuracy: 0.3268 - val_loss: 2.1217 - val_accuracy: 0.3353\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1255 - accuracy: 0.3336 - val_loss: 2.1191 - val_accuracy: 0.3455\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1203 - accuracy: 0.3372 - val_loss: 2.1088 - val_accuracy: 0.3561\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1119 - accuracy: 0.3500 - val_loss: 2.1023 - val_accuracy: 0.3605\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1072 - accuracy: 0.3595 - val_loss: 2.0976 - val_accuracy: 0.3701\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1011 - accuracy: 0.3619 - val_loss: 2.0879 - val_accuracy: 0.3722\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0951 - accuracy: 0.3722 - val_loss: 2.0829 - val_accuracy: 0.3866\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0924 - accuracy: 0.3726 - val_loss: 2.0814 - val_accuracy: 0.3875\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0906 - accuracy: 0.3726 - val_loss: 2.0789 - val_accuracy: 0.3874\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0891 - accuracy: 0.3732 - val_loss: 2.0774 - val_accuracy: 0.3869\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0880 - accuracy: 0.3737 - val_loss: 2.0762 - val_accuracy: 0.3898\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0868 - accuracy: 0.3750 - val_loss: 2.0746 - val_accuracy: 0.3910\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0857 - accuracy: 0.3752 - val_loss: 2.0745 - val_accuracy: 0.3904\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0845 - accuracy: 0.3762 - val_loss: 2.0723 - val_accuracy: 0.3957\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0832 - accuracy: 0.3779 - val_loss: 2.0701 - val_accuracy: 0.3961\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0821 - accuracy: 0.3787 - val_loss: 2.0691 - val_accuracy: 0.3964\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0812 - accuracy: 0.3789 - val_loss: 2.0685 - val_accuracy: 0.3972\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0804 - accuracy: 0.3791 - val_loss: 2.0672 - val_accuracy: 0.3972\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0797 - accuracy: 0.3803 - val_loss: 2.0668 - val_accuracy: 0.3972\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0788 - accuracy: 0.3810 - val_loss: 2.0652 - val_accuracy: 0.3967\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0779 - accuracy: 0.3815 - val_loss: 2.0648 - val_accuracy: 0.3968\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0772 - accuracy: 0.3816 - val_loss: 2.0635 - val_accuracy: 0.4000\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0762 - accuracy: 0.3840 - val_loss: 2.0634 - val_accuracy: 0.3989\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0754 - accuracy: 0.3853 - val_loss: 2.0615 - val_accuracy: 0.4015\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0745 - accuracy: 0.3872 - val_loss: 2.0601 - val_accuracy: 0.4046\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0738 - accuracy: 0.3876 - val_loss: 2.0603 - val_accuracy: 0.4048\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0732 - accuracy: 0.3887 - val_loss: 2.0591 - val_accuracy: 0.4049\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0726 - accuracy: 0.3888 - val_loss: 2.0585 - val_accuracy: 0.4055\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0721 - accuracy: 0.3900 - val_loss: 2.0583 - val_accuracy: 0.4070\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0716 - accuracy: 0.3901 - val_loss: 2.0576 - val_accuracy: 0.4068\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0712 - accuracy: 0.3898 - val_loss: 2.0572 - val_accuracy: 0.4086\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0709 - accuracy: 0.3907 - val_loss: 2.0567 - val_accuracy: 0.4083\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0704 - accuracy: 0.3909 - val_loss: 2.0560 - val_accuracy: 0.4086\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0699 - accuracy: 0.3911 - val_loss: 2.0554 - val_accuracy: 0.4088\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0696 - accuracy: 0.3924 - val_loss: 2.0547 - val_accuracy: 0.4086\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0693 - accuracy: 0.3921 - val_loss: 2.0543 - val_accuracy: 0.4101\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0689 - accuracy: 0.3932 - val_loss: 2.0537 - val_accuracy: 0.4098\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0684 - accuracy: 0.3930 - val_loss: 2.0537 - val_accuracy: 0.4102\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0682 - accuracy: 0.3936 - val_loss: 2.0528 - val_accuracy: 0.4108\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0678 - accuracy: 0.3938 - val_loss: 2.0527 - val_accuracy: 0.4119\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0676 - accuracy: 0.3940 - val_loss: 2.0520 - val_accuracy: 0.4120\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0673 - accuracy: 0.3943 - val_loss: 2.0521 - val_accuracy: 0.4126\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0671 - accuracy: 0.3946 - val_loss: 2.0521 - val_accuracy: 0.4114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0670 - accuracy: 0.3945 - val_loss: 2.0514 - val_accuracy: 0.4135\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0668 - accuracy: 0.3948 - val_loss: 2.0514 - val_accuracy: 0.4125\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0665 - accuracy: 0.3947 - val_loss: 2.0511 - val_accuracy: 0.4131\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0664 - accuracy: 0.3949 - val_loss: 2.0510 - val_accuracy: 0.4124\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0662 - accuracy: 0.3954 - val_loss: 2.0504 - val_accuracy: 0.4127\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0660 - accuracy: 0.3953 - val_loss: 2.0502 - val_accuracy: 0.4130\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0656 - accuracy: 0.3962 - val_loss: 2.0498 - val_accuracy: 0.4132\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0655 - accuracy: 0.3960 - val_loss: 2.0495 - val_accuracy: 0.4128\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0653 - accuracy: 0.3962 - val_loss: 2.0500 - val_accuracy: 0.4139\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0650 - accuracy: 0.3963 - val_loss: 2.0493 - val_accuracy: 0.4132\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0649 - accuracy: 0.3960 - val_loss: 2.0498 - val_accuracy: 0.4134\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0647 - accuracy: 0.3964 - val_loss: 2.0491 - val_accuracy: 0.4135\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0645 - accuracy: 0.3968 - val_loss: 2.0491 - val_accuracy: 0.4138\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0642 - accuracy: 0.3964 - val_loss: 2.0488 - val_accuracy: 0.4149\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0641 - accuracy: 0.3972 - val_loss: 2.0478 - val_accuracy: 0.4149\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0638 - accuracy: 0.3973 - val_loss: 2.0480 - val_accuracy: 0.4140\n",
      "Epoch 71/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0639 - accuracy: 0.3972 - val_loss: 2.0480 - val_accuracy: 0.4155\n",
      "Epoch 72/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0636 - accuracy: 0.3979 - val_loss: 2.0473 - val_accuracy: 0.4160\n",
      "Epoch 73/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0636 - accuracy: 0.3981 - val_loss: 2.0472 - val_accuracy: 0.4144\n",
      "Epoch 74/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0633 - accuracy: 0.3975 - val_loss: 2.0473 - val_accuracy: 0.4153\n",
      "Epoch 75/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0630 - accuracy: 0.3980 - val_loss: 2.0469 - val_accuracy: 0.4173\n",
      "Epoch 76/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0629 - accuracy: 0.3983 - val_loss: 2.0464 - val_accuracy: 0.4171\n",
      "Epoch 77/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0627 - accuracy: 0.3986 - val_loss: 2.0469 - val_accuracy: 0.4174\n",
      "Epoch 78/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0625 - accuracy: 0.3983 - val_loss: 2.0464 - val_accuracy: 0.4157\n",
      "Epoch 79/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0623 - accuracy: 0.3987 - val_loss: 2.0459 - val_accuracy: 0.4184\n",
      "Epoch 80/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0621 - accuracy: 0.3993 - val_loss: 2.0450 - val_accuracy: 0.4177\n",
      "Epoch 81/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0619 - accuracy: 0.3992 - val_loss: 2.0450 - val_accuracy: 0.4193\n",
      "Epoch 82/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0617 - accuracy: 0.3993 - val_loss: 2.0451 - val_accuracy: 0.4193\n",
      "Epoch 83/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0615 - accuracy: 0.4000 - val_loss: 2.0446 - val_accuracy: 0.4186\n",
      "Epoch 84/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0612 - accuracy: 0.3995 - val_loss: 2.0446 - val_accuracy: 0.4189\n",
      "Epoch 85/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0607 - accuracy: 0.3997 - val_loss: 2.0442 - val_accuracy: 0.4177\n",
      "Epoch 86/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0604 - accuracy: 0.4010 - val_loss: 2.0430 - val_accuracy: 0.4192\n",
      "Epoch 87/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0595 - accuracy: 0.4014 - val_loss: 2.0434 - val_accuracy: 0.4199\n",
      "Epoch 88/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0582 - accuracy: 0.4029 - val_loss: 2.0400 - val_accuracy: 0.4230\n",
      "Epoch 89/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0574 - accuracy: 0.4044 - val_loss: 2.0395 - val_accuracy: 0.4237\n",
      "Epoch 90/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0569 - accuracy: 0.4051 - val_loss: 2.0393 - val_accuracy: 0.4219\n",
      "Epoch 91/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0566 - accuracy: 0.4049 - val_loss: 2.0388 - val_accuracy: 0.4236\n",
      "Epoch 92/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0562 - accuracy: 0.4048 - val_loss: 2.0381 - val_accuracy: 0.4236\n",
      "Epoch 93/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0560 - accuracy: 0.4053 - val_loss: 2.0393 - val_accuracy: 0.4232\n",
      "Epoch 94/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0557 - accuracy: 0.4052 - val_loss: 2.0384 - val_accuracy: 0.4244\n",
      "Epoch 95/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0555 - accuracy: 0.4057 - val_loss: 2.0373 - val_accuracy: 0.4240\n",
      "Epoch 96/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0553 - accuracy: 0.4054 - val_loss: 2.0381 - val_accuracy: 0.4246\n",
      "Epoch 97/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0553 - accuracy: 0.4055 - val_loss: 2.0376 - val_accuracy: 0.4262\n",
      "Epoch 98/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0548 - accuracy: 0.4056 - val_loss: 2.0366 - val_accuracy: 0.4241\n",
      "Epoch 99/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0547 - accuracy: 0.4059 - val_loss: 2.0376 - val_accuracy: 0.4258\n",
      "Epoch 100/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0544 - accuracy: 0.4059 - val_loss: 2.0358 - val_accuracy: 0.4248\n",
      "Epoch 101/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0538 - accuracy: 0.4068 - val_loss: 2.0366 - val_accuracy: 0.4253\n",
      "Epoch 102/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0533 - accuracy: 0.4070 - val_loss: 2.0347 - val_accuracy: 0.4278\n",
      "Epoch 103/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0531 - accuracy: 0.4074 - val_loss: 2.0353 - val_accuracy: 0.4252\n",
      "Epoch 104/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0526 - accuracy: 0.4073 - val_loss: 2.0344 - val_accuracy: 0.4252\n",
      "Epoch 105/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0523 - accuracy: 0.4082 - val_loss: 2.0337 - val_accuracy: 0.4278\n",
      "Epoch 106/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0515 - accuracy: 0.4094 - val_loss: 2.0325 - val_accuracy: 0.4289\n",
      "Epoch 107/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0508 - accuracy: 0.4098 - val_loss: 2.0324 - val_accuracy: 0.4304\n",
      "Epoch 108/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0504 - accuracy: 0.4102 - val_loss: 2.0321 - val_accuracy: 0.4304\n",
      "Epoch 109/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0499 - accuracy: 0.4110 - val_loss: 2.0317 - val_accuracy: 0.4289\n",
      "Epoch 110/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0495 - accuracy: 0.4108 - val_loss: 2.0329 - val_accuracy: 0.4275\n",
      "Epoch 111/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0495 - accuracy: 0.4108 - val_loss: 2.0313 - val_accuracy: 0.4288\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0492 - accuracy: 0.4110 - val_loss: 2.0303 - val_accuracy: 0.4285\n",
      "Epoch 113/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0489 - accuracy: 0.4114 - val_loss: 2.0326 - val_accuracy: 0.4297\n",
      "Epoch 114/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0488 - accuracy: 0.4115 - val_loss: 2.0298 - val_accuracy: 0.4292\n",
      "Epoch 115/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0487 - accuracy: 0.4112 - val_loss: 2.0310 - val_accuracy: 0.4286\n",
      "Epoch 116/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0483 - accuracy: 0.4116 - val_loss: 2.0315 - val_accuracy: 0.4294\n",
      "Epoch 117/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0479 - accuracy: 0.4118 - val_loss: 2.0298 - val_accuracy: 0.4313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [21:40, 185.82s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [2:13:10<58:22, 1167.50s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5846 - accuracy: 0.8888 - val_loss: 1.5206 - val_accuracy: 0.9435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 7\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[435 456 298 644 325 389 688 154 252   2 577 337 732  98 672  84 191 692\n",
      " 233 750 199 642 744 602  89 480 670 170  65 608 357 770 747 414 164 292\n",
      " 716 757 451 283 231 572 452 478 188 220 230 636 141 385  70 520 524 150\n",
      " 335 211 425 472 251 258 187 532  16 722 659  92 766 157 623 511 234 115\n",
      " 169 491 533 648 512 542 578 275  48 475 514 158  96 202 223 112 734 289\n",
      " 111 181 273 403 370 772 401 196  23 523 416 740 545 129 531 232 245 631\n",
      " 246 594 513 682 486 638 178 467 276 323 446 147  15 510 165 236 637 548\n",
      " 352 264  97 265 358 778 756 351 698  52  24 395 238 537 739 711 135 635\n",
      " 503 172 151 402 438 339 332 270 528 318 189 433 735 621 487 166 600 409\n",
      " 119 530 305 724 146 426 616 447  59 560 143 505 613  75  28 330 350 715\n",
      "   5 439  21 696 620 218 684 257 361 315 184  10 432 226 658 683 477 606\n",
      " 657 619  25 215  86 632 105  53  56 200 706 721 675 399  71 217 765 176\n",
      " 408 758 671 596  88 177 591 579 479 421 120 379 296 515 212 783 595 197\n",
      "  46 573  22 622 376 628 581 720 338 349 359 629 329 107  62 493 474 597\n",
      " 755  73 490 132 763 375 599 126 241 411 194 567 571 179 464 210 345  79\n",
      " 700 324 110 346 546 161 193 302 762 207 316 458 741 525 363  93 123 371\n",
      " 481 708 286 153 250 128 663 436 713   3 304 342 485 570 353 308 262 134\n",
      "  82 615 125 391 731 393  49 769 144 367  29 448 307 190  35 102  78 501\n",
      "   9 138  40 759 773 614 204 427 278 655 209 527 175 429 516 575 348 484\n",
      " 412 186 624 593 776 266 742 440 678 104 633 365 103 235 228  64 127 248\n",
      " 137 410 418  42 489 690 360 424 279 554 268  38 610 685 341 333 709 372\n",
      " 244  87 182 229 109  11 651 775 156 225 535 643  99 495 281 383 277 398\n",
      " 396  30 159 108 781 737 681 540  67 208 693 295 133 100 779 583 669 444\n",
      " 162 255 282 701 343 676 455 173  76 122 702 203 142 312 550  63 689 650\n",
      " 356 431 311 699  83  39 131 243 261 152 538 748  26 774 167 254  43 124\n",
      " 317 293 326 155 117   7 500  45 564 719 777 674  54 392  14 580 782 767\n",
      " 422 224 297 710 457 417 364 443 723 565 390 381  81   0 171 378 205 498\n",
      " 377 746 405 587 327 101 163 374 496 488 259 253 206 413  90  58 625 271\n",
      " 521 665 309 534 640 407 149 247 714 584 415 561 660 705 470 641 553 145\n",
      " 764  51 667 285 453 168 647 331 617  20 509 114 471 221 598 354 562 662\n",
      "  18 344 183 216 386 174  91 780 566 725 463 492 454 242 369  68 639 462\n",
      " 469 347 695  50 328  80  72 768 668 222 280 185 626 605  77 366 160 430\n",
      " 260 673 680   1 428 582 569 355 227  13 334 568 291 733 113 445 704 558\n",
      " 497 201 121 299 437 449 543 287 745 607 288 586 754 707 611 589 712  57\n",
      " 322 434 136  27 507 652 256 653  44 691 677 612 466 460 419 539 340 504\n",
      " 321  85  60 400 267 634 313  36 506 649 274 459  34 761 362 508 645 483\n",
      " 526 382 529 753 743 494 441 518 666 303 272 687 656 180 718 738 269  55\n",
      " 576 751 726 552 618 519  33 116 301  94 195 239 627 729 198 192 559 380\n",
      " 310 476 423 522 563 592 300 654 368 728 686 213 499 130 588  95 373 319\n",
      " 609 590  69 420 294 736 541 752 306 240 557 730  31 547 139 219 604  12\n",
      " 603  66 771 679 749  47   6 384  19 646 387 442  61 468 249 397  41 290\n",
      " 284 263 549 544  37 106 502 727  74 601 461 320 717  17 404 703 661 314\n",
      " 148  32 555 664 140 630 394 482 536 118 450 406 465 336   8 214 473 694\n",
      "   4 697 388 237 760 517 585 551 556 574]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[ 26  43 107  60  28 154 290 197 140 121 184 124 297 162 165 273 138  98\n",
      " 136 265 239 173 137  95 275 221  19  12  68 285  36 293 110 183 156 234\n",
      " 167  81 169 236  11 203  70 125 226  44  77 145  18 213 241 262  80 200\n",
      " 111  82 256  72  31 208 179 215 295   6 187 176  10 269  16  78 229 204\n",
      "  45 284 175 210 288  88  79  55 132 127 117 251   2 147 161 149  35 185\n",
      "  21  30   3 249 146 134 232 166 177  25  91  97  64 244 272 264 151  17\n",
      "  57 292 157 220 228  66 108   4 250 263 209 274 190  96  49 128   9 131\n",
      " 278 180 240 243 207 230 152 143 253 172 199 112  42 225  62  41 160 276\n",
      " 212  56  86  15  23   0  40 164  63  59 171 198 270  22 217 257 266 119\n",
      " 291 237 255 289  39 231 216 153 205 195 259 206 245  47 163 109 218 192\n",
      " 105 202  34 279  38 122 168 129 106 298  46 126 182   8 277 159 287 116\n",
      " 224  61 286 186  37 201 194 296  13 254  93  52  14  20  51 123 120 196\n",
      " 211 150 248 142 118 101 267   7 170 113 294 260  50  58  90 223 227 219\n",
      " 222 282  24 252 246 139 141 268 238  83 191 174   5 158  74  92  32  76\n",
      "   1 235  73  67 242 283  53 280 271  85  99 148 258  89 115  29 178 189\n",
      " 247 155 104  69 102 214  75 103  94  48 133 181 100  87  54 261  65  27\n",
      " 299 193  71 135 188 233 144 130  84 114  33 281]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[26  5  8 62 77 63 10 30 44  2 95 98 65 82 57  7 22 99 16 61  3 40 97 75\n",
      " 25  0 21 81 80 18 32 31 91 39 74 71 35 29 47 20 46 27 23 49 87 64 14 34\n",
      " 11 83 79 96 93 55 38 72 24 28 70 89 51 33 43 84 50 52 12  1 69 53 36 94\n",
      " 54 76  4 37 67 48 60 42 78 41 58 90 88 45 68  6 92 66 86  9 19 73 15 85\n",
      " 56 17 59 13]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5112 - accuracy: 0.9523 - val_loss: 1.5095 - val_accuracy: 0.9529\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4975 - accuracy: 0.9652 - val_loss: 1.5026 - val_accuracy: 0.9590\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4904 - accuracy: 0.9715 - val_loss: 1.4917 - val_accuracy: 0.9700\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4858 - accuracy: 0.9766 - val_loss: 1.4899 - val_accuracy: 0.9722\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4833 - accuracy: 0.9785 - val_loss: 1.4903 - val_accuracy: 0.9717\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4805 - accuracy: 0.9813 - val_loss: 1.4869 - val_accuracy: 0.9740\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4789 - accuracy: 0.9827 - val_loss: 1.4861 - val_accuracy: 0.9757\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4770 - accuracy: 0.9846 - val_loss: 1.4839 - val_accuracy: 0.9779\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4768 - accuracy: 0.9849 - val_loss: 1.4840 - val_accuracy: 0.9774\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4745 - accuracy: 0.9869 - val_loss: 1.4842 - val_accuracy: 0.9769\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4742 - accuracy: 0.9872 - val_loss: 1.4856 - val_accuracy: 0.9760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:57, 117.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 7\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[467 308 220 488 182 672 658 471 491 231 715  45 300 161  24  58   9 527\n",
      " 259 619 292 535  38  12 773 118 103   1 487 449 479 367 250 503 271 391\n",
      " 309 631 298 520 660 218 294 423 203 749 744 225 559 416 731   8 721 214\n",
      " 409 354 676 774 638 235 246 512 175 460 104 608  80 279 513  57 622 617\n",
      "  63 669 445  64 650 312 232 377 492 570 189 671 536 119 725 247 628 542\n",
      " 684 598 167 291 178   6  81 651 269 478 210 461 468 677 591 315 209 688\n",
      " 514 415 668  89 249 484 474 605 502 576 734  60 313 661 227 568 620 778\n",
      " 216 314 132 733  50  10 554 433 172 485 580 288 456  22 428 406  86  82\n",
      " 173 686  15 407  97 187  42 579 759 319 585 566 592 762  33  20 372  28\n",
      " 243 159 482 352  25 436 511 215 552 758 257 242 233 696 506 633 338  93\n",
      "  41  72 490 635 384  85 296  90 760 438   4 567 144 637 714 524 272 455\n",
      " 328 472 251 386 444 526 486 127 156 404 494 690 130  65 561 450 543 681\n",
      " 369 766 522 162 571 741  70 553 729 102 518 331 193 323 276  34  91 195\n",
      " 609 188 358 614 574 447 417 408 632 375 634  51 462 439 590 737 541  36\n",
      " 310  61 185 517 335 282 743 656 659 329  29 434 435 589 285 373  88 639\n",
      " 710 399 437  94 138 150 459 652 248 108 151 735 115 755 321 674 137 166\n",
      " 500 221 562 117 739 469 344 700 538 208 528 268 368  66 199 190 779  27\n",
      " 403 783 530 241 717 454 258 560 179 597 751 533 394 539 427 736 646 611\n",
      "  13 396 653  16 693 307 168 480 770 505 357 581 347 212 320 311 708 529\n",
      " 703 719 489 316 738  92 495 644 768 569 713 191 453 704 626  44 295 712\n",
      " 699 379 777 780 410 412 131 615 395  69 689  59 324  31 707 555 547 123\n",
      " 458 301 351 654 525 281 283 451 359 675  21 421 337 277 507 534 163 647\n",
      " 280 470 550 135 146 477 670 219 158 196 601 326 549  62  40 781  96 572\n",
      " 531   7 630 501  56 401  55 523 334 342  26 317  19 694 346 365 222 746\n",
      "  17 361 164  99  71 767 756 380 223 303 577  35 627 128 771 740 582  49\n",
      "  52 363 441 764 145 558 709  37 192 648 724 728 521 267 107  48 463 154\n",
      " 695 364 325 604 101 442 481 698 341 641 176 230 350 200 680 388 336 769\n",
      " 732 563 112 165  32 595 673 493 678 184 448 174 390  14 333 400  11 340\n",
      " 197  84 782 702 126 578 754 636 565 711 253 621 519 776 238 587  73 211\n",
      "  76 573 742 419  23  74 155 349 603 466 607 113 765 723 139 730 584 348\n",
      " 446 393 666 382 385 245 476 588 420 509 718 498 116 662 270 356 389 287\n",
      " 623  78 747 418  83  30 413 371 147 304 593 398 306 110 599 687 124 148\n",
      " 273 305 111 392 705 289 504 772 206 432 727 194 229 202 186 383 302 745\n",
      "  43 457 629 667 515 483 411 360  47 625 722   2 345 213 129 537 353 546\n",
      " 425 716 557 720   3 685  75  67 665 105 143 234  18  68 226 109 171 663\n",
      " 430 177 422   0 414 697 121 657 275 330  77 564 183 355 169 255 141 142\n",
      " 606  53 532 465 297 642 205 366 516 181 180 510 556 140 586 133 332 616\n",
      " 508 429 763 726   5 260 775 402 228 370 405 761 649 339 691 236 397 284\n",
      "  95 602 575 290 224 327 464 376 152 207 701 239 496 256 120 753 278 664\n",
      " 286 443 757 440 198 262 374 322  39 106 343 426 252 679 682 551 240 583\n",
      "  79 149 750 264 643 153 610 548 692 640 274 170 387 122 499 100  87 540\n",
      " 265 594 618 318 752 136 114 475 261 157 544 613 431  46 655 160 381 612\n",
      " 706  54 201 125 624  98 293 263 452 134 600 424 217 497 378 266 299 473\n",
      " 748 596 244 683 204 645 254 362 237 545]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[219 126   2 139 124 278  62 243 272 185  58  50  70 175  59 279 198 231\n",
      "  74 136 179 193 130  39   6 234 254 299 228 205  75 262 146 174 250 161\n",
      " 259 186 239 123 177 189  96  11 276  33  32 288  22   5  49  57 268  97\n",
      " 260 227 109 257  42 170 225  78 233   3 135 156 202 111 266  53 286 142\n",
      "  77 297  60 118  14  64  67 287 241  48 122  63 242 265 207  98  88 158\n",
      " 203 232 208  38  73 128   8 183  19 204 273 144  65  23 113 169  13 119\n",
      " 162 114  43 256 253 149 165 129  68 222  35 220 101 138 166 238 285 182\n",
      "  90 236 154 110 295 267  83 192 199   1 163 271  30  94 216  24  82   9\n",
      " 153 190 196 116  15 235  56 291  79 147 187 107 247 181   4  41 284 280\n",
      " 211 210 137 103  44 289 143  80 240 293 224  81 120  55 145 133  93 168\n",
      "  26 255 176 155 298 112 218 121  95 141 197  36 127 277 115 245  40  85\n",
      " 282  99 150 194 148 105  84  37  18  61 159 296 195 160 178 246 164 106\n",
      " 292 132 151  71  16 214  91  54  25 270 263 251 180 248  47  27 167 212\n",
      "  12 131 223 200 249   0 230  17 258  34  20 102 100 191 173  89 213 283\n",
      " 226 217 281  72 201 269 229  45 275 104 261 294  66 184  29 188 290  21\n",
      " 171   7 215  46  10 244 252 140  51 274 134  92 221  86 172 157 264  76\n",
      "  31  28  69 237 108 209 117 152  87  52 125 206]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[49 34 57 66  4 31 89 42 46 50 14 65 83 38 87 78 71  5  8 62 93 24 40 48\n",
      " 11 72 41 26  6 44 79 37 17 43 77 80 68 85 54 21 98 29 60 16 18 15 95 84\n",
      " 45 51 99 96 28 94 30  0 59  9 64 12 55 13 73 19 33 92 63 88 35 90 74 36\n",
      " 76 52 39  7 10 86 20 56 61 58 23 70 82 91 75 22 47 32 67 25 27  2  3  1\n",
      " 69 53 81 97]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5078 - accuracy: 0.9625 - val_loss: 1.4977 - val_accuracy: 0.9672\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4878 - accuracy: 0.9771 - val_loss: 1.4952 - val_accuracy: 0.9682\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4822 - accuracy: 0.9819 - val_loss: 1.4910 - val_accuracy: 0.9717\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4791 - accuracy: 0.9843 - val_loss: 1.4916 - val_accuracy: 0.9705\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4766 - accuracy: 0.9862 - val_loss: 1.4893 - val_accuracy: 0.9726\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9874 - val_loss: 1.4892 - val_accuracy: 0.9731\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4734 - accuracy: 0.9892 - val_loss: 1.4883 - val_accuracy: 0.9739\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4728 - accuracy: 0.9894 - val_loss: 1.4904 - val_accuracy: 0.9706\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4724 - accuracy: 0.9898 - val_loss: 1.4885 - val_accuracy: 0.9730\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4716 - accuracy: 0.9905 - val_loss: 1.4872 - val_accuracy: 0.9743\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4713 - accuracy: 0.9905 - val_loss: 1.4882 - val_accuracy: 0.9733\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4704 - accuracy: 0.9914 - val_loss: 1.4874 - val_accuracy: 0.9748\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4701 - accuracy: 0.9918 - val_loss: 1.4873 - val_accuracy: 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:02, 119.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 7\n",
      "rows to prune in layer 0 : 588\n",
      "[  0   2   3   5   7  11  14  17  18  19  23  26  30  32  35  37  39  40\n",
      "  43  46  47  48  49  52  53  54  55  56  62  67  68  71  73  74  75  76\n",
      "  77  78  79  83  84  87  95  96  98  99 100 101 105 106 107 109 110 111\n",
      " 112 113 114 116 120 121 122 124 125 126 128 129 133 134 135 136 139 140\n",
      " 141 142 143 145 146 147 148 149 152 153 154 155 157 158 160 163 164 165\n",
      " 169 170 171 174 176 177 180 181 183 184 186 192 194 196 197 198 200 201\n",
      " 202 204 205 206 207 211 213 217 219 222 223 224 226 228 229 230 234 236\n",
      " 237 238 239 240 244 245 252 253 254 255 256 260 261 262 263 264 265 266\n",
      " 267 270 273 274 275 278 280 284 286 287 289 290 293 297 299 302 303 304\n",
      " 305 306 317 318 322 325 326 327 330 332 333 334 336 339 340 341 342 343\n",
      " 345 346 348 349 350 353 355 356 360 361 362 363 364 365 366 370 371 374\n",
      " 376 378 380 381 382 383 385 387 388 389 390 392 393 397 398 400 401 402\n",
      " 405 411 413 414 418 419 420 422 424 425 426 429 430 431 432 440 441 442\n",
      " 443 446 448 452 457 463 464 465 466 470 473 475 476 477 481 483 493 496\n",
      " 497 498 499 501 504 507 508 509 510 515 516 519 521 523 531 532 534 537\n",
      " 540 544 545 546 548 549 550 551 556 557 558 563 564 565 572 573 575 577\n",
      " 578 582 583 584 586 587 588 593 594 595 596 599 600 601 602 603 604 606\n",
      " 607 610 612 613 616 618 621 623 624 625 627 629 630 636 640 641 642 643\n",
      " 645 647 648 649 655 657 662 663 664 665 666 667 670 673 678 679 680 682\n",
      " 683 685 687 691 692 694 695 697 698 701 702 705 706 709 711 716 718 720\n",
      " 722 723 724 726 727 728 730 732 740 742 745 746 747 748 750 752 753 754\n",
      " 756 757 761 763 764 765 767 769 771 772 775 776 781 782]\n",
      "[107 531   5  54 549 414 642 339 750 142 146 374 245 346 701 147 557  77\n",
      " 742 136 422 761  55 244 371 781 284 772 398 207  79 501 122 697 226 430\n",
      " 621 152 695  14 583 753 499 587 545 575 593 464 260  68 563  73 441 663\n",
      " 466 640 348 694 274 629 349 205 764 192 683 304 202 112 388 286 378 213\n",
      " 504 116 196 440  17 596 643 496 616 599 419 425 332  75 290 100 465 239\n",
      " 325 345 509 174 206 727 366 342 429  37 381 718 133 426 360 588 318 397\n",
      " 532 380 234 730 222 240 551 757 627 317 129  83 702 740 270 680   0 662\n",
      " 165  71 280 128  11 558 229 267 765 389 390 278 265 327 728 365 356 385\n",
      " 523 139 110 497 771 754 716 473 169 400 550 411 223 649 546 219  48 326\n",
      " 134 548 401 691 565 106 158 657 341  53 305 722 140 197 184 171  78 573\n",
      " 355 698 264 424 493 572 266 387 293 253 448 306 679 685  47 418  52  76\n",
      " 236 516  35 141 723 302 664 636  40  46 334 457 508 211 431 343 154 273\n",
      " 228 623 705 782  67 289 155 510 556  95 475  84 263 756 687 610 364 595\n",
      " 255 767 602 709 376  74 463  19 124 648 120 126 153 442 180 170 164   7\n",
      " 383 666  26  98 544 724 603 613  23 584 446 287 534 145 624 148 382 200\n",
      " 363  87   3  62 667 393 322 477 600 157 194 143 204 711 607 149 601 176\n",
      " 682 111 353 645 299 186 726 673 594 481 752 362 476 160 330 230 618 201\n",
      "  18 612 720 578 177 606 452 261 746  32 113 370 604 515  30 745 105 747\n",
      " 297 521   2 163  96  49 769 692 776 748 432 498 333 217 405 678 198 275\n",
      " 540  56 420 537 564 256 125 109 647 775 507 336 135 101 763  39 655 361\n",
      " 641 519 303 706 577 121 181 237  43 402 183 350 252 114 665  99 670 238\n",
      " 732 470 254 625 586 224 262 483 392 340 443 630 413 582]\n",
      "rows to prune in layer 3 : 225\n",
      "[  0   4   7  10  12  16  17  18  20  21  25  26  27  28  29  31  34  36\n",
      "  37  40  41  44  45  46  47  51  52  54  55  56  61  66  69  71  72  76\n",
      "  79  80  81  84  85  86  87  89  91  92  93  95  99 100 102 103 104 105\n",
      " 106 107 108 112 115 117 120 121 125 127 131 132 133 134 137 140 141 143\n",
      " 145 147 148 150 151 152 155 157 159 160 164 167 168 171 172 173 176 178\n",
      " 180 181 184 187 188 191 194 195 197 200 201 206 209 210 211 212 213 214\n",
      " 215 217 218 221 223 224 226 229 230 237 240 244 245 246 247 248 249 251\n",
      " 252 255 258 261 263 264 269 270 274 275 277 280 281 282 283 284 289 290\n",
      " 291 292 293 294 296 298]\n",
      "[ 99 212 283 160 284 270 159 209  10   4 127 121 107 137 131 141  72 246\n",
      " 244 291 269  54 155 197 293  45 106 191 188 148 167 282 178 201 280 117\n",
      " 200  36  86 275  93  31 210 133 187 140 298  55  25 223 249 213 237  12\n",
      " 229 102 218  47 274  95  91 176 211  16  46 151 172 150  85 230  71 157\n",
      " 261 143 264 134 115 217 226 290 195 164  17  92  37 147 120 145 258 184\n",
      "  44  29 252 103  66 248 168 206 180 224 281  26 181  52  84 263 215 277\n",
      " 108  27  61 152  79  40 214  69 125 251 100 296 104 245 105  18 173 289\n",
      "  21 221 292 132 294  34 247 171   7  51   0  87  28  41 255  89 240  20\n",
      "  56 194 112  76  81  80]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 0  1  2  3  7  9 10 12 13 19 20 22 23 25 27 28 30 32 33 35 36 39 47 52\n",
      " 53 55 56 58 59 61 63 64 67 69 70 73 74 75 76 81 82 86 88 90 91 92 94 96\n",
      " 97 99]\n",
      "[22 64 53 92  2 56 28 76 81  7 33 63 88  0 58 23 99 35 82 20 39 47 97 90\n",
      " 69 73 59  1 32 10 86 30  3 52 94 55  9 13 25 75 12 74 19 91 96 70 36 67\n",
      " 27 61]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5658 - accuracy: 0.9129 - val_loss: 1.5277 - val_accuracy: 0.9424\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5192 - accuracy: 0.9500 - val_loss: 1.5180 - val_accuracy: 0.9484\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5093 - accuracy: 0.9582 - val_loss: 1.5119 - val_accuracy: 0.9532\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5037 - accuracy: 0.9621 - val_loss: 1.5099 - val_accuracy: 0.9553\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4997 - accuracy: 0.9662 - val_loss: 1.5073 - val_accuracy: 0.9561\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4968 - accuracy: 0.9683 - val_loss: 1.5086 - val_accuracy: 0.9549\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4947 - accuracy: 0.9702 - val_loss: 1.5056 - val_accuracy: 0.9578\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4924 - accuracy: 0.9722 - val_loss: 1.5054 - val_accuracy: 0.9573\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4910 - accuracy: 0.9734 - val_loss: 1.5037 - val_accuracy: 0.9588\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4893 - accuracy: 0.9750 - val_loss: 1.5044 - val_accuracy: 0.9580\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4880 - accuracy: 0.9761 - val_loss: 1.5024 - val_accuracy: 0.9599\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4870 - accuracy: 0.9768 - val_loss: 1.5041 - val_accuracy: 0.9583\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4862 - accuracy: 0.9774 - val_loss: 1.5040 - val_accuracy: 0.9571\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4852 - accuracy: 0.9782 - val_loss: 1.5021 - val_accuracy: 0.9600\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4843 - accuracy: 0.9790 - val_loss: 1.5022 - val_accuracy: 0.9599\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9800 - val_loss: 1.5022 - val_accuracy: 0.9603\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4829 - accuracy: 0.9805 - val_loss: 1.5039 - val_accuracy: 0.9585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:17, 124.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 7\n",
      "rows to prune in layer 0 : 686\n",
      "[  2   3   7  18  19  23  26  30  32  35  39  40  43  46  49  52  56  62\n",
      "  67  74  76  84  87  95  96  98  99 101 105 109 111 113 114 120 121 124\n",
      " 125 126 135 141 143 145 148 149 153 154 155 157 160 163 164 170 176 177\n",
      " 180 181 183 186 194 198 200 201 204 211 217 224 228 230 236 237 238 252\n",
      " 254 255 256 261 262 263 273 275 287 289 297 299 302 303 322 330 333 334\n",
      " 336 340 343 350 353 361 362 363 364 370 376 382 383 392 393 402 405 413\n",
      " 420 431 432 442 443 446 452 457 463 470 475 476 477 481 483 498 507 508\n",
      " 510 515 516 519 521 534 537 540 544 556 564 577 578 582 584 586 594 595\n",
      " 600 601 602 603 604 606 607 610 612 613 618 623 624 625 630 636 641 645\n",
      " 647 648 655 664 665 666 667 670 673 678 682 687 692 705 706 709 711 720\n",
      " 723 724 726 732 745 746 747 748 752 756 763 767 769 775 776 782]\n",
      "[ 49 211 618 745 540 678 180 602 763 665 383 262 154 126 261 361 263 534\n",
      " 519 256 782 432 109 252 682  30 564 595  19 176 647 601 157 516 135  87\n",
      " 711 756 594 752 584 641  46  43   2 322 692 114 177  39  99 706 556 105\n",
      " 655  35  76  40 732 578 746 481 163 776 723  52 153 606 767 362 470 160\n",
      "  95 515 204 287 194 431 299 124 636 475 303  62 452 607 113 508 198 420\n",
      " 336 170 340 186 775 446 664  32 224  67 343 121 101 687 544 582 463 483\n",
      " 238 667 273 181 537  23 200 125 600 498 236 143  26 623 237  98 333 363\n",
      " 748 297 370 350 610 141 334 155 405 666 507 612 747 726 648 376 393  96\n",
      " 603 201 624  84 443 289 364 120 183  56 521 382 705 673 145 148 769 149\n",
      " 720  74 392 613 353 510 255 254   7 645 164 477 724 604 577 228  18 275\n",
      " 413   3 442 630 230 625 670 586 709 476 457 302 402 330 217 111]\n",
      "rows to prune in layer 3 : 262\n",
      "[  0   7  17  18  20  21  26  27  28  29  34  37  40  41  44  51  52  56\n",
      "  61  66  69  76  79  80  81  84  87  89  92 100 103 104 105 108 112 115\n",
      " 120 125 132 134 145 147 152 164 168 171 173 180 181 184 194 195 206 214\n",
      " 215 217 221 224 226 240 245 247 248 251 252 255 258 263 277 281 289 290\n",
      " 292 294 296]\n",
      "[292 247 281 224 164  18 215 112 168  66 181  29  37  41 125 120 184 115\n",
      "  81 103 206  76 217 263 255 251 226  26  34  61  84 252 171 214  40   0\n",
      " 132  21 290  56  20   7 173  27 180 289  92 134 296 145 258 152  79  51\n",
      " 294 245 195 100 221 105  80  69 104  44  28 108 147  17  89  52 194  87\n",
      " 240 277 248]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 1  3  9 10 12 13 19 25 27 30 32 36 52 55 59 61 67 70 73 74 75 86 91 94\n",
      " 96]\n",
      "[70 67 13 19 32 12 74 30 25 75  9 55 73 27 52 86  1 10 91 96 59  3 36 61\n",
      " 94]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6573 - accuracy: 0.8253 - val_loss: 1.5980 - val_accuracy: 0.8755\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5936 - accuracy: 0.8792 - val_loss: 1.5788 - val_accuracy: 0.8925\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5781 - accuracy: 0.8926 - val_loss: 1.5711 - val_accuracy: 0.8974\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5696 - accuracy: 0.8996 - val_loss: 1.5646 - val_accuracy: 0.9040\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5640 - accuracy: 0.9031 - val_loss: 1.5616 - val_accuracy: 0.9058\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5598 - accuracy: 0.9078 - val_loss: 1.5594 - val_accuracy: 0.9066\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5562 - accuracy: 0.9112 - val_loss: 1.5550 - val_accuracy: 0.9111\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5533 - accuracy: 0.9131 - val_loss: 1.5526 - val_accuracy: 0.9130\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5508 - accuracy: 0.9156 - val_loss: 1.5514 - val_accuracy: 0.9149\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5484 - accuracy: 0.9179 - val_loss: 1.5515 - val_accuracy: 0.9129\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5467 - accuracy: 0.9195 - val_loss: 1.5481 - val_accuracy: 0.9164\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5451 - accuracy: 0.9212 - val_loss: 1.5475 - val_accuracy: 0.9180\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5433 - accuracy: 0.9225 - val_loss: 1.5471 - val_accuracy: 0.9176\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5423 - accuracy: 0.9233 - val_loss: 1.5464 - val_accuracy: 0.9172\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5411 - accuracy: 0.9239 - val_loss: 1.5445 - val_accuracy: 0.9196\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5401 - accuracy: 0.9250 - val_loss: 1.5450 - val_accuracy: 0.9195\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5388 - accuracy: 0.9263 - val_loss: 1.5446 - val_accuracy: 0.9186\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5383 - accuracy: 0.9273 - val_loss: 1.5443 - val_accuracy: 0.9185\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5375 - accuracy: 0.9277 - val_loss: 1.5442 - val_accuracy: 0.9192\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5365 - accuracy: 0.9287 - val_loss: 1.5447 - val_accuracy: 0.9175\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5359 - accuracy: 0.9293 - val_loss: 1.5427 - val_accuracy: 0.9201\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5351 - accuracy: 0.9295 - val_loss: 1.5416 - val_accuracy: 0.9216\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5343 - accuracy: 0.9305 - val_loss: 1.5431 - val_accuracy: 0.9201\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5337 - accuracy: 0.9305 - val_loss: 1.5420 - val_accuracy: 0.9213\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5334 - accuracy: 0.9311 - val_loss: 1.5408 - val_accuracy: 0.9212\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5328 - accuracy: 0.9319 - val_loss: 1.5411 - val_accuracy: 0.9212\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5318 - accuracy: 0.9328 - val_loss: 1.5407 - val_accuracy: 0.9211\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5318 - accuracy: 0.9329 - val_loss: 1.5411 - val_accuracy: 0.9211\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5313 - accuracy: 0.9330 - val_loss: 1.5394 - val_accuracy: 0.9233\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5305 - accuracy: 0.9342 - val_loss: 1.5404 - val_accuracy: 0.9207\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5303 - accuracy: 0.9340 - val_loss: 1.5392 - val_accuracy: 0.9231\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5301 - accuracy: 0.9340 - val_loss: 1.5397 - val_accuracy: 0.9228\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5295 - accuracy: 0.9348 - val_loss: 1.5394 - val_accuracy: 0.9219\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5295 - accuracy: 0.9347 - val_loss: 1.5419 - val_accuracy: 0.9194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:29, 144.63s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 7\n",
      "rows to prune in layer 0 : 735\n",
      "[  3   7  18  23  26  56  67  74  84  96  98 101 111 120 121 125 141 143\n",
      " 145 148 149 155 164 181 183 200 201 217 224 228 230 236 237 238 254 255\n",
      " 273 275 289 297 302 330 333 334 343 350 353 363 364 370 376 382 392 393\n",
      " 402 405 413 442 443 457 463 476 477 483 498 507 510 521 537 544 577 582\n",
      " 586 600 603 604 610 612 613 623 624 625 630 645 648 666 667 670 673 687\n",
      " 705 709 720 724 726 747 748 769]\n",
      "[181 302 230 645 507 201 670 111 350 273 393 726 463  23 769 442 297 600\n",
      "  74 405 376 353 255   7 238 236 724 224 141 603 510 709 521 370 217 237\n",
      " 477 613 483 363  18 623 101 183 402 145 612 577 143   3 413 747 343 382\n",
      " 155 443 537  67 667  98 544  56  26 164 610 630 254 457 289 648 498 625\n",
      " 275 120 705 666 121 149 586 148 228 200 476 392 330 604 748 364 125 624\n",
      " 333  84 334 673 720  96 687 582]\n",
      "rows to prune in layer 3 : 281\n",
      "[  7  17  20  21  27  28  44  51  52  56  69  79  80  87  89  92 100 104\n",
      " 105 108 134 145 147 152 173 180 194 195 221 240 245 248 258 277 289 290\n",
      " 294 296]\n",
      "[105  56  21 108   7  20 248  44 221  87 194  89 152 180  69 104 296  27\n",
      "  28 294 173 258 145 100 277  51  52 240 147  92  79  17 134 195 245 289\n",
      " 290  80]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 1  3 10 27 36 52 59 61 73 86 91 94 96]\n",
      "[94 86  3 27 36 59 96 91 10 61 73 52  1]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0020 - accuracy: 0.4732 - val_loss: 1.8901 - val_accuracy: 0.5805\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8770 - accuracy: 0.5920 - val_loss: 1.8597 - val_accuracy: 0.6091\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8577 - accuracy: 0.6093 - val_loss: 1.8464 - val_accuracy: 0.6182\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8474 - accuracy: 0.6184 - val_loss: 1.8390 - val_accuracy: 0.6260\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8401 - accuracy: 0.6260 - val_loss: 1.8319 - val_accuracy: 0.6334\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8348 - accuracy: 0.6310 - val_loss: 1.8265 - val_accuracy: 0.6365\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8294 - accuracy: 0.6355 - val_loss: 1.8192 - val_accuracy: 0.6464\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8229 - accuracy: 0.6428 - val_loss: 1.8124 - val_accuracy: 0.6541\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8180 - accuracy: 0.6479 - val_loss: 1.8091 - val_accuracy: 0.6545\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8142 - accuracy: 0.6506 - val_loss: 1.8067 - val_accuracy: 0.6586\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8112 - accuracy: 0.6537 - val_loss: 1.8029 - val_accuracy: 0.6607\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8078 - accuracy: 0.6568 - val_loss: 1.8003 - val_accuracy: 0.6629\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8054 - accuracy: 0.6594 - val_loss: 1.7979 - val_accuracy: 0.6668\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8024 - accuracy: 0.6619 - val_loss: 1.7969 - val_accuracy: 0.6655\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8003 - accuracy: 0.6645 - val_loss: 1.7973 - val_accuracy: 0.6653\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7985 - accuracy: 0.6658 - val_loss: 1.7939 - val_accuracy: 0.6702\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7969 - accuracy: 0.6672 - val_loss: 1.7917 - val_accuracy: 0.6710\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7956 - accuracy: 0.6689 - val_loss: 1.7913 - val_accuracy: 0.6709\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7942 - accuracy: 0.6702 - val_loss: 1.7896 - val_accuracy: 0.6734\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7929 - accuracy: 0.6709 - val_loss: 1.7905 - val_accuracy: 0.6722\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7921 - accuracy: 0.6722 - val_loss: 1.7872 - val_accuracy: 0.6751\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7909 - accuracy: 0.6736 - val_loss: 1.7879 - val_accuracy: 0.6748\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7900 - accuracy: 0.6737 - val_loss: 1.7869 - val_accuracy: 0.6757\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7894 - accuracy: 0.6739 - val_loss: 1.7857 - val_accuracy: 0.6760\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7883 - accuracy: 0.6753 - val_loss: 1.7853 - val_accuracy: 0.6773\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7873 - accuracy: 0.6768 - val_loss: 1.7846 - val_accuracy: 0.6762\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7870 - accuracy: 0.6772 - val_loss: 1.7841 - val_accuracy: 0.6775\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7860 - accuracy: 0.6781 - val_loss: 1.7855 - val_accuracy: 0.6758\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7857 - accuracy: 0.6780 - val_loss: 1.7843 - val_accuracy: 0.6778\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7851 - accuracy: 0.6788 - val_loss: 1.7841 - val_accuracy: 0.6776\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7847 - accuracy: 0.6787 - val_loss: 1.7823 - val_accuracy: 0.6787\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7842 - accuracy: 0.6794 - val_loss: 1.7827 - val_accuracy: 0.6779\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7837 - accuracy: 0.6797 - val_loss: 1.7832 - val_accuracy: 0.6775\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7835 - accuracy: 0.6798 - val_loss: 1.7820 - val_accuracy: 0.6776\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7828 - accuracy: 0.6802 - val_loss: 1.7828 - val_accuracy: 0.6768\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7825 - accuracy: 0.6805 - val_loss: 1.7828 - val_accuracy: 0.6776\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7824 - accuracy: 0.6806 - val_loss: 1.7819 - val_accuracy: 0.6780\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7819 - accuracy: 0.6812 - val_loss: 1.7827 - val_accuracy: 0.6786\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7814 - accuracy: 0.6816 - val_loss: 1.7824 - val_accuracy: 0.6785\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7813 - accuracy: 0.6818 - val_loss: 1.7812 - val_accuracy: 0.6797\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7808 - accuracy: 0.6815 - val_loss: 1.7811 - val_accuracy: 0.6796\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7805 - accuracy: 0.6821 - val_loss: 1.7801 - val_accuracy: 0.6794\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7805 - accuracy: 0.6820 - val_loss: 1.7812 - val_accuracy: 0.6780\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7802 - accuracy: 0.6821 - val_loss: 1.7820 - val_accuracy: 0.6787\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7799 - accuracy: 0.6829 - val_loss: 1.7801 - val_accuracy: 0.6806\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7797 - accuracy: 0.6833 - val_loss: 1.7819 - val_accuracy: 0.6778\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7794 - accuracy: 0.6828 - val_loss: 1.7798 - val_accuracy: 0.6804\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7790 - accuracy: 0.6833 - val_loss: 1.7819 - val_accuracy: 0.6798\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7786 - accuracy: 0.6846 - val_loss: 1.7793 - val_accuracy: 0.6814\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7785 - accuracy: 0.6839 - val_loss: 1.7801 - val_accuracy: 0.6816\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7783 - accuracy: 0.6841 - val_loss: 1.7799 - val_accuracy: 0.6802\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7782 - accuracy: 0.6850 - val_loss: 1.7800 - val_accuracy: 0.6815\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:50, 161.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 7\n",
      "rows to prune in layer 0 : 759\n",
      "[  3  26  56  67  84  96  98 120 121 125 148 149 155 164 200 228 254 275\n",
      " 289 330 333 334 343 364 382 392 413 443 457 476 498 537 544 582 586 604\n",
      " 610 624 625 630 648 666 667 673 687 705 720 747 748]\n",
      "[364 289  98 476 148 748  84 125 604 537 334 149 457 705   3 120 687 121\n",
      " 382 624 228 666 164  96  26 747 544 413 333 330 648 392 667 443 673 630\n",
      " 720 586 582 155 200 343 275 625 254 498 610  67  56]\n",
      "rows to prune in layer 3 : 290\n",
      "[ 17  51  52  79  80  92 100 134 145 147 173 195 240 245 258 277 289 290\n",
      " 294]\n",
      "[258 173  52  79  51  17 289 245 290 147 134  92 145 294 100 277 240  80\n",
      " 195]\n",
      "rows to prune in layer 6 : 96\n",
      "[ 1 10 52 61 73 91 96]\n",
      "[61  1 91 96 10 73 52]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1214 - accuracy: 0.3367 - val_loss: 2.0448 - val_accuracy: 0.4199\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0487 - accuracy: 0.4123 - val_loss: 2.0314 - val_accuracy: 0.4304\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0360 - accuracy: 0.4248 - val_loss: 2.0227 - val_accuracy: 0.4405\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0284 - accuracy: 0.4334 - val_loss: 2.0167 - val_accuracy: 0.4445\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0237 - accuracy: 0.4368 - val_loss: 2.0124 - val_accuracy: 0.4494\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0184 - accuracy: 0.4431 - val_loss: 2.0071 - val_accuracy: 0.4519\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0117 - accuracy: 0.4480 - val_loss: 2.0018 - val_accuracy: 0.4598\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0075 - accuracy: 0.4525 - val_loss: 1.9984 - val_accuracy: 0.4575\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0047 - accuracy: 0.4555 - val_loss: 1.9963 - val_accuracy: 0.4635\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0024 - accuracy: 0.4583 - val_loss: 1.9939 - val_accuracy: 0.4663\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0003 - accuracy: 0.4602 - val_loss: 1.9920 - val_accuracy: 0.4695\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9987 - accuracy: 0.4620 - val_loss: 1.9917 - val_accuracy: 0.4690\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9975 - accuracy: 0.4631 - val_loss: 1.9893 - val_accuracy: 0.4712\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9961 - accuracy: 0.4636 - val_loss: 1.9880 - val_accuracy: 0.4727\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9946 - accuracy: 0.4653 - val_loss: 1.9871 - val_accuracy: 0.4742\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9937 - accuracy: 0.4659 - val_loss: 1.9863 - val_accuracy: 0.4737\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9927 - accuracy: 0.4667 - val_loss: 1.9852 - val_accuracy: 0.4727\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9917 - accuracy: 0.4677 - val_loss: 1.9846 - val_accuracy: 0.4760\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9909 - accuracy: 0.4686 - val_loss: 1.9837 - val_accuracy: 0.4753\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9901 - accuracy: 0.4691 - val_loss: 1.9829 - val_accuracy: 0.4755\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9892 - accuracy: 0.4705 - val_loss: 1.9825 - val_accuracy: 0.4768\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9887 - accuracy: 0.4708 - val_loss: 1.9816 - val_accuracy: 0.4771\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9880 - accuracy: 0.4716 - val_loss: 1.9814 - val_accuracy: 0.4773\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9870 - accuracy: 0.4721 - val_loss: 1.9829 - val_accuracy: 0.4753\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9863 - accuracy: 0.4729 - val_loss: 1.9795 - val_accuracy: 0.4796\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9857 - accuracy: 0.4727 - val_loss: 1.9796 - val_accuracy: 0.4797\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9851 - accuracy: 0.4741 - val_loss: 1.9788 - val_accuracy: 0.4796\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9845 - accuracy: 0.4740 - val_loss: 1.9785 - val_accuracy: 0.4807\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9840 - accuracy: 0.4747 - val_loss: 1.9778 - val_accuracy: 0.4804\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9836 - accuracy: 0.4750 - val_loss: 1.9783 - val_accuracy: 0.4806\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9830 - accuracy: 0.4760 - val_loss: 1.9773 - val_accuracy: 0.4807\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9826 - accuracy: 0.4753 - val_loss: 1.9780 - val_accuracy: 0.4831\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9823 - accuracy: 0.4760 - val_loss: 1.9777 - val_accuracy: 0.4806\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9818 - accuracy: 0.4767 - val_loss: 1.9761 - val_accuracy: 0.4818\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9815 - accuracy: 0.4767 - val_loss: 1.9755 - val_accuracy: 0.4823\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9812 - accuracy: 0.4773 - val_loss: 1.9751 - val_accuracy: 0.4845\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9807 - accuracy: 0.4780 - val_loss: 1.9747 - val_accuracy: 0.4831\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9805 - accuracy: 0.4774 - val_loss: 1.9747 - val_accuracy: 0.4828\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9803 - accuracy: 0.4774 - val_loss: 1.9746 - val_accuracy: 0.4833\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9800 - accuracy: 0.4775 - val_loss: 1.9740 - val_accuracy: 0.4844\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9795 - accuracy: 0.4784 - val_loss: 1.9753 - val_accuracy: 0.4769\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9794 - accuracy: 0.4785 - val_loss: 1.9753 - val_accuracy: 0.4825\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9791 - accuracy: 0.4792 - val_loss: 1.9744 - val_accuracy: 0.4838\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [15:45, 165.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 7\n",
      "rows to prune in layer 0 : 771\n",
      "[ 26  56  67 155 200 254 275 330 333 343 392 413 443 498 544 582 586 610\n",
      " 625 630 648 667 673 720 747]\n",
      "[544 254 667 413 392 155 275  67 630  56 586 610 648 720 333 625 443 343\n",
      " 330 498 747  26 673 582 200]\n",
      "rows to prune in layer 3 : 295\n",
      "[ 80  92 100 134 145 147 195 240 277 294]\n",
      "[147 277 240 145 195 134 294  92  80 100]\n",
      "rows to prune in layer 6 : 98\n",
      "[10 52 73 96]\n",
      "[10 52 73 96]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2779 - accuracy: 0.1588 - val_loss: 2.2571 - val_accuracy: 0.1699\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2465 - accuracy: 0.1974 - val_loss: 2.2380 - val_accuracy: 0.2154\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2315 - accuracy: 0.2265 - val_loss: 2.2162 - val_accuracy: 0.2471\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2000 - accuracy: 0.2684 - val_loss: 2.1809 - val_accuracy: 0.2854\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1853 - accuracy: 0.2770 - val_loss: 2.1743 - val_accuracy: 0.2893\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1798 - accuracy: 0.2835 - val_loss: 2.1699 - val_accuracy: 0.2939\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1764 - accuracy: 0.2865 - val_loss: 2.1671 - val_accuracy: 0.2981\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1742 - accuracy: 0.2877 - val_loss: 2.1653 - val_accuracy: 0.3011\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1726 - accuracy: 0.2889 - val_loss: 2.1636 - val_accuracy: 0.3015\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1715 - accuracy: 0.2898 - val_loss: 2.1625 - val_accuracy: 0.3024\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1706 - accuracy: 0.2904 - val_loss: 2.1617 - val_accuracy: 0.3014\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1697 - accuracy: 0.2908 - val_loss: 2.1604 - val_accuracy: 0.3021\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1688 - accuracy: 0.2912 - val_loss: 2.1599 - val_accuracy: 0.3028\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1682 - accuracy: 0.2917 - val_loss: 2.1597 - val_accuracy: 0.3031\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1675 - accuracy: 0.2921 - val_loss: 2.1590 - val_accuracy: 0.3031\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1669 - accuracy: 0.2928 - val_loss: 2.1586 - val_accuracy: 0.3030\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1664 - accuracy: 0.2930 - val_loss: 2.1581 - val_accuracy: 0.3036\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1659 - accuracy: 0.2930 - val_loss: 2.1578 - val_accuracy: 0.3028\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1655 - accuracy: 0.2933 - val_loss: 2.1574 - val_accuracy: 0.3034\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1650 - accuracy: 0.2937 - val_loss: 2.1576 - val_accuracy: 0.3039\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1646 - accuracy: 0.2935 - val_loss: 2.1565 - val_accuracy: 0.3041\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1642 - accuracy: 0.2940 - val_loss: 2.1562 - val_accuracy: 0.3036\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1637 - accuracy: 0.2946 - val_loss: 2.1562 - val_accuracy: 0.3054\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1635 - accuracy: 0.2951 - val_loss: 2.1554 - val_accuracy: 0.3048\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1630 - accuracy: 0.2957 - val_loss: 2.1554 - val_accuracy: 0.3046\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1627 - accuracy: 0.2960 - val_loss: 2.1548 - val_accuracy: 0.3059\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1624 - accuracy: 0.2967 - val_loss: 2.1546 - val_accuracy: 0.3056\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1621 - accuracy: 0.2967 - val_loss: 2.1543 - val_accuracy: 0.3054\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1618 - accuracy: 0.2968 - val_loss: 2.1544 - val_accuracy: 0.3059\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1614 - accuracy: 0.2975 - val_loss: 2.1537 - val_accuracy: 0.3056\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1612 - accuracy: 0.2973 - val_loss: 2.1541 - val_accuracy: 0.3066\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1609 - accuracy: 0.2975 - val_loss: 2.1534 - val_accuracy: 0.3071\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1605 - accuracy: 0.2975 - val_loss: 2.1528 - val_accuracy: 0.3062\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1605 - accuracy: 0.2978 - val_loss: 2.1526 - val_accuracy: 0.3074\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1601 - accuracy: 0.2981 - val_loss: 2.1530 - val_accuracy: 0.3071\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1600 - accuracy: 0.2980 - val_loss: 2.1527 - val_accuracy: 0.3063\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1598 - accuracy: 0.2981 - val_loss: 2.1526 - val_accuracy: 0.3075\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1595 - accuracy: 0.2984 - val_loss: 2.1521 - val_accuracy: 0.3074\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1594 - accuracy: 0.2983 - val_loss: 2.1520 - val_accuracy: 0.3080\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1591 - accuracy: 0.2986 - val_loss: 2.1518 - val_accuracy: 0.3082\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1589 - accuracy: 0.2986 - val_loss: 2.1524 - val_accuracy: 0.3075\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1588 - accuracy: 0.2990 - val_loss: 2.1517 - val_accuracy: 0.3081\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1586 - accuracy: 0.2990 - val_loss: 2.1517 - val_accuracy: 0.3084\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1585 - accuracy: 0.2991 - val_loss: 2.1512 - val_accuracy: 0.3084\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1584 - accuracy: 0.2992 - val_loss: 2.1517 - val_accuracy: 0.3088\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1582 - accuracy: 0.2993 - val_loss: 2.1515 - val_accuracy: 0.3088\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1580 - accuracy: 0.2994 - val_loss: 2.1508 - val_accuracy: 0.3081\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1580 - accuracy: 0.2995 - val_loss: 2.1512 - val_accuracy: 0.3084\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1578 - accuracy: 0.2997 - val_loss: 2.1511 - val_accuracy: 0.3087\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1576 - accuracy: 0.3002 - val_loss: 2.1509 - val_accuracy: 0.3090\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [18:12, 156.13s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [2:31:25<38:11, 1145.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5930 - accuracy: 0.8806 - val_loss: 1.5301 - val_accuracy: 0.9348\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 8\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[688 760  74 253 545 438   1 700  11 571 362  21 309 431 274 199 752 758\n",
      " 556 717 217  42 633 268 495 768 690 255 129 513 190 183 620  26  33 350\n",
      " 503 528 329 744 475 697 569 616 734 568 720  77 313 209 723 593 135  19\n",
      " 347 705 587  60 687 207 163 679 736 378 148 483  18 769  34 552 125 216\n",
      " 441 360 147 715 776 358 730 442 439 103  15 165 650  29 630 761 625 632\n",
      " 779 548  53 337   3 746 430 444 541 460 282 428 206 468 169 531 389 359\n",
      " 111 327 116 657   0  96 774 479   5 542 496 401 643 114 410 242 621 663\n",
      " 414 180 293 711 648 418 266  87  97 534  25 140 453 538 423 251 198 208\n",
      " 188 153  99 138 537 755 269 314 235 529 533 601 767 763 281 349 159 694\n",
      " 530 393 448 740 280 224 472 666 155 435 355 104 628 570 574 626 201 517\n",
      " 489 196 124 640 145 273 659 747 341 505 197 107 634 291 398 317  83  65\n",
      " 187 515 728 498  22  39  55 285   7 405 501  76 732 667 134 674 669 511\n",
      " 584 299 638 249 373 573 612 749 664 290 143 636 579 578 167  47 456  73\n",
      " 422 222 585 318 115 704 118 577 363 677 502 698 595 151 591  51  46 527\n",
      "  67 617 328 543 445 238 304 120 661 214 567 429  70 123 592 481 615 218\n",
      " 228 105  23  58 425 390 154  20 781 409 778  81 189 366 463 599 379 522\n",
      " 385 611 395 391 406 683 493 252 258 508 369 662 461 770 629 110  13  61\n",
      " 223  95 226 535 473 195 686 294 702 322 407 608 394 764 310 490 139 298\n",
      " 500 482 426 550 777 400  10 381 351 583 474 416 465  66 132 175 609 284\n",
      " 725 247 335 590 380 348 622 434 565 485 332 471  91 213 641  24 572  98\n",
      " 440 302 142 516 742 757 563 564 602 660   2  68 382 260  37 239 783 631\n",
      " 576 152 594  78 557 141 605 131 344 225 762 136 245 162 451 703 191 722\n",
      " 558 184 230 340 459 192 421 417 647 270 193   4 672 160 739 559 719 315\n",
      " 157 117 161  32  48 264 331 244 754  12 457  16 681 524  17 685 156 277\n",
      " 404 525 296 782 227  14 399 424  79  38  57 684 383 365 200 262 205 173\n",
      " 618 384 261 248 710 652 168 544 731 499 202 610 676 488 101 376 113  54\n",
      " 204 185 215  86 295 126 305 279 708 250  80 469 289 707  28 494 301 613\n",
      " 413 178 420  31 733 780 257 102 263 368 370 345 716 236 549 300 377 311\n",
      " 536 234 130 735 635 170  69 551 619 306 267 649 487 566 330 171   6 714\n",
      " 283 342 326 540 693 484 486 514 336 144 354 256 729 194 323 229 521 696\n",
      " 709 288 276 427 598 718 219 462 695 506   8 724 665 211 467  64 775 561\n",
      " 338 562  41 756 308 582 751 232 364 455 623 137  45 271 108 412 334 371\n",
      "  50 575 367 397 212 443  93 303 278  71 312 721 392 547 240 656 186 458\n",
      " 671 272 346 750  88 642 181 589 275 670 492 402 741 237 254 286 361 352\n",
      "  89 403 624 644 437 164 419 759 411 673 600 554 415 433 133  62 388 127\n",
      " 174 292  82 265 372 766 726  40 606 691 470 436 287 678 588 658 182 753\n",
      " 580 307 100  43  52 316 450 668 177 210 765 737 581 743 512 387 771 243\n",
      "  84 408 701  36 712 158 353 150 339 651 518 396 319 333 106 449 386 614\n",
      " 637 532  59 560 689 343 121 172 604 233 597 519 374 738 221 320  27 491\n",
      "  63 166 454 645 122 119 446 220 713 325  30 596 655 727 520 146  90 357\n",
      " 675  75 476 259 526 109 510 241 773 553 466 699  72 478 627  44 477 706\n",
      " 748 555 179 321 646 772  92 607 586 112 497 324  94 356  85 432 246  49\n",
      " 375 539 509 452  35 745 603 654 546 507 149 692 504 128 653 680   9 297\n",
      " 682 176  56 523 231 480 464 639 203 447]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[139  74 279 138 213 148  11  58  40  15  26 233 172  78 127 282 269 243\n",
      "  24 298 206 251  96  35   0 273  62 136 270  61  34  44 153 108 194 254\n",
      " 199 190 160 255 268  16 234  91 203 232 173 129   1 100  53 237   9 119\n",
      " 185 106 227 143 166 123 144  37 164  25 226 288 210 231 181  83  55 265\n",
      " 131 154 253 151 114 182 193 103 238 201 249 197 283   5 239  68 212  50\n",
      " 122 183 220 195 147 165 296  51 132 208  84 124 125 214  72 170 142 109\n",
      " 276   2 192 150 271 205 245 101  21  30 149  66 161 118  65 274 155 169\n",
      " 267  59  41  67  38  33 168 200  85 285 137 263 102 297  39  18  64 261\n",
      "  52 191 158  49  31  81 246  28 293 290 294 287 156 196 223 162  89 176\n",
      " 202  97 159   8 281 250  57 178 222  75 244 241  45 174  73  48  14 278\n",
      " 184 133 211 216  77  17 291 163 272 180 187  43  98  69 252 113  92 134\n",
      "  22 111 107 240 117  36  80 217  94   6 215 235 120  13  88 128 177 275\n",
      " 145 204 289 221 264  82 110  86 152  60 130 259  20 224   7 171 207 280\n",
      " 188 266 179 115   3  46 229 167  95 262 116 230  71  12 141 258  27 189\n",
      "  63 146 175 247 295 299 104 277  70  76 209 219  79  87 257  42  23  10\n",
      "  54 236 248  32 228   4  47 121 198  19 286 242 284  90  29 135  93 126\n",
      " 112 260 218 157 186 140 225 256 292 105  56  99]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[42 59 54 51 24 93 65 18 13 45 52 99 40 87 67 35 73 61 46 56 26  9 75 94\n",
      " 30 34 63 37 36  6 66  2 47 62 70 92 74 53 91 85 14 16 84 11 55 81  8 86\n",
      " 50 60 22 97 31 28 21 41  0 32 19  3 38 12 95 27 39 79 90 43 29 64 57 20\n",
      "  5 15 96 76 80 33 83 69 89 17 71 82 10 48 68 49 72 88 23 44 78 77 25 58\n",
      "  7  1 98  4]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5119 - accuracy: 0.9518 - val_loss: 1.5056 - val_accuracy: 0.9583\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4984 - accuracy: 0.9645 - val_loss: 1.4985 - val_accuracy: 0.9641\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4919 - accuracy: 0.9702 - val_loss: 1.4961 - val_accuracy: 0.9655\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4882 - accuracy: 0.9741 - val_loss: 1.4974 - val_accuracy: 0.9647\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4844 - accuracy: 0.9775 - val_loss: 1.4887 - val_accuracy: 0.9736\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4820 - accuracy: 0.9795 - val_loss: 1.4909 - val_accuracy: 0.9705\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4797 - accuracy: 0.9819 - val_loss: 1.4864 - val_accuracy: 0.9747\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4782 - accuracy: 0.9833 - val_loss: 1.4897 - val_accuracy: 0.9722\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4769 - accuracy: 0.9849 - val_loss: 1.4877 - val_accuracy: 0.9737\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4766 - accuracy: 0.9848 - val_loss: 1.4858 - val_accuracy: 0.9753\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4751 - accuracy: 0.9863 - val_loss: 1.4850 - val_accuracy: 0.9764\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4743 - accuracy: 0.9871 - val_loss: 1.4864 - val_accuracy: 0.9748\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4740 - accuracy: 0.9876 - val_loss: 1.4830 - val_accuracy: 0.9789\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4726 - accuracy: 0.9887 - val_loss: 1.4838 - val_accuracy: 0.9771\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9883 - val_loss: 1.4880 - val_accuracy: 0.9733\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4722 - accuracy: 0.9892 - val_loss: 1.4826 - val_accuracy: 0.9789\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4717 - accuracy: 0.9896 - val_loss: 1.4818 - val_accuracy: 0.9792\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4716 - accuracy: 0.9896 - val_loss: 1.4822 - val_accuracy: 0.9788\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4711 - accuracy: 0.9902 - val_loss: 1.4848 - val_accuracy: 0.9761\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4712 - accuracy: 0.9901 - val_loss: 1.4829 - val_accuracy: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [03:33, 213.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 8\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[425 636 101 586 163 307 653 325  42 344  71 372 244 616 351 167 781 447\n",
      " 433 270 113 670  16 611 340 237 757 763 151 376 558  80  49 145 100 107\n",
      " 385 730 285 517 177 472 689 709 170 631 139 512 533  61  81 349 568 369\n",
      " 207  36 432 705 347 475  72 187 493 159 233  66 538 483  58 407 515  68\n",
      " 267 572 541 179 762 162 575 485 138 332 188 735 264 214  93 742 121 401\n",
      " 442 527 543 178  25 157 355 704 567 498  57 503 450 675  96  44 471 231\n",
      " 274 615 526 220 259 393  33 239 479 273 576 354 727  65 246 711 120 462\n",
      " 497 257 734 658 169 258 377 280 324 415 175  23 278 346 662 648 647 639\n",
      " 211 717 127 646 619 779 690 625 655   8 337 587 122 291 747 116 680 262\n",
      " 222  62 651 635 168 621 663 529 165 206 380 584 144 756 158 290 232 195\n",
      " 461 728 227 761 104 506 353   2  53 564 626 379 141 356 583 703 725 605\n",
      " 681 313 549 746 318 579 737 384 423 212  85 570 125 143 581   6  91 183\n",
      " 171 767 766  87 192 706 361 312 185 665 729 492 119 438  41 209  19 316\n",
      " 191 350 521  47 352 731 428 614 263 368 130  60 457 115 589 197 230 523\n",
      " 146 514 394 223 281 180 620 645 303 686 424 677 463 365 585 666  11 128\n",
      " 708 182 370 778 391 693  27 685 599 467 245 562 723 669 189 536 133 439\n",
      " 252 741 712  21 421 490 760 338 775 357 509 362 296 469 240  69 150 667\n",
      " 759 574 276 569 248 540 566 411 437 607  84 528 634   9  86 643 534 664\n",
      " 470 216 546 446 367 251  17 765  43 310  12 323 659 507  46 500 745 502\n",
      " 482 416 565 203 721 256 764 284  90 265  82 427 780 156 364 743 277 713\n",
      " 491 249 519 602 409 293 408 477 271 147 435 210 656 525  88  31 484 770\n",
      " 592 105  29 672  37 452 451 261 628 674 132 738 516 505 600 459 718  34\n",
      " 744 580  97 740 752 773 190 440 719 218 736 389 473  26 326 358 596 229\n",
      " 539 449 604 110 308 306  67 466 194  14  76 413 363 444 486 465 103 531\n",
      " 236  56 398 513 176 673 578 771 371 343 317 268 489 632 392  63 637 700\n",
      " 618 548 595 649 366 142 154 399 496 118 254  51 373 671 166 381 588 554\n",
      " 298  94  83 181 124 149 426 213 375 598 199 769 126 495 460 228 387   1\n",
      " 556 701 640 520 241 577 654 692 571 707 481 726 219  55 474 678 499 739\n",
      " 691 501 328  54 330 311 487 106 205 275 378 683 112 597 131 315 255 594\n",
      " 550 714 402 453 582 283 334 715  74 783 406 448 768 314 410 591 383 301\n",
      " 117 336 235 160 560 547 390  92  32 545 532 295 108 215   5 750 782 224\n",
      " 650  52 633 494 302  35 694 774 282 198  39 174 331 687 535 608  28 555\n",
      " 418 134  45  78 417 217 309  24 724 601 152 114 294  20 260 345 511 186\n",
      "  77 468 388 458   7 420 436  50  22 552 638 720 748 129 289  64 696 140\n",
      " 676 123 429 610 286 524  59 652 551 542 557 299 518 563 679   0 135 319\n",
      " 305  79 749  89 609 476 327 488  40 204 266 234 751 321 434 360 630 225\n",
      " 622 733 269 208 253 732 287 629  13 196 322 641 242 395 559 772 320 530\n",
      " 606 419 716  48 297 697 537 153 397 722 613 148 657 777 200 443 412 573\n",
      " 161  95  70 202 510 155  15 699 623 702 454 508 455 754 612 480 753 698\n",
      " 341 400 445 441 456 644 590 279  99 333 661 603 250 522 288 684 226   4\n",
      " 184 624 342 668 431 272  30 172 173 164 136  73 359 758 405 329  75 300\n",
      " 617 688 695 382 430 710 464 238 396 221 348 561 193 339 593   3 111 201\n",
      " 292 553 660 776 682 102 335 374 403 404 137  18 642 247  10 504  38 478\n",
      " 109 627 544 422 304 414 386 243 755  98]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[274 163 299 278 100 143 216 224 169 148 123 121 269 147  37 122 120  96\n",
      " 268 188 140  28 242 270 156  11  47 238  63  67 260 165 112  31 166 208\n",
      "  15 279 189  71 172 249 110 107  75 136  25  40 273 153  43  82  48 243\n",
      " 233 252 283  57  46  79 246 294 201 284 251 210 214 221 167 149 266 194\n",
      " 263 293 258 248 106  59 161 109  99 139 176 174 286 272  62 177 277  42\n",
      " 264 200 265 108 235  56 285 196 262 250 199   5 240  64 241  16 164  21\n",
      "  33 204 271 297  22 130 222  89 244 203   0   1 230  69 195 253 197 236\n",
      " 229 105   2  58 180 155  14  20 111 275 125  27   4 183 115  76 154 150\n",
      " 162 118  60 207 256 205  87  41 255 133 226 102 192 220  88 137  92 185\n",
      "  66 282 280 151 267 281 228  98 292 101  74 247  49 206  26 113 213 187\n",
      " 217  13  85 104 116  23 198 209 134  81  93 171  95  52  12 117  73 287\n",
      " 191  10  90  29 158 225 128  32 218 157 138 178 141 160  50 290  35 142\n",
      "   9 295   3 170  45 288  30  24   7 291 212 227 145 103  55 127  34 193\n",
      " 223  18 168  70 124 184  86 289 237  84 114  39 232 179 219  72 257 261\n",
      "  44  54 159 129 234 173  77 175 215 259 211 146 152   6  83 132 296  51\n",
      " 135  19 239 202 119 190  91 298  94  17  36  65 182  53  78 254 144  38\n",
      " 181   8 126 231  97  68 131  80 186  61 276 245]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[ 6 12 27 28  7 19 77 23 13 55 89 84  1 36 42 81 72 76 65 21 80 92  0 10\n",
      " 93 82 54 26 11 68 22 94 34 14 46 41 88 78  4 70 47 97 90 25 86 64 49  5\n",
      " 59 29  3 20 99 57 71 53 32 39 91 40 37 24 85 56 66 58 16 15  9 38 18 50\n",
      " 79 31 17 83 69 45 61 74 35  8 43 87 33 60 63  2 51 62 75 67 96 98 30 48\n",
      " 73 95 44 52]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4948 - accuracy: 0.9729 - val_loss: 1.4918 - val_accuracy: 0.9716\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9843 - val_loss: 1.4894 - val_accuracy: 0.9739\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9876 - val_loss: 1.4884 - val_accuracy: 0.9744\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4737 - accuracy: 0.9892 - val_loss: 1.4873 - val_accuracy: 0.9741\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4720 - accuracy: 0.9907 - val_loss: 1.4860 - val_accuracy: 0.9759\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9911 - val_loss: 1.4856 - val_accuracy: 0.9764\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9919 - val_loss: 1.4852 - val_accuracy: 0.9769\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4698 - accuracy: 0.9924 - val_loss: 1.4856 - val_accuracy: 0.9766\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4694 - accuracy: 0.9925 - val_loss: 1.4841 - val_accuracy: 0.9773\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4685 - accuracy: 0.9933 - val_loss: 1.4859 - val_accuracy: 0.9754\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4683 - accuracy: 0.9936 - val_loss: 1.4838 - val_accuracy: 0.9784\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4682 - accuracy: 0.9937 - val_loss: 1.4848 - val_accuracy: 0.9766\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4678 - accuracy: 0.9938 - val_loss: 1.4876 - val_accuracy: 0.9741\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4677 - accuracy: 0.9939 - val_loss: 1.4857 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [05:40, 187.59s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 8\n",
      "rows to prune in layer 0 : 588\n",
      "[  0   1   3   4   5   7  10  13  14  15  18  20  22  24  26  28  30  32\n",
      "  34  35  38  39  40  45  48  50  51  52  54  55  56  59  63  64  67  70\n",
      "  73  74  75  76  77  78  79  83  89  92  94  95  97  98  99 102 103 106\n",
      " 108 109 110 111 112 114 117 118 123 124 126 129 131 134 135 136 137 140\n",
      " 142 148 149 152 153 154 155 160 161 164 166 172 173 174 176 181 184 186\n",
      " 190 193 194 196 198 199 200 201 202 204 205 208 213 215 217 218 219 221\n",
      " 224 225 226 228 229 234 235 236 238 241 242 243 247 250 253 254 255 260\n",
      " 266 268 269 272 275 279 282 283 286 287 288 289 292 294 295 297 298 299\n",
      " 300 301 302 304 305 306 308 309 311 314 315 317 319 320 321 322 326 327\n",
      " 328 329 330 331 333 334 335 336 339 341 342 343 345 348 358 359 360 363\n",
      " 366 371 373 374 375 378 381 382 383 386 387 388 389 390 392 395 396 397\n",
      " 398 399 400 402 403 404 405 406 410 412 413 414 417 418 419 420 422 426\n",
      " 429 430 431 434 436 440 441 443 444 445 448 449 453 454 455 456 458 459\n",
      " 460 464 465 466 468 473 474 476 478 480 481 486 487 488 489 494 495 496\n",
      " 499 501 504 508 510 511 513 518 520 522 524 530 531 532 535 537 539 542\n",
      " 544 545 547 548 550 551 552 553 554 555 556 557 559 560 561 563 571 573\n",
      " 577 578 580 582 588 590 591 593 594 595 596 597 598 600 601 603 604 606\n",
      " 608 609 610 612 613 617 618 622 623 624 627 629 630 632 633 637 638 640\n",
      " 641 642 644 649 650 652 654 657 660 661 668 671 673 676 678 679 682 683\n",
      " 684 687 688 691 692 694 695 696 697 698 699 700 701 702 707 710 714 715\n",
      " 716 718 719 720 722 724 726 732 733 736 739 740 744 748 749 750 751 752\n",
      " 753 754 755 758 768 769 771 772 773 774 776 777 782 783]\n",
      "[388 392 135 129 260 630 402 695 406 736 186 668 103 559 229  34 215 137\n",
      " 715 397 436 311 688 720 339 319 578  99 443 255 345 326 328 110 601 476\n",
      "  20 716 124 269 166 193  18 373 520 638 172 272 360 405  13 618  64 184\n",
      " 691  40 641 434 224  56  83 389 221  10 123 199  35 400  28 164 386 750\n",
      " 381 577 683  89 247 395  30 531 204 111 330 481 396 404 561  50 593 218\n",
      " 642 496 118   1 106 464  63 459 594 213 292 623 348 194  14 295 363 542\n",
      " 772 305 518 286 288 478 219 508 126 456 136 308 633 522 687 554 289 448\n",
      " 225 748 341 190 557  77 480 196 334 152 545 398 783 604 302 253  75  38\n",
      " 551 282 176 660 441 430 632 571  97 555 201 754  59 588 494 321 117 440\n",
      " 758   5 383 142 335  79 649 682 657 556 676 684 600 403 539 109  26 486\n",
      "  73 692 454 544 329 333 417 488 726 744 234 250 283 697 205  95 208 774\n",
      " 596  67 460 429 679 331  94 532 732 468 535 530 200 449   0 613 382 342\n",
      "   4 287 181 327 606 309 699 236 701 304 466 552 343 243 640 487  98  32\n",
      " 300 444 431 149 591 671  76 510 504 226 610 297 317 241 426 573 453 235\n",
      " 268 134 420 718   3 595 627 390 598 242 777 694  78 140  48 654 513 418\n",
      " 768 582 315 661 719 458 560 773 314 254 612 603 710 455  54 298 366 722\n",
      " 714 378 700 413  55 198 524 320 112 358 547 374 652 673 217 751  22 160\n",
      " 707 696 771 617 499 473 322  39 419 590 622 114 410 336 155 299 154  24\n",
      " 739 495  51 537  74 733 563 769 173 375 131 422  15 371 266 550 609 301\n",
      " 755 148 650 153 412 629  92 474 279 174 511 108 637 359 678 489 238 306\n",
      " 553 275 228 644 753 445 399  70  45 294 608 597 752 624  52 749 702 698\n",
      " 580 501 740 782 387 548 102 724 202 414   7 776 465 161]\n",
      "rows to prune in layer 3 : 225\n",
      "[  3   6   7   8   9  10  12  13  17  18  19  23  24  26  29  30  32  34\n",
      "  35  36  38  39  41  44  45  49  50  51  52  53  54  55  61  65  66  68\n",
      "  70  72  73  74  77  78  80  81  83  84  85  86  87  88  90  91  92  93\n",
      "  94  95  97  98 101 102 103 104 113 114 116 117 119 124 126 127 128 129\n",
      " 131 132 133 134 135 137 138 141 142 144 145 146 151 152 157 158 159 160\n",
      " 168 170 171 173 175 178 179 181 182 184 185 186 187 190 191 192 193 198\n",
      " 202 206 209 211 212 213 215 217 218 219 220 223 225 226 227 228 231 232\n",
      " 234 237 239 245 247 254 255 257 259 261 267 276 280 281 282 287 288 289\n",
      " 290 291 292 295 296 298]\n",
      "[239  86 144   9 298  87  45 211  54 288 104 184  70 191 232 157  26  94\n",
      " 245  23 124 116 175  50 137 289  74  18  17 261  95  65 295 151  38 190\n",
      "  97 234 291  52 186 117 135 198 182 209 220 134 127  35 296 179  78  44\n",
      "  30 193  84 228  41 178 292 142  13 171 276 129 259 280 138   6 141 160\n",
      " 255  49 217 257 219 152 226 187 237 223  98  55  77  29 213  68 206 287\n",
      " 102   7  88 247 282 119 113 181 128 170 202  90  24 290  92 132  32  10\n",
      " 231  39   8  53  73 215  81 267 133 281  51 227 173  36 145 225  12 254\n",
      " 212  66 146 192  34 168 126  61  80 131 101 158 159  91  93  72  19  83\n",
      " 218  85 185   3 114 103]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 2  3  8  9 15 16 17 18 20 24 30 31 32 33 35 37 38 39 40 43 44 45 48 50\n",
      " 51 52 53 56 57 58 60 61 62 63 66 67 69 71 73 74 75 79 83 85 87 91 95 96\n",
      " 98 99]\n",
      "[79 96 40 39 50 52 66 95 24  9 33  3 91 98 35 83 38 71 67 48 74 15 30  2\n",
      " 61 73 17 18 51 87 45 53 16 63 32 37 99 60  8 31 85 56 20 58 44 75 43 69\n",
      " 57 62]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5536 - accuracy: 0.9278 - val_loss: 1.5251 - val_accuracy: 0.9440\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5147 - accuracy: 0.9539 - val_loss: 1.5156 - val_accuracy: 0.9509\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5057 - accuracy: 0.9616 - val_loss: 1.5124 - val_accuracy: 0.9524\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5000 - accuracy: 0.9660 - val_loss: 1.5074 - val_accuracy: 0.9573\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4959 - accuracy: 0.9700 - val_loss: 1.5053 - val_accuracy: 0.9593\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4930 - accuracy: 0.9723 - val_loss: 1.5044 - val_accuracy: 0.9593\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4906 - accuracy: 0.9740 - val_loss: 1.5036 - val_accuracy: 0.9595\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4884 - accuracy: 0.9761 - val_loss: 1.5022 - val_accuracy: 0.9606\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4867 - accuracy: 0.9776 - val_loss: 1.5028 - val_accuracy: 0.9607\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4854 - accuracy: 0.9788 - val_loss: 1.5023 - val_accuracy: 0.9606\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4840 - accuracy: 0.9800 - val_loss: 1.5008 - val_accuracy: 0.9617\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9804 - val_loss: 1.4999 - val_accuracy: 0.9628\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4821 - accuracy: 0.9814 - val_loss: 1.5007 - val_accuracy: 0.9609\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4814 - accuracy: 0.9822 - val_loss: 1.4987 - val_accuracy: 0.9634\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4805 - accuracy: 0.9828 - val_loss: 1.4993 - val_accuracy: 0.9633\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4799 - accuracy: 0.9833 - val_loss: 1.4989 - val_accuracy: 0.9631\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4793 - accuracy: 0.9836 - val_loss: 1.4989 - val_accuracy: 0.9625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [07:47, 169.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 8\n",
      "rows to prune in layer 0 : 686\n",
      "[  0   3   4   7  15  22  24  32  39  45  48  51  52  54  55  67  70  74\n",
      "  76  78  92  94  98 102 108 112 114 131 134 140 148 149 153 154 155 160\n",
      " 161 173 174 181 198 200 202 208 217 226 228 235 236 238 241 242 243 254\n",
      " 266 268 275 279 287 294 297 298 299 300 301 304 306 309 314 315 317 320\n",
      " 322 327 331 336 342 343 358 359 366 371 374 375 378 382 387 390 399 410\n",
      " 412 413 414 418 419 420 422 426 429 431 444 445 449 453 455 458 460 465\n",
      " 466 468 473 474 487 489 495 499 501 504 510 511 513 524 530 532 535 537\n",
      " 547 548 550 552 553 560 563 573 580 582 590 591 595 596 597 598 603 606\n",
      " 608 609 610 612 613 617 622 624 627 629 637 640 644 650 652 654 661 671\n",
      " 673 678 679 694 696 698 699 700 701 702 707 710 714 718 719 722 724 732\n",
      " 733 739 740 749 751 752 753 755 768 769 771 773 774 776 777 782]\n",
      "[238 597 342 699 774 654 591 701 148 698 412 782 449 304 590 134  52 198\n",
      " 298 751 315 724 297 114 331 710 306  94 487 155 173  24 444 552 707 242\n",
      " 661 112 200   3 550 563 769 637 617 776 429 598  48 102 719 535 300 327\n",
      " 241 243  98 755 673 375 595 374 445 560 458 694 287 254 431   4 414 131\n",
      " 322 612 696  22 455 644 580  70 714 530 768  55  45 422 603  78 652 722\n",
      " 671 733 537 752 718 489  51 418 202 466 108 622 314 208 739  54 773 153\n",
      " 275 140 426  67 336 174 235 228 309 702   7 582 399  39 358 606  92 640\n",
      " 359 532 474 753 511 495 371  32 777 410 366 679 149 524  74 627 501 301\n",
      " 378 465 268 217 299 161 749 420 390 181 413 320 608 499 453 382 160 624\n",
      " 573 279 504 473  15 387 629 732 460 610 317 700 294 236  76 266 650 510\n",
      " 548 596   0 609 547 771 419 154 226 468 740 513 553 678 613 343]\n",
      "rows to prune in layer 3 : 262\n",
      "[  3   7   8  10  12  19  24  29  32  34  36  39  51  53  55  61  66  68\n",
      "  72  73  77  80  81  83  85  88  90  91  92  93  98 101 102 103 113 114\n",
      " 119 126 128 131 132 133 145 146 152 158 159 168 170 173 181 185 187 192\n",
      " 202 206 212 213 215 218 219 223 225 226 227 231 237 247 254 257 267 281\n",
      " 282 287 290]\n",
      "[170  80  53  24  92  90  73  66  12 218  88 101 223 213 152  29 290 227\n",
      " 206 282  77   7 257  83 103 226 119  72  61 145 231 113 287 247  32 102\n",
      "  93  19  68  39 126 181 131 225 219 158 254 114 146 212 168 237  85 159\n",
      "  91  51 128 215 202 281  36 173 132  81  55 192 187   3 185 267  98  10\n",
      "   8 133  34]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 8 16 17 18 20 31 32 37 43 44 45 51 53 56 57 58 60 62 63 69 73 75 85 87\n",
      " 99]\n",
      "[16 60 73 69 53 99 51 75 62 43 45 85 32 56 37  8 18 31 87 57 58 20 44 17\n",
      " 63]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7081 - accuracy: 0.7821 - val_loss: 1.6029 - val_accuracy: 0.8750\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5919 - accuracy: 0.8834 - val_loss: 1.5824 - val_accuracy: 0.8864\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5754 - accuracy: 0.8965 - val_loss: 1.5716 - val_accuracy: 0.8966\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5666 - accuracy: 0.9036 - val_loss: 1.5674 - val_accuracy: 0.8993\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5606 - accuracy: 0.9086 - val_loss: 1.5624 - val_accuracy: 0.9051\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5560 - accuracy: 0.9124 - val_loss: 1.5616 - val_accuracy: 0.9038\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5526 - accuracy: 0.9151 - val_loss: 1.5572 - val_accuracy: 0.9090\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5497 - accuracy: 0.9174 - val_loss: 1.5542 - val_accuracy: 0.9121\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5470 - accuracy: 0.9199 - val_loss: 1.5551 - val_accuracy: 0.9100\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5448 - accuracy: 0.9213 - val_loss: 1.5519 - val_accuracy: 0.9134\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5428 - accuracy: 0.9237 - val_loss: 1.5519 - val_accuracy: 0.9132\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5409 - accuracy: 0.9255 - val_loss: 1.5506 - val_accuracy: 0.9127\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5393 - accuracy: 0.9270 - val_loss: 1.5493 - val_accuracy: 0.9147\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5380 - accuracy: 0.9282 - val_loss: 1.5489 - val_accuracy: 0.9152\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5366 - accuracy: 0.9292 - val_loss: 1.5488 - val_accuracy: 0.9141\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5357 - accuracy: 0.9293 - val_loss: 1.5481 - val_accuracy: 0.9142\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5343 - accuracy: 0.9312 - val_loss: 1.5461 - val_accuracy: 0.9165\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5332 - accuracy: 0.9320 - val_loss: 1.5479 - val_accuracy: 0.9150\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5325 - accuracy: 0.9331 - val_loss: 1.5454 - val_accuracy: 0.9171\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5317 - accuracy: 0.9336 - val_loss: 1.5450 - val_accuracy: 0.9182\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5308 - accuracy: 0.9344 - val_loss: 1.5445 - val_accuracy: 0.9187\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5299 - accuracy: 0.9351 - val_loss: 1.5446 - val_accuracy: 0.9187\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5294 - accuracy: 0.9354 - val_loss: 1.5442 - val_accuracy: 0.9189\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5287 - accuracy: 0.9366 - val_loss: 1.5442 - val_accuracy: 0.9178\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5280 - accuracy: 0.9365 - val_loss: 1.5445 - val_accuracy: 0.9177\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5276 - accuracy: 0.9368 - val_loss: 1.5436 - val_accuracy: 0.9184\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5271 - accuracy: 0.9377 - val_loss: 1.5437 - val_accuracy: 0.9183\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5264 - accuracy: 0.9380 - val_loss: 1.5429 - val_accuracy: 0.9197\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5264 - accuracy: 0.9382 - val_loss: 1.5432 - val_accuracy: 0.9187\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5255 - accuracy: 0.9391 - val_loss: 1.5444 - val_accuracy: 0.9180\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5252 - accuracy: 0.9393 - val_loss: 1.5418 - val_accuracy: 0.9211\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5249 - accuracy: 0.9395 - val_loss: 1.5411 - val_accuracy: 0.9220\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5243 - accuracy: 0.9401 - val_loss: 1.5444 - val_accuracy: 0.9182\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5240 - accuracy: 0.9401 - val_loss: 1.5420 - val_accuracy: 0.9201\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5236 - accuracy: 0.9402 - val_loss: 1.5419 - val_accuracy: 0.9203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [10:45, 171.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 8\n",
      "rows to prune in layer 0 : 735\n",
      "[  0   7  15  32  39  54  67  74  76  92 108 140 149 153 154 160 161 174\n",
      " 181 202 208 217 226 228 235 236 266 268 275 279 294 299 301 309 314 317\n",
      " 320 336 343 358 359 366 371 378 382 387 390 399 410 413 419 420 426 453\n",
      " 460 465 466 468 473 474 495 499 501 504 510 511 513 524 532 547 548 553\n",
      " 573 582 596 606 608 609 610 613 622 624 627 629 640 650 678 679 700 702\n",
      " 732 739 740 749 753 771 773 777]\n",
      "[378 228 279 532 426 140 548 608 499 650 622 343 468  76 153 749   0 266\n",
      " 627 495 753 453 268 771 511 640 336 573 420 236 501 740 702 309 317 474\n",
      " 413 299 473 181 359 629 390 596 466 678 732 161 371  67 679  54 217 154\n",
      " 610 366 174 149 700 301   7  39 739 358 275 609 624 410 226 524 202 314\n",
      " 510  74  92 320 513  15 399 613 160 504 419 773 553 208 108 547  32 235\n",
      " 387 582 777 460 465 606 294 382]\n",
      "rows to prune in layer 3 : 281\n",
      "[  3   8  10  19  34  36  39  51  55  68  81  85  91  98 114 126 128 131\n",
      " 132 133 146 158 159 168 173 181 185 187 192 202 212 215 219 225 237 254\n",
      " 267 281]\n",
      "[128 254 237 181 131 212 168 173   3 114  68 192 146  55   8  81 185 215\n",
      "  91  34  51 132  36 267  39  85  10 281 158  98 126 219 187 159  19 225\n",
      " 202 133]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 8 17 18 20 31 32 37 44 56 57 58 63 87]\n",
      "[18 32 31 58 20 17 63 87 44 56  8 57 37]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0033 - accuracy: 0.4628 - val_loss: 1.8950 - val_accuracy: 0.5770\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8446 - accuracy: 0.6284 - val_loss: 1.8126 - val_accuracy: 0.6586\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8097 - accuracy: 0.6593 - val_loss: 1.7918 - val_accuracy: 0.6789\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7950 - accuracy: 0.6723 - val_loss: 1.7805 - val_accuracy: 0.6880\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7854 - accuracy: 0.6815 - val_loss: 1.7745 - val_accuracy: 0.6933\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7782 - accuracy: 0.6886 - val_loss: 1.7722 - val_accuracy: 0.6932\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7725 - accuracy: 0.6936 - val_loss: 1.7626 - val_accuracy: 0.7052\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7678 - accuracy: 0.6983 - val_loss: 1.7590 - val_accuracy: 0.7078\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7638 - accuracy: 0.7017 - val_loss: 1.7563 - val_accuracy: 0.7095\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7601 - accuracy: 0.7046 - val_loss: 1.7537 - val_accuracy: 0.7092\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7555 - accuracy: 0.7097 - val_loss: 1.7506 - val_accuracy: 0.7143\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7510 - accuracy: 0.7139 - val_loss: 1.7460 - val_accuracy: 0.7183\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7473 - accuracy: 0.7174 - val_loss: 1.7436 - val_accuracy: 0.7195\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7441 - accuracy: 0.7213 - val_loss: 1.7412 - val_accuracy: 0.7226\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7411 - accuracy: 0.7246 - val_loss: 1.7378 - val_accuracy: 0.7266\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7387 - accuracy: 0.7262 - val_loss: 1.7357 - val_accuracy: 0.7274\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7366 - accuracy: 0.7276 - val_loss: 1.7334 - val_accuracy: 0.7303\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7345 - accuracy: 0.7293 - val_loss: 1.7310 - val_accuracy: 0.7320\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7264 - accuracy: 0.7368 - val_loss: 1.7127 - val_accuracy: 0.7506\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7141 - accuracy: 0.7505 - val_loss: 1.7064 - val_accuracy: 0.7585\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7095 - accuracy: 0.7554 - val_loss: 1.7046 - val_accuracy: 0.7571\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7064 - accuracy: 0.7580 - val_loss: 1.7009 - val_accuracy: 0.7628\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7036 - accuracy: 0.7605 - val_loss: 1.7000 - val_accuracy: 0.7639\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7016 - accuracy: 0.7632 - val_loss: 1.6986 - val_accuracy: 0.7653\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7004 - accuracy: 0.7641 - val_loss: 1.6972 - val_accuracy: 0.7667\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6989 - accuracy: 0.7651 - val_loss: 1.6973 - val_accuracy: 0.7655\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6981 - accuracy: 0.7663 - val_loss: 1.6948 - val_accuracy: 0.7701\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6971 - accuracy: 0.7669 - val_loss: 1.6939 - val_accuracy: 0.7699\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6963 - accuracy: 0.7674 - val_loss: 1.6934 - val_accuracy: 0.7699\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6953 - accuracy: 0.7689 - val_loss: 1.6933 - val_accuracy: 0.7699\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6942 - accuracy: 0.7704 - val_loss: 1.6912 - val_accuracy: 0.7730\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6938 - accuracy: 0.7702 - val_loss: 1.6923 - val_accuracy: 0.7704\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6929 - accuracy: 0.7710 - val_loss: 1.6911 - val_accuracy: 0.7724\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6926 - accuracy: 0.7717 - val_loss: 1.6920 - val_accuracy: 0.7721\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6918 - accuracy: 0.7726 - val_loss: 1.6900 - val_accuracy: 0.7723\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6909 - accuracy: 0.7732 - val_loss: 1.6912 - val_accuracy: 0.7713\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6908 - accuracy: 0.7731 - val_loss: 1.6905 - val_accuracy: 0.7713\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6902 - accuracy: 0.7733 - val_loss: 1.6908 - val_accuracy: 0.7701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [13:56, 177.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 8\n",
      "rows to prune in layer 0 : 759\n",
      "[  7  15  32  39  54  67  74  92 108 149 154 160 174 202 208 217 226 235\n",
      " 275 294 301 314 320 358 366 382 387 399 410 419 460 465 504 510 513 524\n",
      " 547 553 582 606 609 610 613 624 679 700 739 773 777]\n",
      "[ 54 301 609 624 208 524 174 294 777   7 510 547 154 382 610 773 226 399\n",
      " 553 460 700 410  15 149 419  67 160  32 358 235 465 739 217 387 582 606\n",
      " 202 108 613 320 679 366  92  74  39 275 504 314 513]\n",
      "rows to prune in layer 3 : 290\n",
      "[ 10  19  34  36  39  51  85  98 126 132 133 158 159 187 202 219 225 267\n",
      " 281]\n",
      "[267  51  39  36 281 225 132  19 126 187 202 133 219  98 158  34  85 159\n",
      "  10]\n",
      "rows to prune in layer 6 : 96\n",
      "[ 8 37 44 56 57 63 87]\n",
      "[44 56 37 57 87 63  8]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1495 - accuracy: 0.3058 - val_loss: 2.1016 - val_accuracy: 0.3581\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0891 - accuracy: 0.3706 - val_loss: 2.0685 - val_accuracy: 0.3906\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0626 - accuracy: 0.4013 - val_loss: 2.0486 - val_accuracy: 0.4193\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0454 - accuracy: 0.4221 - val_loss: 2.0359 - val_accuracy: 0.4325\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0279 - accuracy: 0.4429 - val_loss: 2.0179 - val_accuracy: 0.4536\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0087 - accuracy: 0.4650 - val_loss: 2.0000 - val_accuracy: 0.4717\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9942 - accuracy: 0.4786 - val_loss: 1.9873 - val_accuracy: 0.4834\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9837 - accuracy: 0.4874 - val_loss: 1.9789 - val_accuracy: 0.4885\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9759 - accuracy: 0.4940 - val_loss: 1.9736 - val_accuracy: 0.4938\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9703 - accuracy: 0.4979 - val_loss: 1.9689 - val_accuracy: 0.4981\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9661 - accuracy: 0.5010 - val_loss: 1.9653 - val_accuracy: 0.5004\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9628 - accuracy: 0.5028 - val_loss: 1.9632 - val_accuracy: 0.5001\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9601 - accuracy: 0.5050 - val_loss: 1.9607 - val_accuracy: 0.5022\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9577 - accuracy: 0.5067 - val_loss: 1.9580 - val_accuracy: 0.5044\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9558 - accuracy: 0.5083 - val_loss: 1.9568 - val_accuracy: 0.5069\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9541 - accuracy: 0.5095 - val_loss: 1.9544 - val_accuracy: 0.5088\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9525 - accuracy: 0.5100 - val_loss: 1.9525 - val_accuracy: 0.5110\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9512 - accuracy: 0.5113 - val_loss: 1.9508 - val_accuracy: 0.5133\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9501 - accuracy: 0.5125 - val_loss: 1.9496 - val_accuracy: 0.5139\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9491 - accuracy: 0.5130 - val_loss: 1.9488 - val_accuracy: 0.5141\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9482 - accuracy: 0.5136 - val_loss: 1.9484 - val_accuracy: 0.5148\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9475 - accuracy: 0.5139 - val_loss: 1.9475 - val_accuracy: 0.5145\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9468 - accuracy: 0.5145 - val_loss: 1.9466 - val_accuracy: 0.5158\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9459 - accuracy: 0.5157 - val_loss: 1.9464 - val_accuracy: 0.5161\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9451 - accuracy: 0.5157 - val_loss: 1.9441 - val_accuracy: 0.5172\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9435 - accuracy: 0.5170 - val_loss: 1.9431 - val_accuracy: 0.5191\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9422 - accuracy: 0.5182 - val_loss: 1.9421 - val_accuracy: 0.5187\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9414 - accuracy: 0.5188 - val_loss: 1.9410 - val_accuracy: 0.5187\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9406 - accuracy: 0.5192 - val_loss: 1.9402 - val_accuracy: 0.5219\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9400 - accuracy: 0.5203 - val_loss: 1.9398 - val_accuracy: 0.5212\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9393 - accuracy: 0.5207 - val_loss: 1.9397 - val_accuracy: 0.5214\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9388 - accuracy: 0.5210 - val_loss: 1.9386 - val_accuracy: 0.5216\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9385 - accuracy: 0.5212 - val_loss: 1.9380 - val_accuracy: 0.5222\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9380 - accuracy: 0.5221 - val_loss: 1.9379 - val_accuracy: 0.5234\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9377 - accuracy: 0.5219 - val_loss: 1.9382 - val_accuracy: 0.5225\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9374 - accuracy: 0.5224 - val_loss: 1.9374 - val_accuracy: 0.5218\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9372 - accuracy: 0.5224 - val_loss: 1.9371 - val_accuracy: 0.5231\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9369 - accuracy: 0.5225 - val_loss: 1.9368 - val_accuracy: 0.5229\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9366 - accuracy: 0.5228 - val_loss: 1.9359 - val_accuracy: 0.5247\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9363 - accuracy: 0.5233 - val_loss: 1.9362 - val_accuracy: 0.5247\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9360 - accuracy: 0.5235 - val_loss: 1.9359 - val_accuracy: 0.5235\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9358 - accuracy: 0.5239 - val_loss: 1.9362 - val_accuracy: 0.5234\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9356 - accuracy: 0.5242 - val_loss: 1.9359 - val_accuracy: 0.5231\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9355 - accuracy: 0.5242 - val_loss: 1.9355 - val_accuracy: 0.5224\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9354 - accuracy: 0.5242 - val_loss: 1.9354 - val_accuracy: 0.5246\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9351 - accuracy: 0.5241 - val_loss: 1.9351 - val_accuracy: 0.5241\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9350 - accuracy: 0.5246 - val_loss: 1.9350 - val_accuracy: 0.5250\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9346 - accuracy: 0.5257 - val_loss: 1.9344 - val_accuracy: 0.5256\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9345 - accuracy: 0.5252 - val_loss: 1.9348 - val_accuracy: 0.5239\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9344 - accuracy: 0.5252 - val_loss: 1.9348 - val_accuracy: 0.5243\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9341 - accuracy: 0.5257 - val_loss: 1.9340 - val_accuracy: 0.5258\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9340 - accuracy: 0.5255 - val_loss: 1.9349 - val_accuracy: 0.5232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9339 - accuracy: 0.5258 - val_loss: 1.9336 - val_accuracy: 0.5268\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9337 - accuracy: 0.5262 - val_loss: 1.9338 - val_accuracy: 0.5249\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9336 - accuracy: 0.5265 - val_loss: 1.9339 - val_accuracy: 0.5245\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9334 - accuracy: 0.5264 - val_loss: 1.9334 - val_accuracy: 0.5263\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9334 - accuracy: 0.5266 - val_loss: 1.9332 - val_accuracy: 0.5265\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9333 - accuracy: 0.5263 - val_loss: 1.9338 - val_accuracy: 0.5265\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9329 - accuracy: 0.5273 - val_loss: 1.9326 - val_accuracy: 0.5268\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9329 - accuracy: 0.5268 - val_loss: 1.9332 - val_accuracy: 0.5267\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9327 - accuracy: 0.5268 - val_loss: 1.9335 - val_accuracy: 0.5265\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9326 - accuracy: 0.5272 - val_loss: 1.9328 - val_accuracy: 0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [17:25, 187.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 8\n",
      "rows to prune in layer 0 : 771\n",
      "[ 32  39  67  74  92 108 160 202 217 235 275 314 320 358 366 387 419 465\n",
      " 504 513 582 606 613 679 739]\n",
      "[160  39 613 739 419 358 320 679 606  92 387  67 314  32 217 504 582 513\n",
      " 235 108  74 202 366 275 465]\n",
      "rows to prune in layer 3 : 295\n",
      "[ 10  34  85  98 133 158 159 187 202 219]\n",
      "[159 202 219  98  10  85 133 187  34 158]\n",
      "rows to prune in layer 6 : 98\n",
      "[ 8 57 63 87]\n",
      "[87  8 57 63]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2337 - accuracy: 0.2209 - val_loss: 2.2081 - val_accuracy: 0.2483\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2046 - accuracy: 0.2513 - val_loss: 2.2019 - val_accuracy: 0.2520\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1753 - accuracy: 0.2786 - val_loss: 2.1589 - val_accuracy: 0.2943\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1544 - accuracy: 0.3003 - val_loss: 2.1417 - val_accuracy: 0.3198\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1438 - accuracy: 0.3139 - val_loss: 2.1350 - val_accuracy: 0.3252\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1386 - accuracy: 0.3181 - val_loss: 2.1306 - val_accuracy: 0.3283\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1347 - accuracy: 0.3228 - val_loss: 2.1269 - val_accuracy: 0.3331\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1315 - accuracy: 0.3259 - val_loss: 2.1239 - val_accuracy: 0.3371\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1288 - accuracy: 0.3282 - val_loss: 2.1215 - val_accuracy: 0.3359\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1260 - accuracy: 0.3320 - val_loss: 2.1188 - val_accuracy: 0.3409\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1236 - accuracy: 0.3357 - val_loss: 2.1166 - val_accuracy: 0.3455\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1219 - accuracy: 0.3381 - val_loss: 2.1156 - val_accuracy: 0.3424\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1203 - accuracy: 0.3393 - val_loss: 2.1136 - val_accuracy: 0.3474\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1188 - accuracy: 0.3418 - val_loss: 2.1120 - val_accuracy: 0.3503\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1175 - accuracy: 0.3442 - val_loss: 2.1109 - val_accuracy: 0.3519\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1161 - accuracy: 0.3465 - val_loss: 2.1093 - val_accuracy: 0.3528\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1150 - accuracy: 0.3471 - val_loss: 2.1087 - val_accuracy: 0.3541\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1140 - accuracy: 0.3472 - val_loss: 2.1071 - val_accuracy: 0.3547\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1132 - accuracy: 0.3478 - val_loss: 2.1063 - val_accuracy: 0.3543\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1125 - accuracy: 0.3480 - val_loss: 2.1056 - val_accuracy: 0.3558\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1118 - accuracy: 0.3484 - val_loss: 2.1054 - val_accuracy: 0.3550\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1111 - accuracy: 0.3490 - val_loss: 2.1052 - val_accuracy: 0.3535\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1106 - accuracy: 0.3494 - val_loss: 2.1048 - val_accuracy: 0.3562\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1101 - accuracy: 0.3496 - val_loss: 2.1034 - val_accuracy: 0.3554\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1096 - accuracy: 0.3502 - val_loss: 2.1030 - val_accuracy: 0.3562\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1090 - accuracy: 0.3511 - val_loss: 2.1025 - val_accuracy: 0.3563\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1086 - accuracy: 0.3509 - val_loss: 2.1025 - val_accuracy: 0.3568\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1075 - accuracy: 0.3519 - val_loss: 2.1017 - val_accuracy: 0.3577\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1067 - accuracy: 0.3528 - val_loss: 2.1011 - val_accuracy: 0.3587\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1061 - accuracy: 0.3540 - val_loss: 2.1010 - val_accuracy: 0.3583\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1055 - accuracy: 0.3538 - val_loss: 2.1005 - val_accuracy: 0.3583\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1051 - accuracy: 0.3540 - val_loss: 2.1006 - val_accuracy: 0.3607\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1047 - accuracy: 0.3546 - val_loss: 2.0993 - val_accuracy: 0.3595\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1043 - accuracy: 0.3546 - val_loss: 2.0990 - val_accuracy: 0.3608\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1039 - accuracy: 0.3553 - val_loss: 2.0987 - val_accuracy: 0.3604\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1036 - accuracy: 0.3557 - val_loss: 2.0984 - val_accuracy: 0.3596\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1033 - accuracy: 0.3557 - val_loss: 2.0983 - val_accuracy: 0.3603\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1030 - accuracy: 0.3560 - val_loss: 2.0977 - val_accuracy: 0.3617\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1027 - accuracy: 0.3565 - val_loss: 2.0978 - val_accuracy: 0.3613\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1026 - accuracy: 0.3566 - val_loss: 2.0972 - val_accuracy: 0.3612\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1023 - accuracy: 0.3566 - val_loss: 2.0972 - val_accuracy: 0.3621\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1022 - accuracy: 0.3567 - val_loss: 2.0970 - val_accuracy: 0.3622\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1021 - accuracy: 0.3566 - val_loss: 2.0965 - val_accuracy: 0.3634\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1019 - accuracy: 0.3569 - val_loss: 2.0965 - val_accuracy: 0.3624\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1017 - accuracy: 0.3575 - val_loss: 2.0963 - val_accuracy: 0.3637\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1014 - accuracy: 0.3572 - val_loss: 2.0965 - val_accuracy: 0.3629\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1013 - accuracy: 0.3570 - val_loss: 2.0961 - val_accuracy: 0.3625\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1011 - accuracy: 0.3571 - val_loss: 2.0957 - val_accuracy: 0.3640\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1010 - accuracy: 0.3575 - val_loss: 2.0954 - val_accuracy: 0.3641\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1008 - accuracy: 0.3576 - val_loss: 2.0957 - val_accuracy: 0.3633\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1007 - accuracy: 0.3571 - val_loss: 2.0951 - val_accuracy: 0.3627\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1005 - accuracy: 0.3582 - val_loss: 2.0955 - val_accuracy: 0.3631\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1004 - accuracy: 0.3580 - val_loss: 2.0950 - val_accuracy: 0.3629\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1002 - accuracy: 0.3586 - val_loss: 2.0951 - val_accuracy: 0.3641\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1001 - accuracy: 0.3589 - val_loss: 2.0954 - val_accuracy: 0.3640\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0999 - accuracy: 0.3586 - val_loss: 2.0953 - val_accuracy: 0.3626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [20:06, 172.30s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [2:51:33<19:24, 1164.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5893 - accuracy: 0.8859 - val_loss: 1.5206 - val_accuracy: 0.9446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 9\n",
      "rows to prune in layer 0 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[377 651 579 337 633  63 739 179 715 114 311 693 423 678 643 105 150 253\n",
      " 775 232 436 516 496 583 181  58 513 342 607 225 306 211 274 431 741 435\n",
      " 476 202 703 662  39 300 544 501 360 185 489 159 331 217 212 233  91   6\n",
      " 210 708 352 771  36 316  64 667  53 635 540  69 241 608 173 193 525 162\n",
      " 756 127 492 134 615  55 547 348 373 191 657 384 221 459 585 229  83 629\n",
      " 493  60 514 638 668 611 239 155 765 639 198 165 456 763 151  30   7 692\n",
      " 280  41 347 292 444 332 140 190 420 252 673 705 139 590 732  52 712 604\n",
      "  97 479 470 164 554 675   8 661 245 688 408  90 452 731 628 726 364 404\n",
      " 582  13 495 182 370 621 358 261  56 367 224 587 158  21 419 577 684 573\n",
      " 310 606  92 398 718 557 101 234 571 214 522 298 205 553  42 646 175 412\n",
      " 709 576  62   4 725 391 581 285 494 169 399  40  32 541 283  74 595 437\n",
      "  46 552 281 282 271 199 374 777 333 450 269 634 390 626 509 500 361 163\n",
      "  94 520 637 325 588 674 778  86 154 250 490 110 551 730 428 388 338 249\n",
      " 178 538 192 511 680  54 313 531  80  57 502 524 272 567 180  81 350 694\n",
      " 238 319 357  93 472 305 574 189 120 562 695 343 485 772 736 434 303 415\n",
      "  95 146 432 100 425 355 536 658 237 545 286 550 749 401 689 160 392 148\n",
      " 659 167 670 275 770 243 746 594 438 326 681  50 596 157 201 454 644 727\n",
      " 462 762 430 592 773  29 123 312 666 482 240 379 527 614   5 781 478  26\n",
      " 108  16 293 260  34 723 410 711 526 170 706 104 417 593 433 296 258 600\n",
      " 766 714 407 262 386 453 747 546 488 247 743 598 631 382 153 710 556 340\n",
      " 752 324 564 346 738 251 207 187 334 317  18 295 344  20  11 617  27 751\n",
      " 396 291  82 559 353 698 505 672  51 177 530 464 701 679 560 779 117 327\n",
      " 636 696  17 776  45 589 622 244  71 369 508 294 757 248 138 742 359 116\n",
      " 440 466 468 754 700  33 716 215 195 487 321 461 414 455  49 469 648 649\n",
      " 335 220 439 135 411 568 769 196  98 188 519 642 669  66 682  73 363 421\n",
      " 413 486  88 683 578 737 290 416 284 449 328  15 106 304 142 102 315 704\n",
      " 218 690 394 605  31 713  47 702 256  67 307 591 498 663 612   2 671 697\n",
      " 288 624 186 418 748  19 314 385  77 601 572 354 515 640 627 270  44 308\n",
      " 491 650  35 471 602 276  61 641 656 653  99 767 287 402 279 400 378 660\n",
      " 380 429 441 528 555 676 376 339 563 759 171 341 383 397 758 289 107 375\n",
      " 213 580 273 209 483 497 503 161 126 323 257 124 403 613 484 302 728 226\n",
      " 720 477  59 443 539 183  12 166 753 255 517 112 783  24 480 266 336 599\n",
      " 745  37 542 389 115 136 176  72 309 768  85 231  25 584 230 506 301 236\n",
      "   3 103 362 448 168 618 619 228  96 297 445 133 149 458 267 548 512 632\n",
      " 318 654 365 128   1 330 474 223 561 597 566 263 761  87 208 393 409 349\n",
      " 278 130 463 457 219 254 625 172 755 277 691 664  48 717  14 721 570 174\n",
      " 569 320 113 427 782 206 299   9 645 722 351 132 125 460 143 774 395  38\n",
      " 532  79 422 147 529  22 144  10 246  43 729 565 387 760 534 630 533 264\n",
      " 265 131 200 623 764 744 152 406 735 543 203 156 356 510 184 518 707 424\n",
      " 119 381  76 734 129 647 780 699   0 652 216 442  78 122 109 268 446 259\n",
      "  68 610 197  23 405 242 603 194 451 616 677 111 222 145 481 586 366 322\n",
      " 687  84 740 575 733  28 750 685  70 465 719 467 609 504 535  65 475 686\n",
      " 118 537 227 345  89 523 121 549 507 665 368 521 329 724 620 655 473 371\n",
      " 499 137 204  75 372 426 141 558 235 447]\n",
      "rows to prune in layer 3 : 0\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[  0 136  89 106  77 195 239 226 176  22  24 235 215  95 142 289 214  55\n",
      " 130 259 212  58 143 134 251  35  70 225 279 281 223 169 137 184 141  45\n",
      " 262 127 280 116 191   2   7  42 196  49  51 133 173  68 297 128 138 278\n",
      "  14 174 198  57 224 111 247  20 260  60 170 242 183 284  66 140 256 119\n",
      " 252 267 157 290 102   4 282 194 257 108 271 220 254 246   9  65 298 118\n",
      " 158 193 230 159 288  17 261  90 233 209  86 253  98 207 153  93 205 144\n",
      " 211  21  81 268 263 299  92 146 208   8 189  12  46 149 152 150 179  39\n",
      " 155  47 178  11  44  33 291 286 112 199 255 236 110 240 216  71 234  79\n",
      "  54 295 161 265 165  87 272 177 121 276 156 182 114  83 185 222 241 210\n",
      "  36 132 187   1 123  13  50  84 139  34  15  88 204 200 115 104  64 126\n",
      "  18 274  32 275 117 188   6  25 219 238 162 151 264  63 296  56  10 122\n",
      " 192  75   5  76 250 232  59 181  31 105  85  96 231  69  41 248 168  91\n",
      " 167  23  40  53  78 228 120 148 202  82 218 213 258 166 273  30 101 201\n",
      " 135  48 131 160  38 206 175 129 266  62 227  27 243 285  99 171 244 180\n",
      " 163 270 100  67 154 293  19 245 249  61 294 124  94 147 103  97 287 269\n",
      "  74 283  80   3 186 229 237 203  72  28  29 113  43 221  52 107  26  37\n",
      " 217  16 197 109  73 190 125 277 145 164 172 292]\n",
      "rows to prune in layer 6 : 0\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[12 79 58 49 81 66 48 34 96  9 73 71 55 44 43 69 52 24 86 60 16 54 33 13\n",
      " 82 42 39  3 93 38 18 37 27  8 83 14 68  7 59 88 75 76 35 98 15 28 87 26\n",
      " 67  6 94 21 99 22  1 62 30 92  2 63 61 25 47 17  5 32 40 31 72 78 23 50\n",
      " 45 46 70 65 74 41 84 36 19 51 56 10 85 57 29 80 77 95 11  0 91 97 90  4\n",
      " 20 53 64 89]\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5106 - accuracy: 0.9531 - val_loss: 1.5017 - val_accuracy: 0.9613\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4973 - accuracy: 0.9657 - val_loss: 1.4952 - val_accuracy: 0.9674\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4909 - accuracy: 0.9712 - val_loss: 1.4941 - val_accuracy: 0.9673\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4855 - accuracy: 0.9767 - val_loss: 1.4886 - val_accuracy: 0.9731\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4826 - accuracy: 0.9795 - val_loss: 1.4888 - val_accuracy: 0.9732\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9821 - val_loss: 1.4876 - val_accuracy: 0.9739\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4786 - accuracy: 0.9829 - val_loss: 1.4872 - val_accuracy: 0.9744\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4763 - accuracy: 0.9851 - val_loss: 1.4900 - val_accuracy: 0.9714\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4770 - accuracy: 0.9846 - val_loss: 1.4867 - val_accuracy: 0.9744\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4752 - accuracy: 0.9862 - val_loss: 1.4833 - val_accuracy: 0.9780\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4741 - accuracy: 0.9874 - val_loss: 1.4847 - val_accuracy: 0.9769\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4735 - accuracy: 0.9878 - val_loss: 1.4834 - val_accuracy: 0.9781\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4724 - accuracy: 0.9890 - val_loss: 1.4837 - val_accuracy: 0.9774\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:01, 121.35s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 9\n",
      "rows to prune in layer 0 : 392\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305\n",
      " 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323\n",
      " 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341\n",
      " 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359\n",
      " 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377\n",
      " 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395\n",
      " 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413\n",
      " 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431\n",
      " 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449\n",
      " 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467\n",
      " 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485\n",
      " 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503\n",
      " 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521\n",
      " 522 523 524 525 526 527 528 529 530 531 532 533 534 535 536 537 538 539\n",
      " 540 541 542 543 544 545 546 547 548 549 550 551 552 553 554 555 556 557\n",
      " 558 559 560 561 562 563 564 565 566 567 568 569 570 571 572 573 574 575\n",
      " 576 577 578 579 580 581 582 583 584 585 586 587 588 589 590 591 592 593\n",
      " 594 595 596 597 598 599 600 601 602 603 604 605 606 607 608 609 610 611\n",
      " 612 613 614 615 616 617 618 619 620 621 622 623 624 625 626 627 628 629\n",
      " 630 631 632 633 634 635 636 637 638 639 640 641 642 643 644 645 646 647\n",
      " 648 649 650 651 652 653 654 655 656 657 658 659 660 661 662 663 664 665\n",
      " 666 667 668 669 670 671 672 673 674 675 676 677 678 679 680 681 682 683\n",
      " 684 685 686 687 688 689 690 691 692 693 694 695 696 697 698 699 700 701\n",
      " 702 703 704 705 706 707 708 709 710 711 712 713 714 715 716 717 718 719\n",
      " 720 721 722 723 724 725 726 727 728 729 730 731 732 733 734 735 736 737\n",
      " 738 739 740 741 742 743 744 745 746 747 748 749 750 751 752 753 754 755\n",
      " 756 757 758 759 760 761 762 763 764 765 766 767 768 769 770 771 772 773\n",
      " 774 775 776 777 778 779 780 781 782 783]\n",
      "[293 510 262 117 685 564 302 687 529 577 278  51 375 537 161 513 416 676\n",
      " 187 772  99  47 196 761 356 272  21 500 627 656 773 142  78 111 345 664\n",
      " 729 720 751 696 265 100 339 419 231 189  43 140 765  46 585 630 365 417\n",
      " 701 391 770  49 543 119 250 288 702 599 135 551 734 610 180 674 475 498\n",
      "  89 235 258 659 153 712  91  34 422 742 666 550 512 715 611 746 762 504\n",
      " 157 428 781  32 669  92 547 503  17  94 541 600 362 368  61 496 725 579\n",
      "  86 480 603 548 758 203 415  93 618  59 401 252 386 367 286 154 486 458\n",
      " 532 442 581 282   5  57 305 455 370 159 127 430 221  50 565 491  65 554\n",
      " 107 276 719 413 744 113 717  75  48 511 239 137 460 383 230 468 615 394\n",
      " 296 232 215 665 103 531 608 206   8 303 198 357 170 244 256  69 671 158\n",
      " 319 488 284 706 298  35 331 204 748 693 582  12 559 407 182 400 118 619\n",
      " 649 571 395 678 418 526 405 752 175 533 604 440 273 570 538 497 275 567\n",
      " 171 741 777 114 160 776 226 747 109 632 471 449 409 556 128 695 313 690\n",
      " 341 429 755  62 626 718 445 634 463 652 456 431 489 377 441 501 691 304\n",
      " 222 340  13 536 184 325 136 774 126 178 408 310 354 590 709 267 261 670\n",
      "  23 451 707  15 327 121 197 436 324 478 527 481 724 514 621 492   7  87\n",
      " 606 714 392 643 355 105 427 234 123 505 650 525  72 301 101 358 479 766\n",
      " 502 260 682 660 216 544 263 625 402  40 241 329 683 245 371 609  19 485\n",
      " 710 347 359 361 629 259 330 658 320 518 421 145 614  44 164 315 179 353\n",
      " 574 309 155 280 264 703 778 277 168 633 716 420 444 199 406 642 425 211\n",
      " 393 143 727 668 730 692 639  45 535 328 560 229 311 254 308  30  82 411\n",
      " 283  39 539 459 495  98 223  52 306  18 757 314 519 561 457 195 653 667\n",
      " 147 728 323  53 594 723  70 640 209 238 144 102 530 507 167 689 450 177\n",
      " 295 122 414 583 333 473 601 679 754 271   4 731 337 349 775 646 183 516\n",
      "  68 645 697 466 688 138 636 661 242 240  37 589 292 540 380  67 169 201\n",
      " 681  29 108 628  97 566 190 749 494 622 387 587 726  81 200 769 735 257\n",
      " 694 152 248  64 616 399 552 378 300 307 186 344 569 352 453 217 312 771\n",
      " 586 125 165 317 148 174   3 294 338 546 403 553 448 635 346 141 745 396\n",
      " 482   9 699 580 224  26 166 335 176 213 623 426 705 269 578 677 289 779\n",
      "  77 467 620  90 202  28 322 106 149 133 721 318  54 753  63 740 713  66\n",
      " 591 369  71 477 783 520 246 607 738 443 644 508 596 208 446 297 631 233\n",
      " 139 764 131 291 253  88 364 756 612  76 268 663 476 205 493  36 588 680\n",
      " 462 647 575 321 584  38 389 780 592 279  58 381 220 363 602 624 750 266\n",
      "  74 132 573 212  96 598 593 228 173 708 316 412 597 572 768 576 274 638\n",
      " 763 366 151 423 472 193  16 698 410 150 163  11  56 509 116  27 373 437\n",
      "  41 432 454 326 675 490 651 134 162 673 351 522 439 568 372 299 249  31\n",
      " 557 524 465 686 332 617 225  20 447 255 433 376  60 711  24 112 348 334\n",
      "  80 382 172  73 641 207 191 404 558   2 237 181 655 704  14 243 464 342\n",
      " 555 434 743 469 595 110 336 760 188  79  55 379 219 545 523  95  10 534\n",
      " 104  85 210  22 146 120 360 487 438 285 290   0 562 185 672 218 648 461\n",
      " 251 613 287 542 733 528 506 759 374 194 722 350 782 424 247 452 654 281\n",
      " 388 398 684 700 227 732 343 214 474   1 397 384 156 515 435 192 739 130\n",
      "  84 124 657 605 236 499  33 662 549 563 521  25 385 637  83 767 390 484\n",
      " 270 517 483 115  42 470   6 736 737 129]\n",
      "rows to prune in layer 3 : 150\n",
      "[  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n",
      "  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n",
      "  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n",
      "  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n",
      "  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n",
      "  90  91  92  93  94  95  96  97  98  99 100 101 102 103 104 105 106 107\n",
      " 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125\n",
      " 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143\n",
      " 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161\n",
      " 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179\n",
      " 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197\n",
      " 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215\n",
      " 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233\n",
      " 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251\n",
      " 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269\n",
      " 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287\n",
      " 288 289 290 291 292 293 294 295 296 297 298 299]\n",
      "[115 208 125 235  29  50 104  41 285 144   8 271 148 170 187 268  97  40\n",
      " 272  92  60 191 284 158 293 133  72 194 121 232  20 201 140 259  93  84\n",
      "  14 291 288 283  82 240  69 198 127 264  33 172 226 255 163 182 113  47\n",
      " 118 116 153   7 181 185  70 213 261  83 228 219 111 157  77 102  15  53\n",
      " 154  39 195 129 245 130 141 298 212 222 216 161 295 251 280 276 210 107\n",
      " 273 203 105 123 243 248 183  65  51 189 290 218 103 106  43  49 214 177\n",
      "  12 114 237 229 135 156  22 196  99 296 281 250 179  17 270  59 224 165\n",
      " 109 200 221 146 100 215 167  57 160 145  79 151 124  28  42   2 142 134\n",
      " 143 231  45 155 258  75 171 252  25 168 192  78 149 122  46  89 294  96\n",
      "  55 263 131 206 220 223  27 199 225 138 120  71 217   6  81   9 242 207\n",
      "   0  36 299 186 239  32  56 236  66 286  98 117 241 110 175 262 188  91\n",
      "  67 269 101   1  73 244 193  88 278 166  62  30 119 132 137 238 136 209\n",
      " 108  13  64 184  95   3 159  11  31  16 180 253 205  24  58  85  74  54\n",
      " 287   5 249  21 162  35  10 128  52  44 282 211   4 266  94  38 152  34\n",
      " 274 265 247 234 277 190  18 178 227  23 233 176 202 289 204  87 256 150\n",
      " 260  63 257  19 275 292  76  68 279 169  80  48 164  86 246 126 230 139\n",
      " 112 173  37 254 174 297  61 147 197  90 267  26]\n",
      "rows to prune in layer 6 : 50\n",
      "[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23\n",
      " 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47\n",
      " 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71\n",
      " 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95\n",
      " 96 97 98 99]\n",
      "[19 51 18 11 72 65 70 40  7 71 30 64 49 16 99 10 80 95 46 13 38 93 63 14\n",
      " 75 87 74 28 23 21  2 78  8 96 43 35 37 47 56 98 76 79 45 92 86 67 61 41\n",
      " 97  6 36 81 66 20 44 32 77 29 85 94 82 24 69 48 89 54 17 53 60 15 62 12\n",
      "  0 50 22 88 91 57 39 83 33 68  9 26 25 55 27 59  3 31 73 42 84 52  4 58\n",
      "  5 90 34  1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4991 - accuracy: 0.9713 - val_loss: 1.4923 - val_accuracy: 0.9725\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4831 - accuracy: 0.9815 - val_loss: 1.4891 - val_accuracy: 0.9745\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4786 - accuracy: 0.9851 - val_loss: 1.4859 - val_accuracy: 0.9758\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4762 - accuracy: 0.9870 - val_loss: 1.4852 - val_accuracy: 0.9778\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4742 - accuracy: 0.9887 - val_loss: 1.4841 - val_accuracy: 0.9779\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4727 - accuracy: 0.9901 - val_loss: 1.4867 - val_accuracy: 0.9754\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9909 - val_loss: 1.4859 - val_accuracy: 0.9760\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4709 - accuracy: 0.9912 - val_loss: 1.4852 - val_accuracy: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:48, 117.13s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 9\n",
      "rows to prune in layer 0 : 588\n",
      "[  0   1   2   3   4   6   9  10  11  14  16  20  22  24  25  26  27  28\n",
      "  29  31  33  36  37  38  41  42  53  54  55  56  58  60  63  64  66  67\n",
      "  68  70  71  73  74  76  77  79  80  81  83  84  85  88  90  95  96  97\n",
      " 102 104 106 108 110 112 115 116 120 122 124 125 129 130 131 132 133 134\n",
      " 138 139 141 144 146 147 148 149 150 151 152 156 162 163 165 166 167 169\n",
      " 172 173 174 176 177 181 183 185 186 188 190 191 192 193 194 195 200 201\n",
      " 202 205 207 208 209 210 212 213 214 217 218 219 220 224 225 227 228 233\n",
      " 236 237 238 240 242 243 246 247 248 249 251 253 255 257 266 268 269 270\n",
      " 271 274 279 281 285 287 289 290 291 292 294 295 297 299 300 307 312 316\n",
      " 317 318 321 322 323 326 332 333 334 335 336 337 338 342 343 344 346 348\n",
      " 349 350 351 352 360 363 364 366 369 372 373 374 376 378 379 380 381 382\n",
      " 384 385 387 388 389 390 396 397 398 399 403 404 410 412 414 423 424 426\n",
      " 432 433 434 435 437 438 439 443 446 447 448 450 452 453 454 457 461 462\n",
      " 464 465 466 467 469 470 472 473 474 476 477 482 483 484 487 490 493 494\n",
      " 499 506 507 508 509 515 516 517 520 521 522 523 524 528 530 534 540 542\n",
      " 545 546 549 552 553 555 557 558 562 563 566 568 569 572 573 575 576 578\n",
      " 580 583 584 586 587 588 589 591 592 593 594 595 596 597 598 601 602 605\n",
      " 607 612 613 616 617 620 622 623 624 628 631 635 636 637 638 640 641 644\n",
      " 645 646 647 648 651 653 654 655 657 661 662 663 667 672 673 675 677 679\n",
      " 680 681 684 686 688 689 694 697 698 699 700 704 705 708 711 713 721 722\n",
      " 723 726 728 731 732 733 735 736 737 738 739 740 743 745 749 750 753 754\n",
      " 756 759 760 763 764 767 768 769 771 775 779 780 782 783]\n",
      "[523 732 183 388 195 711 594 782 524 552 657  36 555 269 212 399 130  95\n",
      "  73 521 322 446 759 509 238 698 738 163 672 679 225 104 366  74 191 651\n",
      " 335 721  27 769 648 595 414 607 210 364 257 369 218 508  96   1 646 448\n",
      " 344  54 760 596 591 593 450 612 617 493  28 736 578 378  31 173 220 373\n",
      " 750 410 662 363  20 597 243 467 661 426 376 236 580 403 292 146 499 483\n",
      " 297 767 149 334 336 193 764 688 464 588 342 620  64  55 628 312 602 673\n",
      " 743 249 684 290 291 396 680 165 133 390 233 387 622 352 167 700  22 779\n",
      " 623 453 472 516 443 270 279  97 307 749 592 120 566 434 360 346 433 637\n",
      " 248 397 647 318 655 110  29 553  56  88  63 115 640  60  33 389 382 379\n",
      " 374 754 663 333 294 675 207 644   3 540 274 266 542  80 372 705 271 139\n",
      " 124 423 350 530 384 474 351 181 694 476 482 465 343 573 237  14 771  79\n",
      "  37 338 201 242 185  53 202 756 613 435 728  42 228 172 349 713 432 190\n",
      "  26 213 174 507 156 112 697 645 517 461 116 506 477 589 549 332 641 268\n",
      " 424  76 247 299 484  77 584 227  38 246 723  16 188 558 251 129 631 515\n",
      " 214 439 150 462 635 522 528 733 636 739 616  10 152  41 753  85 575 437\n",
      " 323 677  68 219 200 708 689 208 381 745 224 783 176 726 108  67 253 569\n",
      " 138 385 731 177 562 217 398 380 412   9 317 125 209 466  24 457 667 166\n",
      " 587 454   6 295 699 534   2 144 563 102 321 162 487  58  11  70 654 586\n",
      " 151 473 735 132 131 576 186 192   4 722 134 740 704 194 546 438 545 780\n",
      " 326 605 287  90  81   0 681 281 240 763 452 205 469 557 348 147 285  25\n",
      " 572 583 106 169 289 141 568  71 447 122 494 470 255 624 737 775 490  66\n",
      "  84 316 686 337 598 148  83 520 653 300 638 601 404 768]\n",
      "rows to prune in layer 3 : 225\n",
      "[  0   1   3   4   5   6   9  10  11  13  16  18  19  21  23  24  25  26\n",
      "  27  30  31  32  34  35  36  37  38  44  46  48  52  54  55  56  58  61\n",
      "  62  63  64  66  67  68  71  73  74  76  78  80  81  85  86  87  88  89\n",
      "  90  91  94  95  96  98 101 108 110 112 117 119 120 122 126 128 131 132\n",
      " 136 137 138 139 147 149 150 152 159 162 164 166 168 169 171 173 174 175\n",
      " 176 178 180 184 186 188 190 192 193 197 199 202 204 205 206 207 209 211\n",
      " 217 220 223 225 227 230 233 234 236 238 239 241 242 244 246 247 249 252\n",
      " 253 254 256 257 260 262 263 265 266 267 269 274 275 277 278 279 282 286\n",
      " 287 289 292 294 297 299]\n",
      "[ 94  19 239  87 176  63 253 206  56 184 246 205 274 297 101  52  71 152\n",
      " 162  74  36 139  10 186  55   0  89 207 199   4 241 256 173  37 180 230\n",
      " 227 131  67  86  24  11  35  13 175  30 168 286 236  18 262  76  31   1\n",
      " 263  81  85 117 278 242 171  27 279 122 150  96 265 149  25 112  78 257\n",
      "  88  64 254  34  73 269 193  54 174 128 166  95 220  90 223 159 292  38\n",
      " 252   3 282 233  46  61  16 234 178 217 260  58 126 137 119 249   5 209\n",
      " 275   6 277  32 197 238  91 287 299  26 202 138 192 211 289  21  98 164\n",
      " 110 294 120  66   9  23  62 204  44 132  68 244 188 266 190 267 169 108\n",
      " 225 247  48 147 136  80]\n",
      "rows to prune in layer 6 : 75\n",
      "[ 0  1  3  4  5  9 12 15 17 20 22 24 25 26 27 29 31 32 33 34 36 39 42 44\n",
      " 48 50 52 53 54 55 57 58 59 60 62 66 68 69 73 77 81 82 83 84 85 88 89 90\n",
      " 91 94]\n",
      "[50 39 66 29 69 68 89 20 60 15 83  1  4  9 53 62 81 52 44 59  3 34 33 85\n",
      " 90 91 57 36 94 24 48 25 58 55 84 22 31 82 12 88  5 77 27  0 73 26 54 32\n",
      " 42 17]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5711 - accuracy: 0.9163 - val_loss: 1.5297 - val_accuracy: 0.9405\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5215 - accuracy: 0.9488 - val_loss: 1.5178 - val_accuracy: 0.9493\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5116 - accuracy: 0.9564 - val_loss: 1.5121 - val_accuracy: 0.9531\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5061 - accuracy: 0.9601 - val_loss: 1.5087 - val_accuracy: 0.9564\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5014 - accuracy: 0.9643 - val_loss: 1.5072 - val_accuracy: 0.9575\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4980 - accuracy: 0.9678 - val_loss: 1.5085 - val_accuracy: 0.9550\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4955 - accuracy: 0.9694 - val_loss: 1.5066 - val_accuracy: 0.9571\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4938 - accuracy: 0.9705 - val_loss: 1.5029 - val_accuracy: 0.9610\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4917 - accuracy: 0.9724 - val_loss: 1.5024 - val_accuracy: 0.9606\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4901 - accuracy: 0.9742 - val_loss: 1.5014 - val_accuracy: 0.9619\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4890 - accuracy: 0.9749 - val_loss: 1.5016 - val_accuracy: 0.9601\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4876 - accuracy: 0.9763 - val_loss: 1.5006 - val_accuracy: 0.9614\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4866 - accuracy: 0.9772 - val_loss: 1.5006 - val_accuracy: 0.9618\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4857 - accuracy: 0.9780 - val_loss: 1.5011 - val_accuracy: 0.9600\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4847 - accuracy: 0.9788 - val_loss: 1.5001 - val_accuracy: 0.9617\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4838 - accuracy: 0.9794 - val_loss: 1.5019 - val_accuracy: 0.9595\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4831 - accuracy: 0.9802 - val_loss: 1.5000 - val_accuracy: 0.9630\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4830 - accuracy: 0.9800 - val_loss: 1.5018 - val_accuracy: 0.9601\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4820 - accuracy: 0.9810 - val_loss: 1.5007 - val_accuracy: 0.9609\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4818 - accuracy: 0.9812 - val_loss: 1.5013 - val_accuracy: 0.9603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:08, 124.09s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 9\n",
      "rows to prune in layer 0 : 686\n",
      "[  0   2   4   6   9  10  11  16  24  25  26  37  38  41  42  53  58  66\n",
      "  67  68  70  71  76  77  79  81  83  84  85  90 102 106 108 112 116 122\n",
      " 125 129 131 132 134 138 141 144 147 148 150 151 152 156 162 166 169 172\n",
      " 174 176 177 185 186 188 190 192 194 200 201 202 205 208 209 213 214 217\n",
      " 219 224 227 228 240 242 246 247 251 253 255 268 281 285 287 289 295 299\n",
      " 300 316 317 321 323 326 332 337 338 348 349 380 381 385 398 404 412 424\n",
      " 432 435 437 438 439 447 452 454 457 461 462 466 469 470 473 477 484 487\n",
      " 490 494 506 507 515 517 520 522 528 534 545 546 549 557 558 562 563 568\n",
      " 569 572 575 576 583 584 586 587 589 598 601 605 613 616 624 631 635 636\n",
      " 638 641 645 653 654 667 677 681 686 689 697 699 704 708 713 722 723 726\n",
      " 728 731 733 735 737 739 740 745 753 756 763 768 771 775 780 783]\n",
      "[667 534 217 323  53  77  76 299 466 112  79 681 737 202  71 381 214 568\n",
      " 520 704 461  83 613  67  24 242   2 326 134  11 166 194 132 176 224 569\n",
      " 506 300 169 152 251 494 470   0 576 726 587 713 317 515 332 209 708 192\n",
      "   4  25 129 733 338 287 473 638 636  68 745 586 268  90 150 731 686 108\n",
      " 285  70 437 253 780 635  38 116   6 677 507 735  84 601 562 563 616 775\n",
      " 162 172  16 131 205 289  10  41 438  37 641 654 208 190  58 469 412 185\n",
      " 385 756 439 255 186 557 558 771 380 624 477 246 699  85 201 174 188  81\n",
      " 645 689 598 247  66 583 631 487 528 228 316 484 156 122 546 768 213 125\n",
      " 240 728 424 575 435 281 763 783 227 349 295 321 148 457 452 462 141 572\n",
      " 177 404 151 102 348 337  26  42 605 653   9 138 454 740 490 584 739 522\n",
      " 589 517 398 753 432 447 106 545 200 144 147 722 549 697 723 219]\n",
      "rows to prune in layer 3 : 262\n",
      "[  3   5   6   9  16  21  23  26  32  34  38  44  46  48  54  58  61  62\n",
      "  66  68  73  80  90  91  95  98 108 110 119 120 126 128 132 136 137 138\n",
      " 147 159 164 166 169 174 178 188 190 192 193 197 202 204 209 211 217 220\n",
      " 223 225 233 234 238 244 247 249 252 260 266 267 269 275 277 282 287 289\n",
      " 292 294 299]\n",
      "[ 21 269 294   6 147   3 169 202  54 277 120  44 252 192 223   5 275  66\n",
      "  62 190 266 244 282 234 211 138 119 225  80 108 159 217  46 137 287 233\n",
      " 238 126  58  32   9 204 249 193 136 132 197 188 260  16 292  98  68  48\n",
      " 164 299  38  91 174  26 289 267 166 220 178 247  34 209 110  23  95 128\n",
      "  73  90  61]\n",
      "rows to prune in layer 6 : 87\n",
      "[ 0  5 12 17 22 24 25 26 27 31 32 36 42 48 54 55 57 58 73 77 82 84 88 91\n",
      " 94]\n",
      "[88 22 26 12 84 57 42 58 55 24 91 94  5 17 31  0 54 25 82 48 77 32 73 36\n",
      " 27]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6628 - accuracy: 0.8197 - val_loss: 1.5859 - val_accuracy: 0.8870\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5772 - accuracy: 0.8955 - val_loss: 1.5695 - val_accuracy: 0.9006\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5642 - accuracy: 0.9061 - val_loss: 1.5631 - val_accuracy: 0.9049\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5567 - accuracy: 0.9115 - val_loss: 1.5567 - val_accuracy: 0.9115\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5515 - accuracy: 0.9167 - val_loss: 1.5537 - val_accuracy: 0.9125\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5477 - accuracy: 0.9198 - val_loss: 1.5527 - val_accuracy: 0.9130\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5449 - accuracy: 0.9217 - val_loss: 1.5498 - val_accuracy: 0.9151\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5424 - accuracy: 0.9239 - val_loss: 1.5492 - val_accuracy: 0.9161\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5400 - accuracy: 0.9262 - val_loss: 1.5481 - val_accuracy: 0.9162\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5382 - accuracy: 0.9280 - val_loss: 1.5463 - val_accuracy: 0.9180\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5370 - accuracy: 0.9286 - val_loss: 1.5477 - val_accuracy: 0.9153\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5352 - accuracy: 0.9305 - val_loss: 1.5459 - val_accuracy: 0.9182\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5342 - accuracy: 0.9313 - val_loss: 1.5453 - val_accuracy: 0.9171\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5330 - accuracy: 0.9322 - val_loss: 1.5435 - val_accuracy: 0.9201\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5319 - accuracy: 0.9331 - val_loss: 1.5436 - val_accuracy: 0.9209\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5310 - accuracy: 0.9341 - val_loss: 1.5429 - val_accuracy: 0.9197\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5302 - accuracy: 0.9348 - val_loss: 1.5449 - val_accuracy: 0.9168\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5291 - accuracy: 0.9361 - val_loss: 1.5411 - val_accuracy: 0.9210\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5286 - accuracy: 0.9363 - val_loss: 1.5422 - val_accuracy: 0.9201\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5280 - accuracy: 0.9367 - val_loss: 1.5408 - val_accuracy: 0.9223\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5272 - accuracy: 0.9374 - val_loss: 1.5408 - val_accuracy: 0.9210\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5266 - accuracy: 0.9383 - val_loss: 1.5399 - val_accuracy: 0.9227\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5260 - accuracy: 0.9386 - val_loss: 1.5392 - val_accuracy: 0.9234\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5256 - accuracy: 0.9389 - val_loss: 1.5383 - val_accuracy: 0.9233\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5252 - accuracy: 0.9389 - val_loss: 1.5386 - val_accuracy: 0.9244\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5243 - accuracy: 0.9403 - val_loss: 1.5397 - val_accuracy: 0.9225\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5242 - accuracy: 0.9396 - val_loss: 1.5384 - val_accuracy: 0.9230\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:05, 139.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 9\n",
      "rows to prune in layer 0 : 735\n",
      "[  9  26  37  42  58  66  81  85 102 106 122 125 138 141 144 147 148 151\n",
      " 156 174 177 185 186 188 190 200 201 208 213 219 227 228 240 246 247 255\n",
      " 281 295 316 321 337 348 349 380 385 398 404 412 424 432 435 438 439 447\n",
      " 452 454 457 462 469 477 484 487 490 517 522 528 545 546 549 557 558 572\n",
      " 575 583 584 589 598 605 624 631 641 645 653 654 689 697 699 722 723 728\n",
      " 739 740 753 756 763 768 771 783]\n",
      "[247 213 697 653 185 558  42 753 439 138 447 321 151 583 740 246 398 228\n",
      " 575 227 771  85 723 174 432 487 295 689 469 200 404 549 177 190 186 631\n",
      " 545  66 281 457 722 106 438 605 337  81 412 598 645 557 454 699  58 584\n",
      " 484   9 624 452 424  37 219 147 348 102 188 201 572 208 255 349 462 641\n",
      " 435 739 122 763 316 654 756 783 144  26 156 768 477 528 490 141 546 125\n",
      " 148 380 522 517 240 589 385 728]\n",
      "rows to prune in layer 3 : 281\n",
      "[  9  16  23  26  32  34  38  48  58  61  68  73  90  91  95  98 110 126\n",
      " 128 132 136 164 166 174 178 188 193 197 204 209 220 247 249 260 267 289\n",
      " 292 299]\n",
      "[260 292  90  26  73 289   9 188 110  38 247  68 220 197 193  91  58 178\n",
      "  32 249  23  34 164 132 128 136  95  98 299 166  61 267 204 126 174  48\n",
      "  16 209]\n",
      "rows to prune in layer 6 : 93\n",
      "[ 0  5 17 25 27 31 32 36 48 54 73 77 82]\n",
      "[48 77 73 36 32 54 17 25 27  0  5 31 82]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9421 - accuracy: 0.5245 - val_loss: 1.8701 - val_accuracy: 0.5980\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8522 - accuracy: 0.6146 - val_loss: 1.8320 - val_accuracy: 0.6350\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7876 - accuracy: 0.6827 - val_loss: 1.7588 - val_accuracy: 0.7126\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7487 - accuracy: 0.7213 - val_loss: 1.7345 - val_accuracy: 0.7356\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7322 - accuracy: 0.7369 - val_loss: 1.7222 - val_accuracy: 0.7460\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7202 - accuracy: 0.7481 - val_loss: 1.7104 - val_accuracy: 0.7582\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7102 - accuracy: 0.7579 - val_loss: 1.7046 - val_accuracy: 0.7615\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7036 - accuracy: 0.7631 - val_loss: 1.6992 - val_accuracy: 0.7676\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6993 - accuracy: 0.7676 - val_loss: 1.6975 - val_accuracy: 0.7680\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6962 - accuracy: 0.7703 - val_loss: 1.6945 - val_accuracy: 0.7721\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6934 - accuracy: 0.7732 - val_loss: 1.6935 - val_accuracy: 0.7709\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6909 - accuracy: 0.7759 - val_loss: 1.6922 - val_accuracy: 0.7722\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6889 - accuracy: 0.7771 - val_loss: 1.6907 - val_accuracy: 0.7751\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6867 - accuracy: 0.7792 - val_loss: 1.6888 - val_accuracy: 0.7739\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6848 - accuracy: 0.7810 - val_loss: 1.6881 - val_accuracy: 0.7771\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6831 - accuracy: 0.7822 - val_loss: 1.6877 - val_accuracy: 0.7751\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6815 - accuracy: 0.7834 - val_loss: 1.6848 - val_accuracy: 0.7805\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6801 - accuracy: 0.7856 - val_loss: 1.6835 - val_accuracy: 0.7806\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6788 - accuracy: 0.7864 - val_loss: 1.6830 - val_accuracy: 0.7807\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6776 - accuracy: 0.7880 - val_loss: 1.6810 - val_accuracy: 0.7831\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6763 - accuracy: 0.7885 - val_loss: 1.6827 - val_accuracy: 0.7811\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6755 - accuracy: 0.7896 - val_loss: 1.6805 - val_accuracy: 0.7833\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6745 - accuracy: 0.7904 - val_loss: 1.6796 - val_accuracy: 0.7847\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6732 - accuracy: 0.7916 - val_loss: 1.6804 - val_accuracy: 0.7822\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6726 - accuracy: 0.7925 - val_loss: 1.6791 - val_accuracy: 0.7838\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6717 - accuracy: 0.7932 - val_loss: 1.6775 - val_accuracy: 0.7842\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6711 - accuracy: 0.7930 - val_loss: 1.6775 - val_accuracy: 0.7855\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6703 - accuracy: 0.7944 - val_loss: 1.6763 - val_accuracy: 0.7877\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6699 - accuracy: 0.7949 - val_loss: 1.6755 - val_accuracy: 0.7873\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6692 - accuracy: 0.7951 - val_loss: 1.6761 - val_accuracy: 0.7856\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6682 - accuracy: 0.7962 - val_loss: 1.6770 - val_accuracy: 0.7825\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6680 - accuracy: 0.7965 - val_loss: 1.6742 - val_accuracy: 0.7888\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6672 - accuracy: 0.7975 - val_loss: 1.6765 - val_accuracy: 0.7857\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6669 - accuracy: 0.7974 - val_loss: 1.6746 - val_accuracy: 0.7873\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6664 - accuracy: 0.7979 - val_loss: 1.6745 - val_accuracy: 0.7871\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [11:55, 148.85s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 9\n",
      "rows to prune in layer 0 : 759\n",
      "[  9  26  37  58 102 122 125 141 144 147 148 156 188 201 208 219 240 255\n",
      " 316 348 349 380 385 424 435 452 454 462 477 484 490 517 522 528 546 557\n",
      " 572 584 589 624 641 654 699 728 739 756 763 768 783]\n",
      "[517 768 484 728 380 201 349  26 490 522 348 641 763 624   9 424 756 435\n",
      "  58 783 584 148 699 141 452 316 188 255 219 156 589 546 144 557 147 208\n",
      " 122 462 102 572 385 739 654 240 125 477 528 454  37]\n",
      "rows to prune in layer 3 : 290\n",
      "[ 16  23  34  48  61  95  98 126 128 132 136 164 166 174 204 209 249 267\n",
      " 299]\n",
      "[136  95 126 209 204 249 299  98 128  23 132  48 267 166  61 174  16  34\n",
      " 164]\n",
      "rows to prune in layer 6 : 96\n",
      "[ 0  5 17 25 27 31 82]\n",
      "[25 27  5 17 31  0 82]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1011 - accuracy: 0.3586 - val_loss: 2.0279 - val_accuracy: 0.4384\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9930 - accuracy: 0.4705 - val_loss: 1.9665 - val_accuracy: 0.4972\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9591 - accuracy: 0.5021 - val_loss: 1.9380 - val_accuracy: 0.5337\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9240 - accuracy: 0.5549 - val_loss: 1.9062 - val_accuracy: 0.5726\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9022 - accuracy: 0.5740 - val_loss: 1.8907 - val_accuracy: 0.5834\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8918 - accuracy: 0.5810 - val_loss: 1.8829 - val_accuracy: 0.5892\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8855 - accuracy: 0.5848 - val_loss: 1.8780 - val_accuracy: 0.5917\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8808 - accuracy: 0.5889 - val_loss: 1.8727 - val_accuracy: 0.5971\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8770 - accuracy: 0.5913 - val_loss: 1.8699 - val_accuracy: 0.5982\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8740 - accuracy: 0.5939 - val_loss: 1.8676 - val_accuracy: 0.5999\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8715 - accuracy: 0.5957 - val_loss: 1.8657 - val_accuracy: 0.5990\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8693 - accuracy: 0.5976 - val_loss: 1.8636 - val_accuracy: 0.6014\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8676 - accuracy: 0.5987 - val_loss: 1.8623 - val_accuracy: 0.6022\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8658 - accuracy: 0.6000 - val_loss: 1.8604 - val_accuracy: 0.6035\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8640 - accuracy: 0.6021 - val_loss: 1.8589 - val_accuracy: 0.6066\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8623 - accuracy: 0.6041 - val_loss: 1.8581 - val_accuracy: 0.6068\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8607 - accuracy: 0.6053 - val_loss: 1.8574 - val_accuracy: 0.6066\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8591 - accuracy: 0.6072 - val_loss: 1.8558 - val_accuracy: 0.6087\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8580 - accuracy: 0.6082 - val_loss: 1.8547 - val_accuracy: 0.6086\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8567 - accuracy: 0.6088 - val_loss: 1.8529 - val_accuracy: 0.6089\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8554 - accuracy: 0.6097 - val_loss: 1.8526 - val_accuracy: 0.6108\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8543 - accuracy: 0.6098 - val_loss: 1.8490 - val_accuracy: 0.6152\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8521 - accuracy: 0.6122 - val_loss: 1.8481 - val_accuracy: 0.6136\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8510 - accuracy: 0.6126 - val_loss: 1.8451 - val_accuracy: 0.6177\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8496 - accuracy: 0.6144 - val_loss: 1.8446 - val_accuracy: 0.6187\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8486 - accuracy: 0.6153 - val_loss: 1.8440 - val_accuracy: 0.6181\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8478 - accuracy: 0.6159 - val_loss: 1.8424 - val_accuracy: 0.6202\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8467 - accuracy: 0.6166 - val_loss: 1.8426 - val_accuracy: 0.6197\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8461 - accuracy: 0.6175 - val_loss: 1.8423 - val_accuracy: 0.6217\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8453 - accuracy: 0.6183 - val_loss: 1.8418 - val_accuracy: 0.6195\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8446 - accuracy: 0.6191 - val_loss: 1.8401 - val_accuracy: 0.6221\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8441 - accuracy: 0.6196 - val_loss: 1.8394 - val_accuracy: 0.6228\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8433 - accuracy: 0.6205 - val_loss: 1.8390 - val_accuracy: 0.6235\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8427 - accuracy: 0.6206 - val_loss: 1.8388 - val_accuracy: 0.6234\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8422 - accuracy: 0.6216 - val_loss: 1.8391 - val_accuracy: 0.6221\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8417 - accuracy: 0.6218 - val_loss: 1.8390 - val_accuracy: 0.6236\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8413 - accuracy: 0.6215 - val_loss: 1.8371 - val_accuracy: 0.6257\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8408 - accuracy: 0.6223 - val_loss: 1.8374 - val_accuracy: 0.6248\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8405 - accuracy: 0.6227 - val_loss: 1.8364 - val_accuracy: 0.6260\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8399 - accuracy: 0.6236 - val_loss: 1.8374 - val_accuracy: 0.6255\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8395 - accuracy: 0.6238 - val_loss: 1.8357 - val_accuracy: 0.6268\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8392 - accuracy: 0.6239 - val_loss: 1.8368 - val_accuracy: 0.6243\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8389 - accuracy: 0.6244 - val_loss: 1.8364 - val_accuracy: 0.6243\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8385 - accuracy: 0.6246 - val_loss: 1.8356 - val_accuracy: 0.6261\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8382 - accuracy: 0.6250 - val_loss: 1.8361 - val_accuracy: 0.6251\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8379 - accuracy: 0.6250 - val_loss: 1.8354 - val_accuracy: 0.6269\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8376 - accuracy: 0.6252 - val_loss: 1.8353 - val_accuracy: 0.6251\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8374 - accuracy: 0.6251 - val_loss: 1.8352 - val_accuracy: 0.6254\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8372 - accuracy: 0.6259 - val_loss: 1.8343 - val_accuracy: 0.6275\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8369 - accuracy: 0.6263 - val_loss: 1.8355 - val_accuracy: 0.6253\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8367 - accuracy: 0.6263 - val_loss: 1.8342 - val_accuracy: 0.6264\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8365 - accuracy: 0.6262 - val_loss: 1.8342 - val_accuracy: 0.6275\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8361 - accuracy: 0.6268 - val_loss: 1.8340 - val_accuracy: 0.6267\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8358 - accuracy: 0.6269 - val_loss: 1.8340 - val_accuracy: 0.6271\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8357 - accuracy: 0.6268 - val_loss: 1.8330 - val_accuracy: 0.6274\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8355 - accuracy: 0.6270 - val_loss: 1.8328 - val_accuracy: 0.6280\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8353 - accuracy: 0.6268 - val_loss: 1.8326 - val_accuracy: 0.6287\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8349 - accuracy: 0.6272 - val_loss: 1.8320 - val_accuracy: 0.6307\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8347 - accuracy: 0.6275 - val_loss: 1.8318 - val_accuracy: 0.6302\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8344 - accuracy: 0.6282 - val_loss: 1.8325 - val_accuracy: 0.6296\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8342 - accuracy: 0.6281 - val_loss: 1.8318 - val_accuracy: 0.6296\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8338 - accuracy: 0.6283 - val_loss: 1.8322 - val_accuracy: 0.6305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [15:29, 168.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 9\n",
      "rows to prune in layer 0 : 771\n",
      "[ 37 102 122 125 144 147 156 188 208 219 240 255 316 385 452 454 462 477\n",
      " 528 546 557 572 589 654 739]\n",
      "[654 208 477 147 572 557 219 528 462 102 122 316  37 385 240 156 454 255\n",
      " 188 739 546 452 125 144 589]\n",
      "rows to prune in layer 3 : 295\n",
      "[ 16  23  34  48  61 132 164 166 174 267]\n",
      "[ 34  16 174 166  23  61 132 164  48 267]\n",
      "rows to prune in layer 6 : 98\n",
      "[ 0 17 31 82]\n",
      "[ 0 82 31 17]\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2143 - accuracy: 0.2406 - val_loss: 2.1821 - val_accuracy: 0.2782\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1805 - accuracy: 0.2810 - val_loss: 2.1704 - val_accuracy: 0.2874\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1724 - accuracy: 0.2866 - val_loss: 2.1642 - val_accuracy: 0.2982\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1675 - accuracy: 0.2932 - val_loss: 2.1594 - val_accuracy: 0.3016\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1625 - accuracy: 0.2982 - val_loss: 2.1551 - val_accuracy: 0.3074\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1579 - accuracy: 0.3033 - val_loss: 2.1513 - val_accuracy: 0.3093\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1542 - accuracy: 0.3070 - val_loss: 2.1480 - val_accuracy: 0.3159\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1475 - accuracy: 0.3133 - val_loss: 2.1351 - val_accuracy: 0.3276\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1370 - accuracy: 0.3234 - val_loss: 2.1270 - val_accuracy: 0.3346\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1302 - accuracy: 0.3285 - val_loss: 2.1227 - val_accuracy: 0.3375\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1265 - accuracy: 0.3318 - val_loss: 2.1191 - val_accuracy: 0.3436\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1227 - accuracy: 0.3369 - val_loss: 2.1161 - val_accuracy: 0.3455\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1188 - accuracy: 0.3422 - val_loss: 2.1123 - val_accuracy: 0.3483\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1148 - accuracy: 0.3472 - val_loss: 2.1085 - val_accuracy: 0.3519\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1117 - accuracy: 0.3501 - val_loss: 2.1055 - val_accuracy: 0.3531\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1091 - accuracy: 0.3523 - val_loss: 2.1028 - val_accuracy: 0.3565\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1066 - accuracy: 0.3538 - val_loss: 2.1007 - val_accuracy: 0.3588\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1040 - accuracy: 0.3568 - val_loss: 2.0972 - val_accuracy: 0.3638\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1013 - accuracy: 0.3618 - val_loss: 2.0946 - val_accuracy: 0.3667\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0990 - accuracy: 0.3641 - val_loss: 2.0932 - val_accuracy: 0.3719\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0970 - accuracy: 0.3673 - val_loss: 2.0905 - val_accuracy: 0.3759\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0952 - accuracy: 0.3695 - val_loss: 2.0885 - val_accuracy: 0.3763\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0935 - accuracy: 0.3716 - val_loss: 2.0873 - val_accuracy: 0.3785\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0920 - accuracy: 0.3733 - val_loss: 2.0853 - val_accuracy: 0.3798\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0906 - accuracy: 0.3742 - val_loss: 2.0840 - val_accuracy: 0.3804\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0895 - accuracy: 0.3749 - val_loss: 2.0834 - val_accuracy: 0.3828\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0882 - accuracy: 0.3758 - val_loss: 2.0817 - val_accuracy: 0.3837\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0867 - accuracy: 0.3770 - val_loss: 2.0802 - val_accuracy: 0.3850\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0856 - accuracy: 0.3774 - val_loss: 2.0785 - val_accuracy: 0.3866\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0844 - accuracy: 0.3787 - val_loss: 2.0778 - val_accuracy: 0.3872\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0834 - accuracy: 0.3792 - val_loss: 2.0776 - val_accuracy: 0.3880\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0826 - accuracy: 0.3789 - val_loss: 2.0758 - val_accuracy: 0.3866\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0817 - accuracy: 0.3794 - val_loss: 2.0754 - val_accuracy: 0.3875\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0810 - accuracy: 0.3804 - val_loss: 2.0745 - val_accuracy: 0.3876\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0804 - accuracy: 0.3808 - val_loss: 2.0741 - val_accuracy: 0.3867\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0800 - accuracy: 0.3805 - val_loss: 2.0734 - val_accuracy: 0.3881\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0796 - accuracy: 0.3810 - val_loss: 2.0736 - val_accuracy: 0.3888\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0792 - accuracy: 0.3819 - val_loss: 2.0728 - val_accuracy: 0.3899\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0788 - accuracy: 0.3817 - val_loss: 2.0724 - val_accuracy: 0.3890\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0786 - accuracy: 0.3819 - val_loss: 2.0722 - val_accuracy: 0.3892\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0783 - accuracy: 0.3818 - val_loss: 2.0717 - val_accuracy: 0.3897\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0781 - accuracy: 0.3820 - val_loss: 2.0719 - val_accuracy: 0.3897\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0778 - accuracy: 0.3825 - val_loss: 2.0719 - val_accuracy: 0.3913\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0776 - accuracy: 0.3824 - val_loss: 2.0710 - val_accuracy: 0.3906\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0774 - accuracy: 0.3829 - val_loss: 2.0706 - val_accuracy: 0.3903\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0772 - accuracy: 0.3822 - val_loss: 2.0704 - val_accuracy: 0.3915\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0769 - accuracy: 0.3834 - val_loss: 2.0703 - val_accuracy: 0.3932\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0768 - accuracy: 0.3830 - val_loss: 2.0700 - val_accuracy: 0.3920\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0766 - accuracy: 0.3832 - val_loss: 2.0699 - val_accuracy: 0.3912\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0765 - accuracy: 0.3830 - val_loss: 2.0694 - val_accuracy: 0.3924\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0763 - accuracy: 0.3838 - val_loss: 2.0691 - val_accuracy: 0.3930\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0760 - accuracy: 0.3837 - val_loss: 2.0696 - val_accuracy: 0.3925\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0758 - accuracy: 0.3835 - val_loss: 2.0700 - val_accuracy: 0.3924\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0757 - accuracy: 0.3842 - val_loss: 2.0688 - val_accuracy: 0.3938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0755 - accuracy: 0.3844 - val_loss: 2.0686 - val_accuracy: 0.3933\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0752 - accuracy: 0.3845 - val_loss: 2.0679 - val_accuracy: 0.3937\n",
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0749 - accuracy: 0.3852 - val_loss: 2.0681 - val_accuracy: 0.3937\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0748 - accuracy: 0.3853 - val_loss: 2.0681 - val_accuracy: 0.3948\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0743 - accuracy: 0.3854 - val_loss: 2.0671 - val_accuracy: 0.3951\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0739 - accuracy: 0.3861 - val_loss: 2.0669 - val_accuracy: 0.3939\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0738 - accuracy: 0.3856 - val_loss: 2.0663 - val_accuracy: 0.3967\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0733 - accuracy: 0.3862 - val_loss: 2.0661 - val_accuracy: 0.3955\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0729 - accuracy: 0.3869 - val_loss: 2.0654 - val_accuracy: 0.3952\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0724 - accuracy: 0.3886 - val_loss: 2.0648 - val_accuracy: 0.3976\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0721 - accuracy: 0.3877 - val_loss: 2.0647 - val_accuracy: 0.3964\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0718 - accuracy: 0.3888 - val_loss: 2.0649 - val_accuracy: 0.3966\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0714 - accuracy: 0.3887 - val_loss: 2.0646 - val_accuracy: 0.3977\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0711 - accuracy: 0.3896 - val_loss: 2.0636 - val_accuracy: 0.3995\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0707 - accuracy: 0.3903 - val_loss: 2.0633 - val_accuracy: 0.3995\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0703 - accuracy: 0.3898 - val_loss: 2.0628 - val_accuracy: 0.3996\n",
      "Epoch 71/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0699 - accuracy: 0.3912 - val_loss: 2.0623 - val_accuracy: 0.4009\n",
      "Epoch 72/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0695 - accuracy: 0.3917 - val_loss: 2.0623 - val_accuracy: 0.4005\n",
      "Epoch 73/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0691 - accuracy: 0.3925 - val_loss: 2.0612 - val_accuracy: 0.4013\n",
      "Epoch 74/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0686 - accuracy: 0.3929 - val_loss: 2.0609 - val_accuracy: 0.4021\n",
      "Epoch 75/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0682 - accuracy: 0.3931 - val_loss: 2.0602 - val_accuracy: 0.4023\n",
      "Epoch 76/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0678 - accuracy: 0.3940 - val_loss: 2.0606 - val_accuracy: 0.4043\n",
      "Epoch 77/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0674 - accuracy: 0.3954 - val_loss: 2.0604 - val_accuracy: 0.4020\n",
      "Epoch 78/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0672 - accuracy: 0.3947 - val_loss: 2.0595 - val_accuracy: 0.4028\n",
      "Epoch 79/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0668 - accuracy: 0.3950 - val_loss: 2.0596 - val_accuracy: 0.4040\n",
      "Epoch 80/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0666 - accuracy: 0.3952 - val_loss: 2.0590 - val_accuracy: 0.4056\n",
      "Epoch 81/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0663 - accuracy: 0.3953 - val_loss: 2.0590 - val_accuracy: 0.4046\n",
      "Epoch 82/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0660 - accuracy: 0.3951 - val_loss: 2.0586 - val_accuracy: 0.4044\n",
      "Epoch 83/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0657 - accuracy: 0.3961 - val_loss: 2.0580 - val_accuracy: 0.4057\n",
      "Epoch 84/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0654 - accuracy: 0.3964 - val_loss: 2.0576 - val_accuracy: 0.4046\n",
      "Epoch 85/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0650 - accuracy: 0.3967 - val_loss: 2.0579 - val_accuracy: 0.4042\n",
      "Epoch 86/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0649 - accuracy: 0.3964 - val_loss: 2.0571 - val_accuracy: 0.4047\n",
      "Epoch 87/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0646 - accuracy: 0.3972 - val_loss: 2.0569 - val_accuracy: 0.4057\n",
      "Epoch 88/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0643 - accuracy: 0.3970 - val_loss: 2.0571 - val_accuracy: 0.4066\n",
      "Epoch 89/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0640 - accuracy: 0.3976 - val_loss: 2.0573 - val_accuracy: 0.4051\n",
      "Epoch 90/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0638 - accuracy: 0.3975 - val_loss: 2.0562 - val_accuracy: 0.4063\n",
      "Epoch 91/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0636 - accuracy: 0.3973 - val_loss: 2.0565 - val_accuracy: 0.4066\n",
      "Epoch 92/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0633 - accuracy: 0.3976 - val_loss: 2.0558 - val_accuracy: 0.4067\n",
      "Epoch 93/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0630 - accuracy: 0.3985 - val_loss: 2.0549 - val_accuracy: 0.4080\n",
      "Epoch 94/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0627 - accuracy: 0.3981 - val_loss: 2.0549 - val_accuracy: 0.4066\n",
      "Epoch 95/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0623 - accuracy: 0.3980 - val_loss: 2.0547 - val_accuracy: 0.4069\n",
      "Epoch 96/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0620 - accuracy: 0.3984 - val_loss: 2.0542 - val_accuracy: 0.4071\n",
      "Epoch 97/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0617 - accuracy: 0.3985 - val_loss: 2.0543 - val_accuracy: 0.4072\n",
      "Epoch 98/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0613 - accuracy: 0.3984 - val_loss: 2.0538 - val_accuracy: 0.4079\n",
      "Epoch 99/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0610 - accuracy: 0.3985 - val_loss: 2.0536 - val_accuracy: 0.4074\n",
      "Epoch 100/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0607 - accuracy: 0.3988 - val_loss: 2.0538 - val_accuracy: 0.4076\n",
      "Epoch 101/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0603 - accuracy: 0.3989 - val_loss: 2.0537 - val_accuracy: 0.4081\n",
      "Epoch 102/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0599 - accuracy: 0.3999 - val_loss: 2.0534 - val_accuracy: 0.4077\n",
      "Epoch 103/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0596 - accuracy: 0.4006 - val_loss: 2.0529 - val_accuracy: 0.4075\n",
      "Epoch 104/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0593 - accuracy: 0.4008 - val_loss: 2.0526 - val_accuracy: 0.4064\n",
      "Epoch 105/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0586 - accuracy: 0.4020 - val_loss: 2.0520 - val_accuracy: 0.4083\n",
      "Epoch 106/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0582 - accuracy: 0.4018 - val_loss: 2.0521 - val_accuracy: 0.4082\n",
      "Epoch 107/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0576 - accuracy: 0.4026 - val_loss: 2.0530 - val_accuracy: 0.4059\n",
      "Epoch 108/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0572 - accuracy: 0.4033 - val_loss: 2.0516 - val_accuracy: 0.4091\n",
      "Epoch 109/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0566 - accuracy: 0.4031 - val_loss: 2.0513 - val_accuracy: 0.4085\n",
      "Epoch 110/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0562 - accuracy: 0.4036 - val_loss: 2.0514 - val_accuracy: 0.4081\n",
      "Epoch 111/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0556 - accuracy: 0.4041 - val_loss: 2.0512 - val_accuracy: 0.4084\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 112/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0550 - accuracy: 0.4047 - val_loss: 2.0502 - val_accuracy: 0.4095\n",
      "Epoch 113/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0547 - accuracy: 0.4046 - val_loss: 2.0503 - val_accuracy: 0.4104\n",
      "Epoch 114/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0529 - accuracy: 0.4070 - val_loss: 2.0471 - val_accuracy: 0.4142\n",
      "Epoch 115/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0512 - accuracy: 0.4092 - val_loss: 2.0463 - val_accuracy: 0.4156\n",
      "Epoch 116/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0509 - accuracy: 0.4086 - val_loss: 2.0457 - val_accuracy: 0.4158\n",
      "Epoch 117/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0502 - accuracy: 0.4095 - val_loss: 2.0459 - val_accuracy: 0.4137\n",
      "Epoch 118/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0500 - accuracy: 0.4095 - val_loss: 2.0449 - val_accuracy: 0.4158\n",
      "Epoch 119/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0495 - accuracy: 0.4100 - val_loss: 2.0448 - val_accuracy: 0.4165\n",
      "Epoch 120/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0491 - accuracy: 0.4103 - val_loss: 2.0440 - val_accuracy: 0.4162\n",
      "Epoch 121/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0487 - accuracy: 0.4110 - val_loss: 2.0443 - val_accuracy: 0.4150\n",
      "Epoch 122/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0484 - accuracy: 0.4108 - val_loss: 2.0435 - val_accuracy: 0.4170\n",
      "Epoch 123/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0479 - accuracy: 0.4113 - val_loss: 2.0430 - val_accuracy: 0.4165\n",
      "Epoch 124/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0475 - accuracy: 0.4118 - val_loss: 2.0425 - val_accuracy: 0.4161\n",
      "Epoch 125/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0469 - accuracy: 0.4126 - val_loss: 2.0423 - val_accuracy: 0.4173\n",
      "Epoch 126/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0466 - accuracy: 0.4128 - val_loss: 2.0415 - val_accuracy: 0.4173\n",
      "Epoch 127/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0462 - accuracy: 0.4132 - val_loss: 2.0415 - val_accuracy: 0.4183\n",
      "Epoch 128/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0457 - accuracy: 0.4135 - val_loss: 2.0404 - val_accuracy: 0.4175\n",
      "Epoch 129/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0451 - accuracy: 0.4140 - val_loss: 2.0402 - val_accuracy: 0.4184\n",
      "Epoch 130/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0448 - accuracy: 0.4142 - val_loss: 2.0403 - val_accuracy: 0.4182\n",
      "Epoch 131/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0444 - accuracy: 0.4148 - val_loss: 2.0400 - val_accuracy: 0.4193\n",
      "Epoch 132/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0443 - accuracy: 0.4155 - val_loss: 2.0405 - val_accuracy: 0.4191\n",
      "Epoch 133/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0441 - accuracy: 0.4154 - val_loss: 2.0399 - val_accuracy: 0.4203\n",
      "Epoch 134/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0438 - accuracy: 0.4157 - val_loss: 2.0395 - val_accuracy: 0.4204\n",
      "Epoch 135/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0435 - accuracy: 0.4164 - val_loss: 2.0386 - val_accuracy: 0.4195\n",
      "Epoch 136/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0435 - accuracy: 0.4159 - val_loss: 2.0384 - val_accuracy: 0.4211\n",
      "Epoch 137/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0433 - accuracy: 0.4165 - val_loss: 2.0380 - val_accuracy: 0.4220\n",
      "Epoch 138/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0430 - accuracy: 0.4164 - val_loss: 2.0382 - val_accuracy: 0.4204\n",
      "Epoch 139/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0430 - accuracy: 0.4164 - val_loss: 2.0386 - val_accuracy: 0.4201\n",
      "Epoch 140/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0428 - accuracy: 0.4173 - val_loss: 2.0385 - val_accuracy: 0.4215\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [21:26, 183.79s/it]\u001b[A\n",
      "100%|██████████| 10/10 [3:13:02<00:00, 1158.26s/it]\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='random', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5777 - accuracy: 0.8958 - val_loss: 1.5199 - val_accuracy: 0.9447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 0\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5130 - accuracy: 0.9510 - val_loss: 1.5076 - val_accuracy: 0.9550\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4992 - accuracy: 0.9631 - val_loss: 1.4946 - val_accuracy: 0.9673\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4921 - accuracy: 0.9698 - val_loss: 1.4955 - val_accuracy: 0.9669\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4870 - accuracy: 0.9753 - val_loss: 1.4914 - val_accuracy: 0.9697\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9786 - val_loss: 1.4883 - val_accuracy: 0.9733\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4817 - accuracy: 0.9798 - val_loss: 1.4865 - val_accuracy: 0.9758\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4790 - accuracy: 0.9825 - val_loss: 1.4923 - val_accuracy: 0.9689\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4783 - accuracy: 0.9833 - val_loss: 1.4847 - val_accuracy: 0.9761\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4767 - accuracy: 0.9848 - val_loss: 1.4853 - val_accuracy: 0.9762\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4764 - accuracy: 0.9850 - val_loss: 1.4831 - val_accuracy: 0.9777\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4748 - accuracy: 0.9866 - val_loss: 1.4854 - val_accuracy: 0.9761\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4740 - accuracy: 0.9873 - val_loss: 1.4825 - val_accuracy: 0.9790\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4738 - accuracy: 0.9876 - val_loss: 1.4893 - val_accuracy: 0.9721\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4736 - accuracy: 0.9877 - val_loss: 1.4825 - val_accuracy: 0.9793\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4732 - accuracy: 0.9881 - val_loss: 1.4833 - val_accuracy: 0.9779\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4724 - accuracy: 0.9891 - val_loss: 1.4846 - val_accuracy: 0.9765\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9897 - val_loss: 1.4868 - val_accuracy: 0.9746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:31, 151.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 0\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4891 - accuracy: 0.9742 - val_loss: 1.4894 - val_accuracy: 0.9724\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4783 - accuracy: 0.9840 - val_loss: 1.4889 - val_accuracy: 0.9730\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4751 - accuracy: 0.9870 - val_loss: 1.4870 - val_accuracy: 0.9750\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4732 - accuracy: 0.9889 - val_loss: 1.4883 - val_accuracy: 0.9737\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4720 - accuracy: 0.9899 - val_loss: 1.4858 - val_accuracy: 0.9758\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4712 - accuracy: 0.9907 - val_loss: 1.4879 - val_accuracy: 0.9735\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9915 - val_loss: 1.4859 - val_accuracy: 0.9759\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4699 - accuracy: 0.9917 - val_loss: 1.4859 - val_accuracy: 0.9756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:29, 141.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 0\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5743 - accuracy: 0.9051 - val_loss: 1.5335 - val_accuracy: 0.9345\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5198 - accuracy: 0.9476 - val_loss: 1.5247 - val_accuracy: 0.9407\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5102 - accuracy: 0.9561 - val_loss: 1.5183 - val_accuracy: 0.9467\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5044 - accuracy: 0.9612 - val_loss: 1.5167 - val_accuracy: 0.9471\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5003 - accuracy: 0.9645 - val_loss: 1.5147 - val_accuracy: 0.9487\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4971 - accuracy: 0.9678 - val_loss: 1.5122 - val_accuracy: 0.9502\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4945 - accuracy: 0.9701 - val_loss: 1.5117 - val_accuracy: 0.9508\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4924 - accuracy: 0.9719 - val_loss: 1.5113 - val_accuracy: 0.9523\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4903 - accuracy: 0.9739 - val_loss: 1.5103 - val_accuracy: 0.9517\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4891 - accuracy: 0.9743 - val_loss: 1.5075 - val_accuracy: 0.9550\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4874 - accuracy: 0.9761 - val_loss: 1.5084 - val_accuracy: 0.9530\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4862 - accuracy: 0.9775 - val_loss: 1.5089 - val_accuracy: 0.9532\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4852 - accuracy: 0.9780 - val_loss: 1.5082 - val_accuracy: 0.9527\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:34, 136.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 0\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7285 - accuracy: 0.7563 - val_loss: 1.6071 - val_accuracy: 0.8712\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5983 - accuracy: 0.8753 - val_loss: 1.5889 - val_accuracy: 0.8817\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5829 - accuracy: 0.8880 - val_loss: 1.5800 - val_accuracy: 0.8882\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5745 - accuracy: 0.8943 - val_loss: 1.5758 - val_accuracy: 0.8914\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5690 - accuracy: 0.8991 - val_loss: 1.5728 - val_accuracy: 0.8925\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5645 - accuracy: 0.9032 - val_loss: 1.5698 - val_accuracy: 0.8953\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5616 - accuracy: 0.9056 - val_loss: 1.5680 - val_accuracy: 0.8967\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5588 - accuracy: 0.9088 - val_loss: 1.5647 - val_accuracy: 0.8996\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5567 - accuracy: 0.9097 - val_loss: 1.5640 - val_accuracy: 0.9003\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5548 - accuracy: 0.9115 - val_loss: 1.5651 - val_accuracy: 0.8995\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5530 - accuracy: 0.9134 - val_loss: 1.5631 - val_accuracy: 0.9017\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5515 - accuracy: 0.9143 - val_loss: 1.5619 - val_accuracy: 0.9016\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5502 - accuracy: 0.9159 - val_loss: 1.5610 - val_accuracy: 0.9021\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5489 - accuracy: 0.9168 - val_loss: 1.5606 - val_accuracy: 0.9032\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5480 - accuracy: 0.9173 - val_loss: 1.5598 - val_accuracy: 0.9039\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5468 - accuracy: 0.9189 - val_loss: 1.5593 - val_accuracy: 0.9032\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5462 - accuracy: 0.9192 - val_loss: 1.5594 - val_accuracy: 0.9032\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5450 - accuracy: 0.9201 - val_loss: 1.5589 - val_accuracy: 0.9030\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5442 - accuracy: 0.9214 - val_loss: 1.5603 - val_accuracy: 0.9016\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5437 - accuracy: 0.9211 - val_loss: 1.5580 - val_accuracy: 0.9039\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5428 - accuracy: 0.9222 - val_loss: 1.5598 - val_accuracy: 0.9021\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5423 - accuracy: 0.9226 - val_loss: 1.5583 - val_accuracy: 0.9048\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5416 - accuracy: 0.9231 - val_loss: 1.5588 - val_accuracy: 0.9046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [08:59, 139.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 0\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0143 - accuracy: 0.4629 - val_loss: 1.8903 - val_accuracy: 0.5940\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8559 - accuracy: 0.6173 - val_loss: 1.8331 - val_accuracy: 0.6406\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8187 - accuracy: 0.6512 - val_loss: 1.8149 - val_accuracy: 0.6526\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8063 - accuracy: 0.6608 - val_loss: 1.8072 - val_accuracy: 0.6587\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7988 - accuracy: 0.6682 - val_loss: 1.8039 - val_accuracy: 0.6602\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7940 - accuracy: 0.6722 - val_loss: 1.7976 - val_accuracy: 0.6668\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7898 - accuracy: 0.6761 - val_loss: 1.7955 - val_accuracy: 0.6688\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7865 - accuracy: 0.6793 - val_loss: 1.7918 - val_accuracy: 0.6725\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7835 - accuracy: 0.6824 - val_loss: 1.7881 - val_accuracy: 0.6786\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7805 - accuracy: 0.6846 - val_loss: 1.7845 - val_accuracy: 0.6820\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7768 - accuracy: 0.6886 - val_loss: 1.7822 - val_accuracy: 0.6834\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7743 - accuracy: 0.6908 - val_loss: 1.7800 - val_accuracy: 0.6858\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7717 - accuracy: 0.6936 - val_loss: 1.7765 - val_accuracy: 0.6882\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7697 - accuracy: 0.6961 - val_loss: 1.7746 - val_accuracy: 0.6922\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7677 - accuracy: 0.6979 - val_loss: 1.7736 - val_accuracy: 0.6929\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7662 - accuracy: 0.6988 - val_loss: 1.7725 - val_accuracy: 0.6921\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7647 - accuracy: 0.7006 - val_loss: 1.7701 - val_accuracy: 0.6964\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7632 - accuracy: 0.7019 - val_loss: 1.7704 - val_accuracy: 0.6937\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7619 - accuracy: 0.7026 - val_loss: 1.7687 - val_accuracy: 0.6957\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7608 - accuracy: 0.7038 - val_loss: 1.7674 - val_accuracy: 0.6972\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7595 - accuracy: 0.7049 - val_loss: 1.7661 - val_accuracy: 0.6978\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7586 - accuracy: 0.7062 - val_loss: 1.7652 - val_accuracy: 0.6986\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7576 - accuracy: 0.7071 - val_loss: 1.7664 - val_accuracy: 0.6985\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7568 - accuracy: 0.7077 - val_loss: 1.7636 - val_accuracy: 0.7006\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7558 - accuracy: 0.7088 - val_loss: 1.7638 - val_accuracy: 0.6992\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7552 - accuracy: 0.7097 - val_loss: 1.7633 - val_accuracy: 0.7011\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7545 - accuracy: 0.7098 - val_loss: 1.7620 - val_accuracy: 0.7021\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7536 - accuracy: 0.7108 - val_loss: 1.7646 - val_accuracy: 0.6966\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7531 - accuracy: 0.7106 - val_loss: 1.7615 - val_accuracy: 0.7020\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7523 - accuracy: 0.7119 - val_loss: 1.7598 - val_accuracy: 0.7051\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7516 - accuracy: 0.7128 - val_loss: 1.7605 - val_accuracy: 0.7046\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7510 - accuracy: 0.7126 - val_loss: 1.7596 - val_accuracy: 0.7039\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7506 - accuracy: 0.7136 - val_loss: 1.7590 - val_accuracy: 0.7040\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7501 - accuracy: 0.7136 - val_loss: 1.7603 - val_accuracy: 0.7030\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7492 - accuracy: 0.7147 - val_loss: 1.7587 - val_accuracy: 0.7048\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7487 - accuracy: 0.7150 - val_loss: 1.7592 - val_accuracy: 0.7033\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7484 - accuracy: 0.7154 - val_loss: 1.7582 - val_accuracy: 0.7038\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7478 - accuracy: 0.7161 - val_loss: 1.7588 - val_accuracy: 0.7023\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7473 - accuracy: 0.7167 - val_loss: 1.7573 - val_accuracy: 0.7063\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7471 - accuracy: 0.7173 - val_loss: 1.7577 - val_accuracy: 0.7053\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7464 - accuracy: 0.7177 - val_loss: 1.7577 - val_accuracy: 0.7050\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7462 - accuracy: 0.7170 - val_loss: 1.7566 - val_accuracy: 0.7065\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7459 - accuracy: 0.7178 - val_loss: 1.7579 - val_accuracy: 0.7046\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7456 - accuracy: 0.7183 - val_loss: 1.7577 - val_accuracy: 0.7042\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7451 - accuracy: 0.7187 - val_loss: 1.7577 - val_accuracy: 0.7041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:27, 159.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 0\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1433 - accuracy: 0.3074 - val_loss: 2.1351 - val_accuracy: 0.3200\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1127 - accuracy: 0.3430 - val_loss: 2.1007 - val_accuracy: 0.3457\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0855 - accuracy: 0.3681 - val_loss: 2.0843 - val_accuracy: 0.3696\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0737 - accuracy: 0.3810 - val_loss: 2.0804 - val_accuracy: 0.3714\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0702 - accuracy: 0.3839 - val_loss: 2.0789 - val_accuracy: 0.3723\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0685 - accuracy: 0.3852 - val_loss: 2.0768 - val_accuracy: 0.3735\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0672 - accuracy: 0.3862 - val_loss: 2.0757 - val_accuracy: 0.3751\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0661 - accuracy: 0.3865 - val_loss: 2.0749 - val_accuracy: 0.3743\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0654 - accuracy: 0.3872 - val_loss: 2.0739 - val_accuracy: 0.3761\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0649 - accuracy: 0.3879 - val_loss: 2.0737 - val_accuracy: 0.3745\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0641 - accuracy: 0.3883 - val_loss: 2.0728 - val_accuracy: 0.3761\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0637 - accuracy: 0.3887 - val_loss: 2.0732 - val_accuracy: 0.3765\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0633 - accuracy: 0.3884 - val_loss: 2.0725 - val_accuracy: 0.3761\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0628 - accuracy: 0.3900 - val_loss: 2.0715 - val_accuracy: 0.3777\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0622 - accuracy: 0.3902 - val_loss: 2.0714 - val_accuracy: 0.3777\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0620 - accuracy: 0.3898 - val_loss: 2.0709 - val_accuracy: 0.3776\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0617 - accuracy: 0.3909 - val_loss: 2.0708 - val_accuracy: 0.3774\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0613 - accuracy: 0.3917 - val_loss: 2.0706 - val_accuracy: 0.3787\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0608 - accuracy: 0.3914 - val_loss: 2.0708 - val_accuracy: 0.3714\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0607 - accuracy: 0.3914 - val_loss: 2.0700 - val_accuracy: 0.3792\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0604 - accuracy: 0.3918 - val_loss: 2.0700 - val_accuracy: 0.3796\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0601 - accuracy: 0.3920 - val_loss: 2.0700 - val_accuracy: 0.3789\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0599 - accuracy: 0.3928 - val_loss: 2.0695 - val_accuracy: 0.3794\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0596 - accuracy: 0.3923 - val_loss: 2.0692 - val_accuracy: 0.3793\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0595 - accuracy: 0.3921 - val_loss: 2.0699 - val_accuracy: 0.3788\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0594 - accuracy: 0.3925 - val_loss: 2.0695 - val_accuracy: 0.3789\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0592 - accuracy: 0.3929 - val_loss: 2.0691 - val_accuracy: 0.3793\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0591 - accuracy: 0.3934 - val_loss: 2.0689 - val_accuracy: 0.3791\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0589 - accuracy: 0.3929 - val_loss: 2.0684 - val_accuracy: 0.3804\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0587 - accuracy: 0.3934 - val_loss: 2.0687 - val_accuracy: 0.3806\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0455 - accuracy: 0.4114 - val_loss: 2.0424 - val_accuracy: 0.4155\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0319 - accuracy: 0.4271 - val_loss: 2.0405 - val_accuracy: 0.4153\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0307 - accuracy: 0.4272 - val_loss: 2.0398 - val_accuracy: 0.4161\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0297 - accuracy: 0.4283 - val_loss: 2.0387 - val_accuracy: 0.4155\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0289 - accuracy: 0.4286 - val_loss: 2.0379 - val_accuracy: 0.4172\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0282 - accuracy: 0.4289 - val_loss: 2.0374 - val_accuracy: 0.4172\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0278 - accuracy: 0.4281 - val_loss: 2.0373 - val_accuracy: 0.4156\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0272 - accuracy: 0.4295 - val_loss: 2.0368 - val_accuracy: 0.4163\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0265 - accuracy: 0.4298 - val_loss: 2.0361 - val_accuracy: 0.4183\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0253 - accuracy: 0.4312 - val_loss: 2.0327 - val_accuracy: 0.4231\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0232 - accuracy: 0.4357 - val_loss: 2.0313 - val_accuracy: 0.4246\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0221 - accuracy: 0.4367 - val_loss: 2.0313 - val_accuracy: 0.4240\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0216 - accuracy: 0.4369 - val_loss: 2.0310 - val_accuracy: 0.4269\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0212 - accuracy: 0.4371 - val_loss: 2.0306 - val_accuracy: 0.4268\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0209 - accuracy: 0.4374 - val_loss: 2.0306 - val_accuracy: 0.4261\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0206 - accuracy: 0.4376 - val_loss: 2.0317 - val_accuracy: 0.4244\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0203 - accuracy: 0.4380 - val_loss: 2.0305 - val_accuracy: 0.4261\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0200 - accuracy: 0.4383 - val_loss: 2.0298 - val_accuracy: 0.4259\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0200 - accuracy: 0.4379 - val_loss: 2.0294 - val_accuracy: 0.4279\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0196 - accuracy: 0.4385 - val_loss: 2.0303 - val_accuracy: 0.4258\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0193 - accuracy: 0.4388 - val_loss: 2.0308 - val_accuracy: 0.4250\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0191 - accuracy: 0.4393 - val_loss: 2.0291 - val_accuracy: 0.4279\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0190 - accuracy: 0.4387 - val_loss: 2.0292 - val_accuracy: 0.4270\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0187 - accuracy: 0.4391 - val_loss: 2.0296 - val_accuracy: 0.4266\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0186 - accuracy: 0.4395 - val_loss: 2.0286 - val_accuracy: 0.4289\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0185 - accuracy: 0.4389 - val_loss: 2.0286 - val_accuracy: 0.4266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0183 - accuracy: 0.4400 - val_loss: 2.0283 - val_accuracy: 0.4288\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0180 - accuracy: 0.4399 - val_loss: 2.0281 - val_accuracy: 0.4284\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0180 - accuracy: 0.4402 - val_loss: 2.0291 - val_accuracy: 0.4263\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0178 - accuracy: 0.4406 - val_loss: 2.0299 - val_accuracy: 0.4245\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0178 - accuracy: 0.4403 - val_loss: 2.0288 - val_accuracy: 0.4269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [16:20, 181.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 0\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2795 - accuracy: 0.1641 - val_loss: 2.2315 - val_accuracy: 0.2254\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2211 - accuracy: 0.2334 - val_loss: 2.2137 - val_accuracy: 0.2340\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2025 - accuracy: 0.2438 - val_loss: 2.1972 - val_accuracy: 0.2569\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1915 - accuracy: 0.2613 - val_loss: 2.1922 - val_accuracy: 0.2563\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1878 - accuracy: 0.2611 - val_loss: 2.1900 - val_accuracy: 0.2566\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1856 - accuracy: 0.2615 - val_loss: 2.1879 - val_accuracy: 0.2575\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1841 - accuracy: 0.2621 - val_loss: 2.1870 - val_accuracy: 0.2577\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1829 - accuracy: 0.2624 - val_loss: 2.1858 - val_accuracy: 0.2583\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1817 - accuracy: 0.2630 - val_loss: 2.1849 - val_accuracy: 0.2587\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1809 - accuracy: 0.2633 - val_loss: 2.1843 - val_accuracy: 0.2584\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1801 - accuracy: 0.2639 - val_loss: 2.1837 - val_accuracy: 0.2595\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1795 - accuracy: 0.2643 - val_loss: 2.1830 - val_accuracy: 0.2603\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1789 - accuracy: 0.2650 - val_loss: 2.1824 - val_accuracy: 0.2606\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1785 - accuracy: 0.2653 - val_loss: 2.1819 - val_accuracy: 0.2613\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1779 - accuracy: 0.2664 - val_loss: 2.1815 - val_accuracy: 0.2614\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1775 - accuracy: 0.2668 - val_loss: 2.1814 - val_accuracy: 0.2635\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1772 - accuracy: 0.2673 - val_loss: 2.1808 - val_accuracy: 0.2632\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1769 - accuracy: 0.2676 - val_loss: 2.1805 - val_accuracy: 0.2638\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1765 - accuracy: 0.2683 - val_loss: 2.1803 - val_accuracy: 0.2634\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1762 - accuracy: 0.2684 - val_loss: 2.1800 - val_accuracy: 0.2635\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1759 - accuracy: 0.2687 - val_loss: 2.1796 - val_accuracy: 0.2645\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1757 - accuracy: 0.2690 - val_loss: 2.1795 - val_accuracy: 0.2641\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1754 - accuracy: 0.2692 - val_loss: 2.1797 - val_accuracy: 0.2645\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1753 - accuracy: 0.2693 - val_loss: 2.1791 - val_accuracy: 0.2641\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1750 - accuracy: 0.2696 - val_loss: 2.1788 - val_accuracy: 0.2652\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1749 - accuracy: 0.2695 - val_loss: 2.1786 - val_accuracy: 0.2659\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1744 - accuracy: 0.2702 - val_loss: 2.1792 - val_accuracy: 0.2657\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1745 - accuracy: 0.2698 - val_loss: 2.1782 - val_accuracy: 0.2655\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1744 - accuracy: 0.2700 - val_loss: 2.1783 - val_accuracy: 0.2663\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1741 - accuracy: 0.2700 - val_loss: 2.1779 - val_accuracy: 0.2662\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1740 - accuracy: 0.2703 - val_loss: 2.1785 - val_accuracy: 0.2661\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1739 - accuracy: 0.2702 - val_loss: 2.1776 - val_accuracy: 0.2658\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1738 - accuracy: 0.2703 - val_loss: 2.1776 - val_accuracy: 0.2662\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1737 - accuracy: 0.2705 - val_loss: 2.1773 - val_accuracy: 0.2662\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1735 - accuracy: 0.2705 - val_loss: 2.1773 - val_accuracy: 0.2668\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1735 - accuracy: 0.2706 - val_loss: 2.1775 - val_accuracy: 0.2658\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1733 - accuracy: 0.2706 - val_loss: 2.1769 - val_accuracy: 0.2664\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1732 - accuracy: 0.2707 - val_loss: 2.1775 - val_accuracy: 0.2668\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1732 - accuracy: 0.2708 - val_loss: 2.1768 - val_accuracy: 0.2672\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1730 - accuracy: 0.2711 - val_loss: 2.1766 - val_accuracy: 0.2675\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1729 - accuracy: 0.2710 - val_loss: 2.1766 - val_accuracy: 0.2673\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1728 - accuracy: 0.2712 - val_loss: 2.1764 - val_accuracy: 0.2676\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1728 - accuracy: 0.2713 - val_loss: 2.1769 - val_accuracy: 0.2671\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1727 - accuracy: 0.2712 - val_loss: 2.1762 - val_accuracy: 0.2678\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1725 - accuracy: 0.2714 - val_loss: 2.1762 - val_accuracy: 0.2677\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1725 - accuracy: 0.2712 - val_loss: 2.1764 - val_accuracy: 0.2674\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1724 - accuracy: 0.2714 - val_loss: 2.1760 - val_accuracy: 0.2672\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1723 - accuracy: 0.2713 - val_loss: 2.1757 - val_accuracy: 0.2681\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1723 - accuracy: 0.2715 - val_loss: 2.1760 - val_accuracy: 0.2675\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1722 - accuracy: 0.2714 - val_loss: 2.1759 - val_accuracy: 0.2683\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1722 - accuracy: 0.2716 - val_loss: 2.1760 - val_accuracy: 0.2675\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [19:10, 164.42s/it]\u001b[A\n",
      " 10%|█         | 1/10 [19:13<2:52:58, 1153.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5832 - accuracy: 0.8910 - val_loss: 1.5196 - val_accuracy: 0.9455\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 1\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5106 - accuracy: 0.9531 - val_loss: 1.5061 - val_accuracy: 0.9574\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4964 - accuracy: 0.9665 - val_loss: 1.4952 - val_accuracy: 0.9675\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4905 - accuracy: 0.9718 - val_loss: 1.4948 - val_accuracy: 0.9668\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4860 - accuracy: 0.9762 - val_loss: 1.4891 - val_accuracy: 0.9730\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4828 - accuracy: 0.9791 - val_loss: 1.4912 - val_accuracy: 0.9704\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4805 - accuracy: 0.9815 - val_loss: 1.4917 - val_accuracy: 0.9694\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4789 - accuracy: 0.9827 - val_loss: 1.4846 - val_accuracy: 0.9770\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4783 - accuracy: 0.9831 - val_loss: 1.4865 - val_accuracy: 0.9761\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9856 - val_loss: 1.4854 - val_accuracy: 0.9755\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4761 - accuracy: 0.9851 - val_loss: 1.4875 - val_accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:07, 127.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 1\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4906 - accuracy: 0.9739 - val_loss: 1.4906 - val_accuracy: 0.9724\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4795 - accuracy: 0.9836 - val_loss: 1.4885 - val_accuracy: 0.9745\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4767 - accuracy: 0.9857 - val_loss: 1.4896 - val_accuracy: 0.9722\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4748 - accuracy: 0.9875 - val_loss: 1.4893 - val_accuracy: 0.9726\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4734 - accuracy: 0.9888 - val_loss: 1.4882 - val_accuracy: 0.9737\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4722 - accuracy: 0.9898 - val_loss: 1.4864 - val_accuracy: 0.9755\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9904 - val_loss: 1.4857 - val_accuracy: 0.9762\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4712 - accuracy: 0.9907 - val_loss: 1.4866 - val_accuracy: 0.9755\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4706 - accuracy: 0.9912 - val_loss: 1.4862 - val_accuracy: 0.9750\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4701 - accuracy: 0.9913 - val_loss: 1.4868 - val_accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:06, 124.98s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 1\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5560 - accuracy: 0.9218 - val_loss: 1.5207 - val_accuracy: 0.9462\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5110 - accuracy: 0.9561 - val_loss: 1.5133 - val_accuracy: 0.9515\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5030 - accuracy: 0.9631 - val_loss: 1.5099 - val_accuracy: 0.9546\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4976 - accuracy: 0.9672 - val_loss: 1.5079 - val_accuracy: 0.9547\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4939 - accuracy: 0.9710 - val_loss: 1.5065 - val_accuracy: 0.9569\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4910 - accuracy: 0.9737 - val_loss: 1.5059 - val_accuracy: 0.9567\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4890 - accuracy: 0.9754 - val_loss: 1.5044 - val_accuracy: 0.9583\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4869 - accuracy: 0.9769 - val_loss: 1.5040 - val_accuracy: 0.9581\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4852 - accuracy: 0.9786 - val_loss: 1.5027 - val_accuracy: 0.9596\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4841 - accuracy: 0.9796 - val_loss: 1.5037 - val_accuracy: 0.9575\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4830 - accuracy: 0.9804 - val_loss: 1.5024 - val_accuracy: 0.9602\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4820 - accuracy: 0.9812 - val_loss: 1.5016 - val_accuracy: 0.9611\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4811 - accuracy: 0.9820 - val_loss: 1.5027 - val_accuracy: 0.9590\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4801 - accuracy: 0.9830 - val_loss: 1.5026 - val_accuracy: 0.9594\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4795 - accuracy: 0.9834 - val_loss: 1.5021 - val_accuracy: 0.9596\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:17, 126.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 1\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7371 - accuracy: 0.7487 - val_loss: 1.6200 - val_accuracy: 0.8552\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6053 - accuracy: 0.8676 - val_loss: 1.5949 - val_accuracy: 0.8760\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5890 - accuracy: 0.8812 - val_loss: 1.5857 - val_accuracy: 0.8824\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5810 - accuracy: 0.8880 - val_loss: 1.5788 - val_accuracy: 0.8893\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5752 - accuracy: 0.8931 - val_loss: 1.5755 - val_accuracy: 0.8917\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5711 - accuracy: 0.8966 - val_loss: 1.5720 - val_accuracy: 0.8940\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5675 - accuracy: 0.8996 - val_loss: 1.5697 - val_accuracy: 0.8948\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5648 - accuracy: 0.9018 - val_loss: 1.5676 - val_accuracy: 0.8964\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5625 - accuracy: 0.9034 - val_loss: 1.5655 - val_accuracy: 0.8989\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5605 - accuracy: 0.9055 - val_loss: 1.5650 - val_accuracy: 0.8988\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5589 - accuracy: 0.9068 - val_loss: 1.5647 - val_accuracy: 0.9004\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5572 - accuracy: 0.9086 - val_loss: 1.5626 - val_accuracy: 0.9011\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5556 - accuracy: 0.9101 - val_loss: 1.5624 - val_accuracy: 0.9008\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5545 - accuracy: 0.9105 - val_loss: 1.5632 - val_accuracy: 0.9004\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5533 - accuracy: 0.9118 - val_loss: 1.5616 - val_accuracy: 0.9018\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5523 - accuracy: 0.9131 - val_loss: 1.5611 - val_accuracy: 0.9013\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5513 - accuracy: 0.9139 - val_loss: 1.5600 - val_accuracy: 0.9025\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5502 - accuracy: 0.9150 - val_loss: 1.5597 - val_accuracy: 0.9034\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5497 - accuracy: 0.9152 - val_loss: 1.5602 - val_accuracy: 0.9008\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5488 - accuracy: 0.9161 - val_loss: 1.5588 - val_accuracy: 0.9040\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5479 - accuracy: 0.9167 - val_loss: 1.5594 - val_accuracy: 0.9035\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5475 - accuracy: 0.9169 - val_loss: 1.5594 - val_accuracy: 0.9021\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5467 - accuracy: 0.9180 - val_loss: 1.5599 - val_accuracy: 0.9018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:03, 138.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 1\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9419 - accuracy: 0.5349 - val_loss: 1.8763 - val_accuracy: 0.5940\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8655 - accuracy: 0.6040 - val_loss: 1.8260 - val_accuracy: 0.6467\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7853 - accuracy: 0.6880 - val_loss: 1.7502 - val_accuracy: 0.7210\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7454 - accuracy: 0.7247 - val_loss: 1.7342 - val_accuracy: 0.7364\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7339 - accuracy: 0.7350 - val_loss: 1.7288 - val_accuracy: 0.7393\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7266 - accuracy: 0.7413 - val_loss: 1.7198 - val_accuracy: 0.7473\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7213 - accuracy: 0.7455 - val_loss: 1.7175 - val_accuracy: 0.7507\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7175 - accuracy: 0.7490 - val_loss: 1.7120 - val_accuracy: 0.7549\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7143 - accuracy: 0.7518 - val_loss: 1.7088 - val_accuracy: 0.7576\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7114 - accuracy: 0.7538 - val_loss: 1.7073 - val_accuracy: 0.7580\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7091 - accuracy: 0.7569 - val_loss: 1.7066 - val_accuracy: 0.7588\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7072 - accuracy: 0.7581 - val_loss: 1.7045 - val_accuracy: 0.7630\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7056 - accuracy: 0.7597 - val_loss: 1.7027 - val_accuracy: 0.7624\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7040 - accuracy: 0.7614 - val_loss: 1.7001 - val_accuracy: 0.7629\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7027 - accuracy: 0.7625 - val_loss: 1.7007 - val_accuracy: 0.7647\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7012 - accuracy: 0.7640 - val_loss: 1.6989 - val_accuracy: 0.7651\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7004 - accuracy: 0.7641 - val_loss: 1.6970 - val_accuracy: 0.7679\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6992 - accuracy: 0.7649 - val_loss: 1.6973 - val_accuracy: 0.7665\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6983 - accuracy: 0.7654 - val_loss: 1.6966 - val_accuracy: 0.7672\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6972 - accuracy: 0.7665 - val_loss: 1.6950 - val_accuracy: 0.7690\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6962 - accuracy: 0.7678 - val_loss: 1.6952 - val_accuracy: 0.7690\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6955 - accuracy: 0.7677 - val_loss: 1.6936 - val_accuracy: 0.7693\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6948 - accuracy: 0.7687 - val_loss: 1.6946 - val_accuracy: 0.7693\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6939 - accuracy: 0.7699 - val_loss: 1.6939 - val_accuracy: 0.7697\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6933 - accuracy: 0.7704 - val_loss: 1.6938 - val_accuracy: 0.7700\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [11:43, 144.91s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 1\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1007 - accuracy: 0.3716 - val_loss: 2.0384 - val_accuracy: 0.4286\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0142 - accuracy: 0.4575 - val_loss: 1.9930 - val_accuracy: 0.4860\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9527 - accuracy: 0.5334 - val_loss: 1.9410 - val_accuracy: 0.5401\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9289 - accuracy: 0.5505 - val_loss: 1.9265 - val_accuracy: 0.5499\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9159 - accuracy: 0.5588 - val_loss: 1.9149 - val_accuracy: 0.5565\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9071 - accuracy: 0.5671 - val_loss: 1.9078 - val_accuracy: 0.5644\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9014 - accuracy: 0.5711 - val_loss: 1.9031 - val_accuracy: 0.5678\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8971 - accuracy: 0.5738 - val_loss: 1.8999 - val_accuracy: 0.5692\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8936 - accuracy: 0.5758 - val_loss: 1.8973 - val_accuracy: 0.5700\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8909 - accuracy: 0.5770 - val_loss: 1.8946 - val_accuracy: 0.5724\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8887 - accuracy: 0.5791 - val_loss: 1.8931 - val_accuracy: 0.5728\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8866 - accuracy: 0.5802 - val_loss: 1.8915 - val_accuracy: 0.5736\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8846 - accuracy: 0.5813 - val_loss: 1.8893 - val_accuracy: 0.5761\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8828 - accuracy: 0.5830 - val_loss: 1.8881 - val_accuracy: 0.5766\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8812 - accuracy: 0.5841 - val_loss: 1.8874 - val_accuracy: 0.5765\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8798 - accuracy: 0.5853 - val_loss: 1.8867 - val_accuracy: 0.5777\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8786 - accuracy: 0.5861 - val_loss: 1.8846 - val_accuracy: 0.5799\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8776 - accuracy: 0.5865 - val_loss: 1.8843 - val_accuracy: 0.5800\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8767 - accuracy: 0.5876 - val_loss: 1.8834 - val_accuracy: 0.5812\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8759 - accuracy: 0.5878 - val_loss: 1.8820 - val_accuracy: 0.5815\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8750 - accuracy: 0.5884 - val_loss: 1.8817 - val_accuracy: 0.5827\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8741 - accuracy: 0.5893 - val_loss: 1.8804 - val_accuracy: 0.5823\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8733 - accuracy: 0.5897 - val_loss: 1.8800 - val_accuracy: 0.5834\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8726 - accuracy: 0.5906 - val_loss: 1.8797 - val_accuracy: 0.5841\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8719 - accuracy: 0.5915 - val_loss: 1.8794 - val_accuracy: 0.5833\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8710 - accuracy: 0.5919 - val_loss: 1.8796 - val_accuracy: 0.5853\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8704 - accuracy: 0.5927 - val_loss: 1.8768 - val_accuracy: 0.5851\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8694 - accuracy: 0.5935 - val_loss: 1.8770 - val_accuracy: 0.5862\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8686 - accuracy: 0.5942 - val_loss: 1.8755 - val_accuracy: 0.5867\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8679 - accuracy: 0.5953 - val_loss: 1.8754 - val_accuracy: 0.5864\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8676 - accuracy: 0.5948 - val_loss: 1.8757 - val_accuracy: 0.5862\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8668 - accuracy: 0.5959 - val_loss: 1.8749 - val_accuracy: 0.5868\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8664 - accuracy: 0.5960 - val_loss: 1.8732 - val_accuracy: 0.5892\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8661 - accuracy: 0.5966 - val_loss: 1.8731 - val_accuracy: 0.5887\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8656 - accuracy: 0.5968 - val_loss: 1.8732 - val_accuracy: 0.5895\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8652 - accuracy: 0.5968 - val_loss: 1.8731 - val_accuracy: 0.5887\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8649 - accuracy: 0.5967 - val_loss: 1.8725 - val_accuracy: 0.5890\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8644 - accuracy: 0.5977 - val_loss: 1.8730 - val_accuracy: 0.5898\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8642 - accuracy: 0.5976 - val_loss: 1.8722 - val_accuracy: 0.5905\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8639 - accuracy: 0.5982 - val_loss: 1.8720 - val_accuracy: 0.5892\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8637 - accuracy: 0.5984 - val_loss: 1.8730 - val_accuracy: 0.5901\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8634 - accuracy: 0.5982 - val_loss: 1.8718 - val_accuracy: 0.5897\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8632 - accuracy: 0.5987 - val_loss: 1.8712 - val_accuracy: 0.5895\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8629 - accuracy: 0.5990 - val_loss: 1.8708 - val_accuracy: 0.5908\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8627 - accuracy: 0.5994 - val_loss: 1.8706 - val_accuracy: 0.5921\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8625 - accuracy: 0.5998 - val_loss: 1.8712 - val_accuracy: 0.5883\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8625 - accuracy: 0.5992 - val_loss: 1.8709 - val_accuracy: 0.5903\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8623 - accuracy: 0.5998 - val_loss: 1.8709 - val_accuracy: 0.5897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [14:48, 156.95s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 1\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2817 - accuracy: 0.1747 - val_loss: 2.2586 - val_accuracy: 0.1982\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2512 - accuracy: 0.2054 - val_loss: 2.2564 - val_accuracy: 0.1992\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2491 - accuracy: 0.2063 - val_loss: 2.2549 - val_accuracy: 0.1996\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2476 - accuracy: 0.2072 - val_loss: 2.2535 - val_accuracy: 0.2000\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2464 - accuracy: 0.2079 - val_loss: 2.2527 - val_accuracy: 0.2000\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2455 - accuracy: 0.2085 - val_loss: 2.2519 - val_accuracy: 0.2003\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2282 - accuracy: 0.2137 - val_loss: 2.2173 - val_accuracy: 0.2150\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2127 - accuracy: 0.2179 - val_loss: 2.2148 - val_accuracy: 0.2164\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2083 - accuracy: 0.2199 - val_loss: 2.2113 - val_accuracy: 0.2177\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2057 - accuracy: 0.2240 - val_loss: 2.2103 - val_accuracy: 0.2197\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2045 - accuracy: 0.2228 - val_loss: 2.2096 - val_accuracy: 0.2190\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2037 - accuracy: 0.2252 - val_loss: 2.2088 - val_accuracy: 0.2211\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2028 - accuracy: 0.2268 - val_loss: 2.2084 - val_accuracy: 0.2202\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2014 - accuracy: 0.2255 - val_loss: 2.2063 - val_accuracy: 0.2249\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1986 - accuracy: 0.2315 - val_loss: 2.2046 - val_accuracy: 0.2208\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1974 - accuracy: 0.2319 - val_loss: 2.2038 - val_accuracy: 0.2208\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1968 - accuracy: 0.2302 - val_loss: 2.2036 - val_accuracy: 0.2259\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1962 - accuracy: 0.2318 - val_loss: 2.2031 - val_accuracy: 0.2257\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1955 - accuracy: 0.2330 - val_loss: 2.2018 - val_accuracy: 0.2233\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1946 - accuracy: 0.2339 - val_loss: 2.2006 - val_accuracy: 0.2255\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1929 - accuracy: 0.2386 - val_loss: 2.1987 - val_accuracy: 0.2329\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1880 - accuracy: 0.2439 - val_loss: 2.1823 - val_accuracy: 0.2533\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1719 - accuracy: 0.2582 - val_loss: 2.1783 - val_accuracy: 0.2534\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1686 - accuracy: 0.2646 - val_loss: 2.1756 - val_accuracy: 0.2525\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1674 - accuracy: 0.2652 - val_loss: 2.1747 - val_accuracy: 0.2567\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1666 - accuracy: 0.2641 - val_loss: 2.1735 - val_accuracy: 0.2581\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1642 - accuracy: 0.2674 - val_loss: 2.1676 - val_accuracy: 0.2652\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1604 - accuracy: 0.2706 - val_loss: 2.1669 - val_accuracy: 0.2659\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1599 - accuracy: 0.2717 - val_loss: 2.1666 - val_accuracy: 0.2658\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1597 - accuracy: 0.2713 - val_loss: 2.1663 - val_accuracy: 0.2658\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1595 - accuracy: 0.2696 - val_loss: 2.1664 - val_accuracy: 0.2660\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1591 - accuracy: 0.2709 - val_loss: 2.1659 - val_accuracy: 0.2659\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1589 - accuracy: 0.2727 - val_loss: 2.1659 - val_accuracy: 0.2674\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1589 - accuracy: 0.2716 - val_loss: 2.1656 - val_accuracy: 0.2674\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1587 - accuracy: 0.2704 - val_loss: 2.1656 - val_accuracy: 0.2674\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1585 - accuracy: 0.2707 - val_loss: 2.1654 - val_accuracy: 0.2660\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1584 - accuracy: 0.2723 - val_loss: 2.1657 - val_accuracy: 0.2663\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1583 - accuracy: 0.2723 - val_loss: 2.1652 - val_accuracy: 0.2662\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1582 - accuracy: 0.2724 - val_loss: 2.1653 - val_accuracy: 0.2659\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1581 - accuracy: 0.2735 - val_loss: 2.1653 - val_accuracy: 0.2664\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1580 - accuracy: 0.2730 - val_loss: 2.1651 - val_accuracy: 0.2629\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1578 - accuracy: 0.2727 - val_loss: 2.1648 - val_accuracy: 0.2665\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1579 - accuracy: 0.2714 - val_loss: 2.1648 - val_accuracy: 0.2667\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1577 - accuracy: 0.2721 - val_loss: 2.1646 - val_accuracy: 0.2665\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1577 - accuracy: 0.2729 - val_loss: 2.1649 - val_accuracy: 0.2669\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1575 - accuracy: 0.2746 - val_loss: 2.1648 - val_accuracy: 0.2682\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1575 - accuracy: 0.2731 - val_loss: 2.1645 - val_accuracy: 0.2668\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1574 - accuracy: 0.2740 - val_loss: 2.1644 - val_accuracy: 0.2669\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1574 - accuracy: 0.2726 - val_loss: 2.1643 - val_accuracy: 0.2671\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1574 - accuracy: 0.2732 - val_loss: 2.1646 - val_accuracy: 0.2675\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1572 - accuracy: 0.2743 - val_loss: 2.1643 - val_accuracy: 0.2675\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1573 - accuracy: 0.2735 - val_loss: 2.1641 - val_accuracy: 0.2676\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1572 - accuracy: 0.2738 - val_loss: 2.1641 - val_accuracy: 0.2638\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1571 - accuracy: 0.2754 - val_loss: 2.1641 - val_accuracy: 0.2679\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1570 - accuracy: 0.2740 - val_loss: 2.1642 - val_accuracy: 0.2643\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1570 - accuracy: 0.2732 - val_loss: 2.1639 - val_accuracy: 0.2679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1569 - accuracy: 0.2737 - val_loss: 2.1639 - val_accuracy: 0.2692\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1570 - accuracy: 0.2725 - val_loss: 2.1639 - val_accuracy: 0.2676\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1568 - accuracy: 0.2751 - val_loss: 2.1639 - val_accuracy: 0.2644\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1568 - accuracy: 0.2738 - val_loss: 2.1640 - val_accuracy: 0.2693\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [17:44, 152.14s/it]\u001b[A\n",
      " 20%|██        | 2/10 [37:00<2:30:19, 1127.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5821 - accuracy: 0.8907 - val_loss: 1.5181 - val_accuracy: 0.9483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 2\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5109 - accuracy: 0.9525 - val_loss: 1.5035 - val_accuracy: 0.9584\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4976 - accuracy: 0.9649 - val_loss: 1.4936 - val_accuracy: 0.9692\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4905 - accuracy: 0.9720 - val_loss: 1.4940 - val_accuracy: 0.9684\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4851 - accuracy: 0.9767 - val_loss: 1.4894 - val_accuracy: 0.9727\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4827 - accuracy: 0.9789 - val_loss: 1.4875 - val_accuracy: 0.9745\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4803 - accuracy: 0.9815 - val_loss: 1.4867 - val_accuracy: 0.9749\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4783 - accuracy: 0.9833 - val_loss: 1.4837 - val_accuracy: 0.9781\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4782 - accuracy: 0.9831 - val_loss: 1.4885 - val_accuracy: 0.9722\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4755 - accuracy: 0.9859 - val_loss: 1.4860 - val_accuracy: 0.9751\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4756 - accuracy: 0.9859 - val_loss: 1.4884 - val_accuracy: 0.9729\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:02, 122.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 2\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4885 - accuracy: 0.9760 - val_loss: 1.4894 - val_accuracy: 0.9734\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4786 - accuracy: 0.9843 - val_loss: 1.4888 - val_accuracy: 0.9732\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4751 - accuracy: 0.9874 - val_loss: 1.4858 - val_accuracy: 0.9763\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4742 - accuracy: 0.9883 - val_loss: 1.4875 - val_accuracy: 0.9743\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4727 - accuracy: 0.9895 - val_loss: 1.4868 - val_accuracy: 0.9748\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4718 - accuracy: 0.9901 - val_loss: 1.4848 - val_accuracy: 0.9769\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4711 - accuracy: 0.9906 - val_loss: 1.4868 - val_accuracy: 0.9751\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4708 - accuracy: 0.9910 - val_loss: 1.4836 - val_accuracy: 0.9783\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4701 - accuracy: 0.9916 - val_loss: 1.4842 - val_accuracy: 0.9775\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4699 - accuracy: 0.9918 - val_loss: 1.4852 - val_accuracy: 0.9760\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4696 - accuracy: 0.9921 - val_loss: 1.4842 - val_accuracy: 0.9772\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:07, 123.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 2\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5742 - accuracy: 0.9043 - val_loss: 1.5338 - val_accuracy: 0.9357\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5223 - accuracy: 0.9457 - val_loss: 1.5247 - val_accuracy: 0.9415\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5124 - accuracy: 0.9539 - val_loss: 1.5214 - val_accuracy: 0.9436\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5067 - accuracy: 0.9589 - val_loss: 1.5170 - val_accuracy: 0.9467\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5019 - accuracy: 0.9636 - val_loss: 1.5150 - val_accuracy: 0.9480\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4984 - accuracy: 0.9669 - val_loss: 1.5145 - val_accuracy: 0.9487\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4958 - accuracy: 0.9690 - val_loss: 1.5118 - val_accuracy: 0.9514\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4932 - accuracy: 0.9712 - val_loss: 1.5137 - val_accuracy: 0.9499\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4916 - accuracy: 0.9726 - val_loss: 1.5115 - val_accuracy: 0.9512\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4899 - accuracy: 0.9743 - val_loss: 1.5121 - val_accuracy: 0.9504\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4882 - accuracy: 0.9756 - val_loss: 1.5102 - val_accuracy: 0.9522\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4873 - accuracy: 0.9761 - val_loss: 1.5093 - val_accuracy: 0.9538\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4861 - accuracy: 0.9775 - val_loss: 1.5085 - val_accuracy: 0.9543\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4849 - accuracy: 0.9787 - val_loss: 1.5091 - val_accuracy: 0.9526\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4841 - accuracy: 0.9793 - val_loss: 1.5074 - val_accuracy: 0.9540\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.9800 - val_loss: 1.5091 - val_accuracy: 0.9527\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4828 - accuracy: 0.9802 - val_loss: 1.5085 - val_accuracy: 0.9537\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4819 - accuracy: 0.9811 - val_loss: 1.5086 - val_accuracy: 0.9534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:24, 127.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 2\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7237 - accuracy: 0.7551 - val_loss: 1.6158 - val_accuracy: 0.8581\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6068 - accuracy: 0.8642 - val_loss: 1.5956 - val_accuracy: 0.8731\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5915 - accuracy: 0.8773 - val_loss: 1.5868 - val_accuracy: 0.8815\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5839 - accuracy: 0.8845 - val_loss: 1.5825 - val_accuracy: 0.8848\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5785 - accuracy: 0.8895 - val_loss: 1.5789 - val_accuracy: 0.8879\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5746 - accuracy: 0.8926 - val_loss: 1.5761 - val_accuracy: 0.8898\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5711 - accuracy: 0.8959 - val_loss: 1.5768 - val_accuracy: 0.8884\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5685 - accuracy: 0.8981 - val_loss: 1.5769 - val_accuracy: 0.8867\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5661 - accuracy: 0.9004 - val_loss: 1.5720 - val_accuracy: 0.8926\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5641 - accuracy: 0.9021 - val_loss: 1.5731 - val_accuracy: 0.8908\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5623 - accuracy: 0.9041 - val_loss: 1.5706 - val_accuracy: 0.8936\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5606 - accuracy: 0.9056 - val_loss: 1.5708 - val_accuracy: 0.8921\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5591 - accuracy: 0.9065 - val_loss: 1.5707 - val_accuracy: 0.8926\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5578 - accuracy: 0.9078 - val_loss: 1.5697 - val_accuracy: 0.8927\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5566 - accuracy: 0.9093 - val_loss: 1.5695 - val_accuracy: 0.8938\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5554 - accuracy: 0.9101 - val_loss: 1.5695 - val_accuracy: 0.8931\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5542 - accuracy: 0.9110 - val_loss: 1.5692 - val_accuracy: 0.8927\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5533 - accuracy: 0.9119 - val_loss: 1.5668 - val_accuracy: 0.8960\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5522 - accuracy: 0.9132 - val_loss: 1.5684 - val_accuracy: 0.8933\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5517 - accuracy: 0.9133 - val_loss: 1.5678 - val_accuracy: 0.8960\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5511 - accuracy: 0.9140 - val_loss: 1.5658 - val_accuracy: 0.8971\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5500 - accuracy: 0.9149 - val_loss: 1.5674 - val_accuracy: 0.8947\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5492 - accuracy: 0.9154 - val_loss: 1.5648 - val_accuracy: 0.8969\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5484 - accuracy: 0.9164 - val_loss: 1.5660 - val_accuracy: 0.8963\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5478 - accuracy: 0.9172 - val_loss: 1.5644 - val_accuracy: 0.8987\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5472 - accuracy: 0.9177 - val_loss: 1.5646 - val_accuracy: 0.8972\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5468 - accuracy: 0.9182 - val_loss: 1.5666 - val_accuracy: 0.8949\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5460 - accuracy: 0.9189 - val_loss: 1.5651 - val_accuracy: 0.8979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:11, 139.27s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 2\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9925 - accuracy: 0.4770 - val_loss: 1.9169 - val_accuracy: 0.5507\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9022 - accuracy: 0.5637 - val_loss: 1.8944 - val_accuracy: 0.5686\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8857 - accuracy: 0.5782 - val_loss: 1.8649 - val_accuracy: 0.5997\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8519 - accuracy: 0.6124 - val_loss: 1.8480 - val_accuracy: 0.6154\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8419 - accuracy: 0.6217 - val_loss: 1.8421 - val_accuracy: 0.6206\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8363 - accuracy: 0.6270 - val_loss: 1.8390 - val_accuracy: 0.6230\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8325 - accuracy: 0.6309 - val_loss: 1.8350 - val_accuracy: 0.6253\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8297 - accuracy: 0.6338 - val_loss: 1.8356 - val_accuracy: 0.6242\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8275 - accuracy: 0.6339 - val_loss: 1.8317 - val_accuracy: 0.6296\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8253 - accuracy: 0.6363 - val_loss: 1.8298 - val_accuracy: 0.6303\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8237 - accuracy: 0.6374 - val_loss: 1.8307 - val_accuracy: 0.6294\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8221 - accuracy: 0.6391 - val_loss: 1.8281 - val_accuracy: 0.6312\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8206 - accuracy: 0.6409 - val_loss: 1.8269 - val_accuracy: 0.6323\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8194 - accuracy: 0.6420 - val_loss: 1.8270 - val_accuracy: 0.6321\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8184 - accuracy: 0.6424 - val_loss: 1.8260 - val_accuracy: 0.6334\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8176 - accuracy: 0.6436 - val_loss: 1.8237 - val_accuracy: 0.6347\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8166 - accuracy: 0.6436 - val_loss: 1.8234 - val_accuracy: 0.6362\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8156 - accuracy: 0.6452 - val_loss: 1.8236 - val_accuracy: 0.6371\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8150 - accuracy: 0.6456 - val_loss: 1.8228 - val_accuracy: 0.6369\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8139 - accuracy: 0.6467 - val_loss: 1.8217 - val_accuracy: 0.6371\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8135 - accuracy: 0.6467 - val_loss: 1.8224 - val_accuracy: 0.6382\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8127 - accuracy: 0.6478 - val_loss: 1.8223 - val_accuracy: 0.6367\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8121 - accuracy: 0.6485 - val_loss: 1.8205 - val_accuracy: 0.6383\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8115 - accuracy: 0.6490 - val_loss: 1.8198 - val_accuracy: 0.6402\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8110 - accuracy: 0.6491 - val_loss: 1.8203 - val_accuracy: 0.6361\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8104 - accuracy: 0.6502 - val_loss: 1.8200 - val_accuracy: 0.6384\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8100 - accuracy: 0.6500 - val_loss: 1.8195 - val_accuracy: 0.6382\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8095 - accuracy: 0.6505 - val_loss: 1.8188 - val_accuracy: 0.6398\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8093 - accuracy: 0.6508 - val_loss: 1.8187 - val_accuracy: 0.6407\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8088 - accuracy: 0.6509 - val_loss: 1.8190 - val_accuracy: 0.6399\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8084 - accuracy: 0.6515 - val_loss: 1.8179 - val_accuracy: 0.6385\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8081 - accuracy: 0.6522 - val_loss: 1.8183 - val_accuracy: 0.6382\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8076 - accuracy: 0.6528 - val_loss: 1.8186 - val_accuracy: 0.6386\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8071 - accuracy: 0.6530 - val_loss: 1.8172 - val_accuracy: 0.6406\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8071 - accuracy: 0.6527 - val_loss: 1.8177 - val_accuracy: 0.6406\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8067 - accuracy: 0.6533 - val_loss: 1.8169 - val_accuracy: 0.6405\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8062 - accuracy: 0.6533 - val_loss: 1.8169 - val_accuracy: 0.6396\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8062 - accuracy: 0.6532 - val_loss: 1.8162 - val_accuracy: 0.6420\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8056 - accuracy: 0.6543 - val_loss: 1.8161 - val_accuracy: 0.6412\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8056 - accuracy: 0.6544 - val_loss: 1.8172 - val_accuracy: 0.6413\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8052 - accuracy: 0.6545 - val_loss: 1.8161 - val_accuracy: 0.6415\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8052 - accuracy: 0.6543 - val_loss: 1.8155 - val_accuracy: 0.6421\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8047 - accuracy: 0.6551 - val_loss: 1.8157 - val_accuracy: 0.6414\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8046 - accuracy: 0.6545 - val_loss: 1.8166 - val_accuracy: 0.6414\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8044 - accuracy: 0.6545 - val_loss: 1.8163 - val_accuracy: 0.6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:22, 154.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 2\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2132 - accuracy: 0.2414 - val_loss: 2.1472 - val_accuracy: 0.3094\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1206 - accuracy: 0.3467 - val_loss: 2.1106 - val_accuracy: 0.3493\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0923 - accuracy: 0.3719 - val_loss: 2.0657 - val_accuracy: 0.3959\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0581 - accuracy: 0.4064 - val_loss: 2.0558 - val_accuracy: 0.4040\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0504 - accuracy: 0.4119 - val_loss: 2.0509 - val_accuracy: 0.4083\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0453 - accuracy: 0.4173 - val_loss: 2.0472 - val_accuracy: 0.4114\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0422 - accuracy: 0.4192 - val_loss: 2.0462 - val_accuracy: 0.4116\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0396 - accuracy: 0.4204 - val_loss: 2.0403 - val_accuracy: 0.4160\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0177 - accuracy: 0.4453 - val_loss: 2.0148 - val_accuracy: 0.4439\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0118 - accuracy: 0.4497 - val_loss: 2.0119 - val_accuracy: 0.4478\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0096 - accuracy: 0.4518 - val_loss: 2.0103 - val_accuracy: 0.4471\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0081 - accuracy: 0.4523 - val_loss: 2.0085 - val_accuracy: 0.4493\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0069 - accuracy: 0.4537 - val_loss: 2.0073 - val_accuracy: 0.4512\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0060 - accuracy: 0.4552 - val_loss: 2.0073 - val_accuracy: 0.4520\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0051 - accuracy: 0.4556 - val_loss: 2.0063 - val_accuracy: 0.4521\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0043 - accuracy: 0.4571 - val_loss: 2.0061 - val_accuracy: 0.4515\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0036 - accuracy: 0.4573 - val_loss: 2.0042 - val_accuracy: 0.4527\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0031 - accuracy: 0.4576 - val_loss: 2.0035 - val_accuracy: 0.4528\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0024 - accuracy: 0.4586 - val_loss: 2.0034 - val_accuracy: 0.4539\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0018 - accuracy: 0.4591 - val_loss: 2.0030 - val_accuracy: 0.4541\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0012 - accuracy: 0.4595 - val_loss: 2.0020 - val_accuracy: 0.4549\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0009 - accuracy: 0.4597 - val_loss: 2.0023 - val_accuracy: 0.4548\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0004 - accuracy: 0.4597 - val_loss: 2.0021 - val_accuracy: 0.4555\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0001 - accuracy: 0.4604 - val_loss: 2.0022 - val_accuracy: 0.4550\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [14:53, 153.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 2\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2554 - accuracy: 0.1838 - val_loss: 2.2155 - val_accuracy: 0.2317\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2072 - accuracy: 0.2441 - val_loss: 2.1845 - val_accuracy: 0.2738\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1734 - accuracy: 0.2795 - val_loss: 2.1595 - val_accuracy: 0.2869\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1503 - accuracy: 0.2978 - val_loss: 2.1426 - val_accuracy: 0.3017\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1377 - accuracy: 0.3071 - val_loss: 2.1326 - val_accuracy: 0.3146\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1290 - accuracy: 0.3171 - val_loss: 2.1269 - val_accuracy: 0.3209\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1245 - accuracy: 0.3219 - val_loss: 2.1235 - val_accuracy: 0.3155\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1216 - accuracy: 0.3227 - val_loss: 2.1212 - val_accuracy: 0.3219\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1194 - accuracy: 0.3240 - val_loss: 2.1196 - val_accuracy: 0.3193\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1174 - accuracy: 0.3252 - val_loss: 2.1178 - val_accuracy: 0.3205\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1154 - accuracy: 0.3282 - val_loss: 2.1161 - val_accuracy: 0.3261\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1135 - accuracy: 0.3330 - val_loss: 2.1144 - val_accuracy: 0.3268\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1119 - accuracy: 0.3342 - val_loss: 2.1130 - val_accuracy: 0.3275\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1105 - accuracy: 0.3366 - val_loss: 2.1118 - val_accuracy: 0.3349\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1092 - accuracy: 0.3352 - val_loss: 2.1105 - val_accuracy: 0.3282\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1080 - accuracy: 0.3363 - val_loss: 2.1095 - val_accuracy: 0.3358\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1071 - accuracy: 0.3372 - val_loss: 2.1088 - val_accuracy: 0.3270\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1061 - accuracy: 0.3367 - val_loss: 2.1077 - val_accuracy: 0.3287\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1054 - accuracy: 0.3372 - val_loss: 2.1068 - val_accuracy: 0.3363\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1046 - accuracy: 0.3385 - val_loss: 2.1063 - val_accuracy: 0.3383\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1038 - accuracy: 0.3384 - val_loss: 2.1056 - val_accuracy: 0.3365\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1032 - accuracy: 0.3390 - val_loss: 2.1049 - val_accuracy: 0.3379\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1025 - accuracy: 0.3388 - val_loss: 2.1046 - val_accuracy: 0.3371\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1020 - accuracy: 0.3401 - val_loss: 2.1044 - val_accuracy: 0.3372\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1016 - accuracy: 0.3401 - val_loss: 2.1034 - val_accuracy: 0.3373\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1011 - accuracy: 0.3411 - val_loss: 2.1029 - val_accuracy: 0.3388\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1007 - accuracy: 0.3411 - val_loss: 2.1026 - val_accuracy: 0.3386\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1004 - accuracy: 0.3416 - val_loss: 2.1024 - val_accuracy: 0.3383\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0999 - accuracy: 0.3428 - val_loss: 2.1024 - val_accuracy: 0.3380\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0997 - accuracy: 0.3401 - val_loss: 2.1017 - val_accuracy: 0.3382\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0993 - accuracy: 0.3418 - val_loss: 2.1015 - val_accuracy: 0.3324\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0991 - accuracy: 0.3419 - val_loss: 2.1023 - val_accuracy: 0.3316\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0988 - accuracy: 0.3426 - val_loss: 2.1014 - val_accuracy: 0.3419\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0986 - accuracy: 0.3427 - val_loss: 2.1007 - val_accuracy: 0.3407\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0982 - accuracy: 0.3435 - val_loss: 2.1006 - val_accuracy: 0.3402\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0981 - accuracy: 0.3428 - val_loss: 2.1000 - val_accuracy: 0.3419\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0978 - accuracy: 0.3422 - val_loss: 2.1002 - val_accuracy: 0.3400\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0976 - accuracy: 0.3428 - val_loss: 2.1001 - val_accuracy: 0.3394\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0973 - accuracy: 0.3436 - val_loss: 2.0998 - val_accuracy: 0.3439\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0972 - accuracy: 0.3430 - val_loss: 2.0996 - val_accuracy: 0.3335\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0971 - accuracy: 0.3429 - val_loss: 2.1005 - val_accuracy: 0.3329\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0968 - accuracy: 0.3422 - val_loss: 2.0997 - val_accuracy: 0.3342\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0966 - accuracy: 0.3434 - val_loss: 2.0989 - val_accuracy: 0.3409\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0963 - accuracy: 0.3444 - val_loss: 2.0990 - val_accuracy: 0.3405\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0964 - accuracy: 0.3430 - val_loss: 2.0984 - val_accuracy: 0.3413\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0962 - accuracy: 0.3432 - val_loss: 2.0989 - val_accuracy: 0.3444\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0960 - accuracy: 0.3448 - val_loss: 2.0985 - val_accuracy: 0.3405\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0957 - accuracy: 0.3448 - val_loss: 2.0979 - val_accuracy: 0.3403\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0956 - accuracy: 0.3450 - val_loss: 2.0982 - val_accuracy: 0.3434\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0955 - accuracy: 0.3450 - val_loss: 2.0985 - val_accuracy: 0.3420\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0953 - accuracy: 0.3448 - val_loss: 2.0978 - val_accuracy: 0.3333\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0952 - accuracy: 0.3454 - val_loss: 2.0975 - val_accuracy: 0.3416\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0951 - accuracy: 0.3441 - val_loss: 2.0975 - val_accuracy: 0.3454\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0950 - accuracy: 0.3444 - val_loss: 2.0970 - val_accuracy: 0.3442\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0948 - accuracy: 0.3450 - val_loss: 2.0973 - val_accuracy: 0.3350\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0948 - accuracy: 0.3453 - val_loss: 2.0972 - val_accuracy: 0.3451\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0946 - accuracy: 0.3466 - val_loss: 2.0979 - val_accuracy: 0.3338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [17:33, 150.46s/it]\u001b[A\n",
      " 30%|███       | 3/10 [54:35<2:09:00, 1105.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5735 - accuracy: 0.9013 - val_loss: 1.5215 - val_accuracy: 0.9427\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 3\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5126 - accuracy: 0.9513 - val_loss: 1.5053 - val_accuracy: 0.9567\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4987 - accuracy: 0.9643 - val_loss: 1.5018 - val_accuracy: 0.9596\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4914 - accuracy: 0.9711 - val_loss: 1.4998 - val_accuracy: 0.9626\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4870 - accuracy: 0.9753 - val_loss: 1.4945 - val_accuracy: 0.9672\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4843 - accuracy: 0.9776 - val_loss: 1.4891 - val_accuracy: 0.9728\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4818 - accuracy: 0.9802 - val_loss: 1.4880 - val_accuracy: 0.9730\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4800 - accuracy: 0.9818 - val_loss: 1.4893 - val_accuracy: 0.9719\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4786 - accuracy: 0.9829 - val_loss: 1.4840 - val_accuracy: 0.9769\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4769 - accuracy: 0.9846 - val_loss: 1.4868 - val_accuracy: 0.9744\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9858 - val_loss: 1.4828 - val_accuracy: 0.9788\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4745 - accuracy: 0.9869 - val_loss: 1.4826 - val_accuracy: 0.9790\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4744 - accuracy: 0.9870 - val_loss: 1.4837 - val_accuracy: 0.9777\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4743 - accuracy: 0.9872 - val_loss: 1.4841 - val_accuracy: 0.9769\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9883 - val_loss: 1.4822 - val_accuracy: 0.9793\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4732 - accuracy: 0.9881 - val_loss: 1.4837 - val_accuracy: 0.9775\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4716 - accuracy: 0.9897 - val_loss: 1.4837 - val_accuracy: 0.9772\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4714 - accuracy: 0.9898 - val_loss: 1.4812 - val_accuracy: 0.9798\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.9894 - val_loss: 1.4823 - val_accuracy: 0.9791\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4717 - accuracy: 0.9895 - val_loss: 1.4827 - val_accuracy: 0.9784\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9909 - val_loss: 1.4813 - val_accuracy: 0.9798\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [03:44, 224.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 3\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4859 - accuracy: 0.9771 - val_loss: 1.4891 - val_accuracy: 0.9725\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9866 - val_loss: 1.4874 - val_accuracy: 0.9744\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4734 - accuracy: 0.9885 - val_loss: 1.4854 - val_accuracy: 0.9760\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.9901 - val_loss: 1.4851 - val_accuracy: 0.9763\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4704 - accuracy: 0.9912 - val_loss: 1.4861 - val_accuracy: 0.9755\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4700 - accuracy: 0.9919 - val_loss: 1.4854 - val_accuracy: 0.9759\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4693 - accuracy: 0.9923 - val_loss: 1.4853 - val_accuracy: 0.9763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [05:32, 189.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 3\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5448 - accuracy: 0.9321 - val_loss: 1.5213 - val_accuracy: 0.9462\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5103 - accuracy: 0.9569 - val_loss: 1.5153 - val_accuracy: 0.9501\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5024 - accuracy: 0.9634 - val_loss: 1.5117 - val_accuracy: 0.9527\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4973 - accuracy: 0.9678 - val_loss: 1.5086 - val_accuracy: 0.9554\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4939 - accuracy: 0.9711 - val_loss: 1.5068 - val_accuracy: 0.9566\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4911 - accuracy: 0.9735 - val_loss: 1.5065 - val_accuracy: 0.9567\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4891 - accuracy: 0.9750 - val_loss: 1.5047 - val_accuracy: 0.9573\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4876 - accuracy: 0.9762 - val_loss: 1.5037 - val_accuracy: 0.9588\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4857 - accuracy: 0.9783 - val_loss: 1.5038 - val_accuracy: 0.9582\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4848 - accuracy: 0.9787 - val_loss: 1.5033 - val_accuracy: 0.9581\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9798 - val_loss: 1.5028 - val_accuracy: 0.9594\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4827 - accuracy: 0.9805 - val_loss: 1.5039 - val_accuracy: 0.9582\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4817 - accuracy: 0.9811 - val_loss: 1.5027 - val_accuracy: 0.9601\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4810 - accuracy: 0.9820 - val_loss: 1.5027 - val_accuracy: 0.9592\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9830 - val_loss: 1.5033 - val_accuracy: 0.9575\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4799 - accuracy: 0.9828 - val_loss: 1.5017 - val_accuracy: 0.9605\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4791 - accuracy: 0.9836 - val_loss: 1.5025 - val_accuracy: 0.9587\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4781 - accuracy: 0.9844 - val_loss: 1.5044 - val_accuracy: 0.9565\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4778 - accuracy: 0.9847 - val_loss: 1.5015 - val_accuracy: 0.9601\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4776 - accuracy: 0.9846 - val_loss: 1.5018 - val_accuracy: 0.9607\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4769 - accuracy: 0.9854 - val_loss: 1.5024 - val_accuracy: 0.9594\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4767 - accuracy: 0.9857 - val_loss: 1.5015 - val_accuracy: 0.9606\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4763 - accuracy: 0.9860 - val_loss: 1.5022 - val_accuracy: 0.9594\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9865 - val_loss: 1.5016 - val_accuracy: 0.9601\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9869 - val_loss: 1.5029 - val_accuracy: 0.9585\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [08:05, 178.47s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 3\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7052 - accuracy: 0.7756 - val_loss: 1.6207 - val_accuracy: 0.8505\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6053 - accuracy: 0.8655 - val_loss: 1.5995 - val_accuracy: 0.8704\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5908 - accuracy: 0.8773 - val_loss: 1.5911 - val_accuracy: 0.8760\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5824 - accuracy: 0.8850 - val_loss: 1.5849 - val_accuracy: 0.8802\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5766 - accuracy: 0.8902 - val_loss: 1.5809 - val_accuracy: 0.8835\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5723 - accuracy: 0.8942 - val_loss: 1.5777 - val_accuracy: 0.8878\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5689 - accuracy: 0.8972 - val_loss: 1.5744 - val_accuracy: 0.8900\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5655 - accuracy: 0.9010 - val_loss: 1.5731 - val_accuracy: 0.8904\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5634 - accuracy: 0.9027 - val_loss: 1.5723 - val_accuracy: 0.8914\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5608 - accuracy: 0.9052 - val_loss: 1.5704 - val_accuracy: 0.8929\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5589 - accuracy: 0.9071 - val_loss: 1.5713 - val_accuracy: 0.8923\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5572 - accuracy: 0.9087 - val_loss: 1.5698 - val_accuracy: 0.8936\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5558 - accuracy: 0.9099 - val_loss: 1.5715 - val_accuracy: 0.8900\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5545 - accuracy: 0.9113 - val_loss: 1.5683 - val_accuracy: 0.8955\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5533 - accuracy: 0.9125 - val_loss: 1.5688 - val_accuracy: 0.8937\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5519 - accuracy: 0.9140 - val_loss: 1.5691 - val_accuracy: 0.8935\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5513 - accuracy: 0.9141 - val_loss: 1.5670 - val_accuracy: 0.8965\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5504 - accuracy: 0.9145 - val_loss: 1.5694 - val_accuracy: 0.8930\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5494 - accuracy: 0.9154 - val_loss: 1.5678 - val_accuracy: 0.8942\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5485 - accuracy: 0.9168 - val_loss: 1.5669 - val_accuracy: 0.8956\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5477 - accuracy: 0.9172 - val_loss: 1.5671 - val_accuracy: 0.8953\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5472 - accuracy: 0.9178 - val_loss: 1.5675 - val_accuracy: 0.8948\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5464 - accuracy: 0.9188 - val_loss: 1.5657 - val_accuracy: 0.8956\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5457 - accuracy: 0.9193 - val_loss: 1.5653 - val_accuracy: 0.8981\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5452 - accuracy: 0.9194 - val_loss: 1.5656 - val_accuracy: 0.8949\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5445 - accuracy: 0.9203 - val_loss: 1.5658 - val_accuracy: 0.8960\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5440 - accuracy: 0.9207 - val_loss: 1.5651 - val_accuracy: 0.8977\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5430 - accuracy: 0.9220 - val_loss: 1.5645 - val_accuracy: 0.8966\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5429 - accuracy: 0.9216 - val_loss: 1.5650 - val_accuracy: 0.8975\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5423 - accuracy: 0.9223 - val_loss: 1.5639 - val_accuracy: 0.8976\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5418 - accuracy: 0.9224 - val_loss: 1.5645 - val_accuracy: 0.8976\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5414 - accuracy: 0.9229 - val_loss: 1.5649 - val_accuracy: 0.8967\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5408 - accuracy: 0.9239 - val_loss: 1.5648 - val_accuracy: 0.8971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [10:51, 174.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 3\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0130 - accuracy: 0.4557 - val_loss: 1.9163 - val_accuracy: 0.5555\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8960 - accuracy: 0.5734 - val_loss: 1.8777 - val_accuracy: 0.5866\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8687 - accuracy: 0.5984 - val_loss: 1.8606 - val_accuracy: 0.6029\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8589 - accuracy: 0.6065 - val_loss: 1.8537 - val_accuracy: 0.6114\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8538 - accuracy: 0.6110 - val_loss: 1.8514 - val_accuracy: 0.6108\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8498 - accuracy: 0.6145 - val_loss: 1.8478 - val_accuracy: 0.6129\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8466 - accuracy: 0.6169 - val_loss: 1.8448 - val_accuracy: 0.6168\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8439 - accuracy: 0.6191 - val_loss: 1.8412 - val_accuracy: 0.6217\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8417 - accuracy: 0.6208 - val_loss: 1.8391 - val_accuracy: 0.6233\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8397 - accuracy: 0.6218 - val_loss: 1.8395 - val_accuracy: 0.6215\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8379 - accuracy: 0.6243 - val_loss: 1.8367 - val_accuracy: 0.6240\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8367 - accuracy: 0.6247 - val_loss: 1.8364 - val_accuracy: 0.6240\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8351 - accuracy: 0.6272 - val_loss: 1.8350 - val_accuracy: 0.6236\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8339 - accuracy: 0.6278 - val_loss: 1.8343 - val_accuracy: 0.6254\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8330 - accuracy: 0.6285 - val_loss: 1.8318 - val_accuracy: 0.6279\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8320 - accuracy: 0.6290 - val_loss: 1.8315 - val_accuracy: 0.6289\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8309 - accuracy: 0.6299 - val_loss: 1.8318 - val_accuracy: 0.6262\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8300 - accuracy: 0.6312 - val_loss: 1.8313 - val_accuracy: 0.6276\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8290 - accuracy: 0.6321 - val_loss: 1.8300 - val_accuracy: 0.6288\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8282 - accuracy: 0.6325 - val_loss: 1.8285 - val_accuracy: 0.6286\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8269 - accuracy: 0.6333 - val_loss: 1.8262 - val_accuracy: 0.6311\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8176 - accuracy: 0.6449 - val_loss: 1.8142 - val_accuracy: 0.6507\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8136 - accuracy: 0.6487 - val_loss: 1.8113 - val_accuracy: 0.6498\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8118 - accuracy: 0.6502 - val_loss: 1.8103 - val_accuracy: 0.6522\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8097 - accuracy: 0.6525 - val_loss: 1.8090 - val_accuracy: 0.6542\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8075 - accuracy: 0.6548 - val_loss: 1.8065 - val_accuracy: 0.6552\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8058 - accuracy: 0.6566 - val_loss: 1.8059 - val_accuracy: 0.6553\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8045 - accuracy: 0.6574 - val_loss: 1.8053 - val_accuracy: 0.6550\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8036 - accuracy: 0.6585 - val_loss: 1.8057 - val_accuracy: 0.6548\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8026 - accuracy: 0.6592 - val_loss: 1.8032 - val_accuracy: 0.6597\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8016 - accuracy: 0.6603 - val_loss: 1.8040 - val_accuracy: 0.6578\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8009 - accuracy: 0.6608 - val_loss: 1.8019 - val_accuracy: 0.6599\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8003 - accuracy: 0.6621 - val_loss: 1.8033 - val_accuracy: 0.6584\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7998 - accuracy: 0.6617 - val_loss: 1.8013 - val_accuracy: 0.6592\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7988 - accuracy: 0.6628 - val_loss: 1.8019 - val_accuracy: 0.6599\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7985 - accuracy: 0.6630 - val_loss: 1.8014 - val_accuracy: 0.6588\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7980 - accuracy: 0.6639 - val_loss: 1.8027 - val_accuracy: 0.6575\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [13:50, 175.92s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 3\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2350 - accuracy: 0.2138 - val_loss: 2.1710 - val_accuracy: 0.2790\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1683 - accuracy: 0.2766 - val_loss: 2.1576 - val_accuracy: 0.2867\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1516 - accuracy: 0.3099 - val_loss: 2.1376 - val_accuracy: 0.3320\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1333 - accuracy: 0.3289 - val_loss: 2.1220 - val_accuracy: 0.3415\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1191 - accuracy: 0.3404 - val_loss: 2.1111 - val_accuracy: 0.3485\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1105 - accuracy: 0.3457 - val_loss: 2.1054 - val_accuracy: 0.3516\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0974 - accuracy: 0.3628 - val_loss: 2.0782 - val_accuracy: 0.3880\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0751 - accuracy: 0.3910 - val_loss: 2.0703 - val_accuracy: 0.3956\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0693 - accuracy: 0.3945 - val_loss: 2.0668 - val_accuracy: 0.4001\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0659 - accuracy: 0.3976 - val_loss: 2.0642 - val_accuracy: 0.3997\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0635 - accuracy: 0.3993 - val_loss: 2.0619 - val_accuracy: 0.4013\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0616 - accuracy: 0.4011 - val_loss: 2.0608 - val_accuracy: 0.4033\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0601 - accuracy: 0.4019 - val_loss: 2.0599 - val_accuracy: 0.4021\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0588 - accuracy: 0.4031 - val_loss: 2.0595 - val_accuracy: 0.4033\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0580 - accuracy: 0.4038 - val_loss: 2.0581 - val_accuracy: 0.4041\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0572 - accuracy: 0.4043 - val_loss: 2.0584 - val_accuracy: 0.4026\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0565 - accuracy: 0.4049 - val_loss: 2.0571 - val_accuracy: 0.4050\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0558 - accuracy: 0.4056 - val_loss: 2.0564 - val_accuracy: 0.4051\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0552 - accuracy: 0.4060 - val_loss: 2.0568 - val_accuracy: 0.4046\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0545 - accuracy: 0.4068 - val_loss: 2.0563 - val_accuracy: 0.4038\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0541 - accuracy: 0.4071 - val_loss: 2.0558 - val_accuracy: 0.4059\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0535 - accuracy: 0.4076 - val_loss: 2.0557 - val_accuracy: 0.4056\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0531 - accuracy: 0.4078 - val_loss: 2.0554 - val_accuracy: 0.4054\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0528 - accuracy: 0.4078 - val_loss: 2.0552 - val_accuracy: 0.4063\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0524 - accuracy: 0.4083 - val_loss: 2.0542 - val_accuracy: 0.4065\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0521 - accuracy: 0.4088 - val_loss: 2.0546 - val_accuracy: 0.4050\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0517 - accuracy: 0.4085 - val_loss: 2.0542 - val_accuracy: 0.4054\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0514 - accuracy: 0.4091 - val_loss: 2.0541 - val_accuracy: 0.4052\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0513 - accuracy: 0.4093 - val_loss: 2.0534 - val_accuracy: 0.4061\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0510 - accuracy: 0.4090 - val_loss: 2.0536 - val_accuracy: 0.4061\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0509 - accuracy: 0.4090 - val_loss: 2.0527 - val_accuracy: 0.4078\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0505 - accuracy: 0.4094 - val_loss: 2.0526 - val_accuracy: 0.4075\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0504 - accuracy: 0.4099 - val_loss: 2.0526 - val_accuracy: 0.4072\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0502 - accuracy: 0.4097 - val_loss: 2.0520 - val_accuracy: 0.4078\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0500 - accuracy: 0.4101 - val_loss: 2.0522 - val_accuracy: 0.4074\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0499 - accuracy: 0.4098 - val_loss: 2.0518 - val_accuracy: 0.4081\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0499 - accuracy: 0.4099 - val_loss: 2.0517 - val_accuracy: 0.4079\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0497 - accuracy: 0.4103 - val_loss: 2.0521 - val_accuracy: 0.4077\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0495 - accuracy: 0.4103 - val_loss: 2.0514 - val_accuracy: 0.4084\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0494 - accuracy: 0.4102 - val_loss: 2.0517 - val_accuracy: 0.4083\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0493 - accuracy: 0.4105 - val_loss: 2.0511 - val_accuracy: 0.4088\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0492 - accuracy: 0.4106 - val_loss: 2.0507 - val_accuracy: 0.4085\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0489 - accuracy: 0.4110 - val_loss: 2.0514 - val_accuracy: 0.4075\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0487 - accuracy: 0.4110 - val_loss: 2.0506 - val_accuracy: 0.4087\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0487 - accuracy: 0.4110 - val_loss: 2.0505 - val_accuracy: 0.4096\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0487 - accuracy: 0.4107 - val_loss: 2.0506 - val_accuracy: 0.4081\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0486 - accuracy: 0.4112 - val_loss: 2.0505 - val_accuracy: 0.4086\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0485 - accuracy: 0.4115 - val_loss: 2.0505 - val_accuracy: 0.4088\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0484 - accuracy: 0.4113 - val_loss: 2.0500 - val_accuracy: 0.4095\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0484 - accuracy: 0.4114 - val_loss: 2.0498 - val_accuracy: 0.4099\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0482 - accuracy: 0.4117 - val_loss: 2.0503 - val_accuracy: 0.4094\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0480 - accuracy: 0.4115 - val_loss: 2.0515 - val_accuracy: 0.4084\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0480 - accuracy: 0.4115 - val_loss: 2.0504 - val_accuracy: 0.4097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [17:04, 181.31s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 3\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2555 - accuracy: 0.1856 - val_loss: 2.2526 - val_accuracy: 0.1867\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2492 - accuracy: 0.1871 - val_loss: 2.2449 - val_accuracy: 0.1880\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2380 - accuracy: 0.2035 - val_loss: 2.2356 - val_accuracy: 0.2060\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2301 - accuracy: 0.2098 - val_loss: 2.2274 - val_accuracy: 0.2121\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2245 - accuracy: 0.2138 - val_loss: 2.2208 - val_accuracy: 0.2181\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2189 - accuracy: 0.2226 - val_loss: 2.2169 - val_accuracy: 0.2235\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2169 - accuracy: 0.2234 - val_loss: 2.2159 - val_accuracy: 0.2244\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2129 - accuracy: 0.2267 - val_loss: 2.2041 - val_accuracy: 0.2355\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2063 - accuracy: 0.2331 - val_loss: 2.2035 - val_accuracy: 0.2355\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2053 - accuracy: 0.2329 - val_loss: 2.2025 - val_accuracy: 0.2353\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2042 - accuracy: 0.2337 - val_loss: 2.2016 - val_accuracy: 0.2355\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2036 - accuracy: 0.2342 - val_loss: 2.2009 - val_accuracy: 0.2358\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2027 - accuracy: 0.2351 - val_loss: 2.2004 - val_accuracy: 0.2362\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2023 - accuracy: 0.2349 - val_loss: 2.1999 - val_accuracy: 0.2373\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2019 - accuracy: 0.2357 - val_loss: 2.2000 - val_accuracy: 0.2368\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2016 - accuracy: 0.2365 - val_loss: 2.1995 - val_accuracy: 0.2377\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2014 - accuracy: 0.2361 - val_loss: 2.1991 - val_accuracy: 0.2378\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2011 - accuracy: 0.2362 - val_loss: 2.1994 - val_accuracy: 0.2386\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2010 - accuracy: 0.2360 - val_loss: 2.1989 - val_accuracy: 0.2384\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2007 - accuracy: 0.2372 - val_loss: 2.1987 - val_accuracy: 0.2387\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2005 - accuracy: 0.2376 - val_loss: 2.1984 - val_accuracy: 0.2384\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2003 - accuracy: 0.2377 - val_loss: 2.1985 - val_accuracy: 0.2388\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2002 - accuracy: 0.2375 - val_loss: 2.1981 - val_accuracy: 0.2383\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2000 - accuracy: 0.2377 - val_loss: 2.1982 - val_accuracy: 0.2382\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1999 - accuracy: 0.2379 - val_loss: 2.1979 - val_accuracy: 0.2391\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1996 - accuracy: 0.2378 - val_loss: 2.1980 - val_accuracy: 0.2385\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1996 - accuracy: 0.2380 - val_loss: 2.1986 - val_accuracy: 0.2382\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1995 - accuracy: 0.2383 - val_loss: 2.1977 - val_accuracy: 0.2385\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1994 - accuracy: 0.2382 - val_loss: 2.1977 - val_accuracy: 0.2388\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1993 - accuracy: 0.2386 - val_loss: 2.1975 - val_accuracy: 0.2391\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1990 - accuracy: 0.2379 - val_loss: 2.1975 - val_accuracy: 0.2379\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1991 - accuracy: 0.2385 - val_loss: 2.1973 - val_accuracy: 0.2387\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1989 - accuracy: 0.2386 - val_loss: 2.1973 - val_accuracy: 0.2389\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1989 - accuracy: 0.2385 - val_loss: 2.1971 - val_accuracy: 0.2388\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1988 - accuracy: 0.2384 - val_loss: 2.1970 - val_accuracy: 0.2393\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1987 - accuracy: 0.2384 - val_loss: 2.1968 - val_accuracy: 0.2394\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1985 - accuracy: 0.2388 - val_loss: 2.1967 - val_accuracy: 0.2394\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1983 - accuracy: 0.2394 - val_loss: 2.1963 - val_accuracy: 0.2394\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1980 - accuracy: 0.2398 - val_loss: 2.1962 - val_accuracy: 0.2408\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1976 - accuracy: 0.2402 - val_loss: 2.1961 - val_accuracy: 0.2409\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1974 - accuracy: 0.2406 - val_loss: 2.1958 - val_accuracy: 0.2407\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1920 - accuracy: 0.2465 - val_loss: 2.1857 - val_accuracy: 0.2533\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1856 - accuracy: 0.2539 - val_loss: 2.1838 - val_accuracy: 0.2550\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1845 - accuracy: 0.2539 - val_loss: 2.1831 - val_accuracy: 0.2546\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1841 - accuracy: 0.2546 - val_loss: 2.1827 - val_accuracy: 0.2545\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1838 - accuracy: 0.2545 - val_loss: 2.1827 - val_accuracy: 0.2552\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1835 - accuracy: 0.2547 - val_loss: 2.1820 - val_accuracy: 0.2552\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1834 - accuracy: 0.2543 - val_loss: 2.1817 - val_accuracy: 0.2554\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1832 - accuracy: 0.2548 - val_loss: 2.1816 - val_accuracy: 0.2557\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1830 - accuracy: 0.2548 - val_loss: 2.1811 - val_accuracy: 0.2559\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1827 - accuracy: 0.2549 - val_loss: 2.1806 - val_accuracy: 0.2569\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1824 - accuracy: 0.2554 - val_loss: 2.1799 - val_accuracy: 0.2579\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1821 - accuracy: 0.2552 - val_loss: 2.1798 - val_accuracy: 0.2580\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1821 - accuracy: 0.2550 - val_loss: 2.1798 - val_accuracy: 0.2576\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1817 - accuracy: 0.2555 - val_loss: 2.1793 - val_accuracy: 0.2573\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1815 - accuracy: 0.2558 - val_loss: 2.1790 - val_accuracy: 0.2572\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1813 - accuracy: 0.2553 - val_loss: 2.1791 - val_accuracy: 0.2572\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1813 - accuracy: 0.2563 - val_loss: 2.1790 - val_accuracy: 0.2573\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1812 - accuracy: 0.2559 - val_loss: 2.1789 - val_accuracy: 0.2578\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1811 - accuracy: 0.2561 - val_loss: 2.1787 - val_accuracy: 0.2580\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1810 - accuracy: 0.2561 - val_loss: 2.1790 - val_accuracy: 0.2571\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1808 - accuracy: 0.2563 - val_loss: 2.1786 - val_accuracy: 0.2574\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1809 - accuracy: 0.2562 - val_loss: 2.1785 - val_accuracy: 0.2576\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1808 - accuracy: 0.2561 - val_loss: 2.1784 - val_accuracy: 0.2578\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1807 - accuracy: 0.2558 - val_loss: 2.1783 - val_accuracy: 0.2575\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1807 - accuracy: 0.2565 - val_loss: 2.1786 - val_accuracy: 0.2578\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1805 - accuracy: 0.2564 - val_loss: 2.1783 - val_accuracy: 0.2577\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1805 - accuracy: 0.2562 - val_loss: 2.1782 - val_accuracy: 0.2578\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1805 - accuracy: 0.2559 - val_loss: 2.1781 - val_accuracy: 0.2579\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1803 - accuracy: 0.2566 - val_loss: 2.1778 - val_accuracy: 0.2584\n",
      "Epoch 71/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1803 - accuracy: 0.2565 - val_loss: 2.1781 - val_accuracy: 0.2584\n",
      "Epoch 72/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1802 - accuracy: 0.2565 - val_loss: 2.1786 - val_accuracy: 0.2577\n",
      "Epoch 73/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1802 - accuracy: 0.2559 - val_loss: 2.1778 - val_accuracy: 0.2579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [20:27, 175.32s/it]\u001b[A\n",
      " 40%|████      | 4/10 [1:15:05<1:54:17, 1142.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5735 - accuracy: 0.9010 - val_loss: 1.5209 - val_accuracy: 0.9452\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 4\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5103 - accuracy: 0.9534 - val_loss: 1.5029 - val_accuracy: 0.9596\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4973 - accuracy: 0.9652 - val_loss: 1.4990 - val_accuracy: 0.9637\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4899 - accuracy: 0.9726 - val_loss: 1.4946 - val_accuracy: 0.9674\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4862 - accuracy: 0.9758 - val_loss: 1.4916 - val_accuracy: 0.9696\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.9786 - val_loss: 1.4916 - val_accuracy: 0.9707\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4807 - accuracy: 0.9810 - val_loss: 1.4872 - val_accuracy: 0.9748\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4787 - accuracy: 0.9829 - val_loss: 1.4882 - val_accuracy: 0.9723\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4773 - accuracy: 0.9845 - val_loss: 1.4861 - val_accuracy: 0.9753\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4769 - accuracy: 0.9845 - val_loss: 1.4856 - val_accuracy: 0.9760\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9859 - val_loss: 1.4872 - val_accuracy: 0.9740\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9883 - val_loss: 1.4923 - val_accuracy: 0.9684\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4740 - accuracy: 0.9875 - val_loss: 1.4840 - val_accuracy: 0.9769\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4728 - accuracy: 0.9885 - val_loss: 1.4847 - val_accuracy: 0.9761\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9883 - val_loss: 1.4846 - val_accuracy: 0.9770\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9882 - val_loss: 1.4846 - val_accuracy: 0.9770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:44, 164.28s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 4\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4890 - accuracy: 0.9744 - val_loss: 1.4930 - val_accuracy: 0.9688\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4780 - accuracy: 0.9843 - val_loss: 1.4906 - val_accuracy: 0.9709\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4749 - accuracy: 0.9869 - val_loss: 1.4882 - val_accuracy: 0.9746\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9891 - val_loss: 1.4865 - val_accuracy: 0.9753\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4721 - accuracy: 0.9899 - val_loss: 1.4848 - val_accuracy: 0.9768\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4709 - accuracy: 0.9910 - val_loss: 1.4854 - val_accuracy: 0.9768\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4706 - accuracy: 0.9911 - val_loss: 1.4858 - val_accuracy: 0.9756\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4698 - accuracy: 0.9919 - val_loss: 1.4860 - val_accuracy: 0.9755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:48, 152.29s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 4\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5574 - accuracy: 0.9202 - val_loss: 1.5226 - val_accuracy: 0.9433\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5114 - accuracy: 0.9556 - val_loss: 1.5138 - val_accuracy: 0.9517\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5029 - accuracy: 0.9630 - val_loss: 1.5098 - val_accuracy: 0.9551\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4978 - accuracy: 0.9672 - val_loss: 1.5065 - val_accuracy: 0.9577\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4944 - accuracy: 0.9703 - val_loss: 1.5067 - val_accuracy: 0.9571\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4913 - accuracy: 0.9735 - val_loss: 1.5036 - val_accuracy: 0.9592\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4892 - accuracy: 0.9750 - val_loss: 1.5034 - val_accuracy: 0.9591\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4873 - accuracy: 0.9768 - val_loss: 1.5033 - val_accuracy: 0.9594\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4855 - accuracy: 0.9783 - val_loss: 1.5012 - val_accuracy: 0.9615\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4844 - accuracy: 0.9790 - val_loss: 1.5020 - val_accuracy: 0.9607\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.9805 - val_loss: 1.5016 - val_accuracy: 0.9602\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4821 - accuracy: 0.9811 - val_loss: 1.5017 - val_accuracy: 0.9601\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:54, 144.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 4\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7056 - accuracy: 0.7763 - val_loss: 1.5992 - val_accuracy: 0.8745\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5893 - accuracy: 0.8817 - val_loss: 1.5821 - val_accuracy: 0.8859\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5757 - accuracy: 0.8929 - val_loss: 1.5746 - val_accuracy: 0.8923\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5685 - accuracy: 0.8999 - val_loss: 1.5712 - val_accuracy: 0.8944\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5637 - accuracy: 0.9037 - val_loss: 1.5656 - val_accuracy: 0.8997\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5601 - accuracy: 0.9068 - val_loss: 1.5626 - val_accuracy: 0.9030\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5572 - accuracy: 0.9097 - val_loss: 1.5623 - val_accuracy: 0.9026\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5547 - accuracy: 0.9110 - val_loss: 1.5591 - val_accuracy: 0.9045\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5522 - accuracy: 0.9140 - val_loss: 1.5576 - val_accuracy: 0.9057\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5501 - accuracy: 0.9154 - val_loss: 1.5566 - val_accuracy: 0.9067\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5485 - accuracy: 0.9173 - val_loss: 1.5574 - val_accuracy: 0.9067\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5469 - accuracy: 0.9188 - val_loss: 1.5556 - val_accuracy: 0.9088\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5452 - accuracy: 0.9211 - val_loss: 1.5555 - val_accuracy: 0.9076\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5439 - accuracy: 0.9215 - val_loss: 1.5539 - val_accuracy: 0.9108\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5428 - accuracy: 0.9226 - val_loss: 1.5546 - val_accuracy: 0.9095\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5419 - accuracy: 0.9235 - val_loss: 1.5534 - val_accuracy: 0.9096\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5408 - accuracy: 0.9243 - val_loss: 1.5522 - val_accuracy: 0.9102\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5400 - accuracy: 0.9254 - val_loss: 1.5521 - val_accuracy: 0.9102\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5389 - accuracy: 0.9264 - val_loss: 1.5524 - val_accuracy: 0.9103\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5382 - accuracy: 0.9265 - val_loss: 1.5511 - val_accuracy: 0.9116\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5373 - accuracy: 0.9275 - val_loss: 1.5518 - val_accuracy: 0.9104\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5365 - accuracy: 0.9287 - val_loss: 1.5508 - val_accuracy: 0.9112\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5359 - accuracy: 0.9291 - val_loss: 1.5510 - val_accuracy: 0.9113\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5353 - accuracy: 0.9294 - val_loss: 1.5506 - val_accuracy: 0.9116\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5348 - accuracy: 0.9302 - val_loss: 1.5505 - val_accuracy: 0.9117\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5340 - accuracy: 0.9306 - val_loss: 1.5496 - val_accuracy: 0.9119\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5335 - accuracy: 0.9308 - val_loss: 1.5513 - val_accuracy: 0.9102\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5332 - accuracy: 0.9314 - val_loss: 1.5500 - val_accuracy: 0.9113\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5325 - accuracy: 0.9315 - val_loss: 1.5522 - val_accuracy: 0.9095\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:56, 155.72s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 4\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0819 - accuracy: 0.3798 - val_loss: 2.0023 - val_accuracy: 0.4683\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9637 - accuracy: 0.5048 - val_loss: 1.9385 - val_accuracy: 0.5298\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9270 - accuracy: 0.5372 - val_loss: 1.9179 - val_accuracy: 0.5475\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9133 - accuracy: 0.5497 - val_loss: 1.9078 - val_accuracy: 0.5542\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9045 - accuracy: 0.5575 - val_loss: 1.9023 - val_accuracy: 0.5563\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8984 - accuracy: 0.5630 - val_loss: 1.8950 - val_accuracy: 0.5664\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8933 - accuracy: 0.5678 - val_loss: 1.8909 - val_accuracy: 0.5727\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8892 - accuracy: 0.5717 - val_loss: 1.8870 - val_accuracy: 0.5736\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8864 - accuracy: 0.5733 - val_loss: 1.8843 - val_accuracy: 0.5752\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8835 - accuracy: 0.5768 - val_loss: 1.8830 - val_accuracy: 0.5768\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8815 - accuracy: 0.5785 - val_loss: 1.8812 - val_accuracy: 0.5791\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8796 - accuracy: 0.5802 - val_loss: 1.8784 - val_accuracy: 0.5834\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8782 - accuracy: 0.5818 - val_loss: 1.8774 - val_accuracy: 0.5841\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8769 - accuracy: 0.5824 - val_loss: 1.8765 - val_accuracy: 0.5816\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8753 - accuracy: 0.5833 - val_loss: 1.8750 - val_accuracy: 0.5843\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8739 - accuracy: 0.5857 - val_loss: 1.8752 - val_accuracy: 0.5827\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8731 - accuracy: 0.5858 - val_loss: 1.8727 - val_accuracy: 0.5891\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8722 - accuracy: 0.5871 - val_loss: 1.8735 - val_accuracy: 0.5858\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8709 - accuracy: 0.5885 - val_loss: 1.8735 - val_accuracy: 0.5858\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8701 - accuracy: 0.5892 - val_loss: 1.8707 - val_accuracy: 0.5864\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8693 - accuracy: 0.5900 - val_loss: 1.8700 - val_accuracy: 0.5909\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8687 - accuracy: 0.5893 - val_loss: 1.8697 - val_accuracy: 0.5915\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8682 - accuracy: 0.5906 - val_loss: 1.8710 - val_accuracy: 0.5883\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8675 - accuracy: 0.5920 - val_loss: 1.8693 - val_accuracy: 0.5890\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8667 - accuracy: 0.5925 - val_loss: 1.8694 - val_accuracy: 0.5884\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8664 - accuracy: 0.5922 - val_loss: 1.8679 - val_accuracy: 0.5903\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8659 - accuracy: 0.5921 - val_loss: 1.8674 - val_accuracy: 0.5912\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8653 - accuracy: 0.5933 - val_loss: 1.8673 - val_accuracy: 0.5913\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8644 - accuracy: 0.5943 - val_loss: 1.8678 - val_accuracy: 0.5891\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8641 - accuracy: 0.5940 - val_loss: 1.8665 - val_accuracy: 0.5918\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8636 - accuracy: 0.5942 - val_loss: 1.8662 - val_accuracy: 0.5927\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8635 - accuracy: 0.5943 - val_loss: 1.8675 - val_accuracy: 0.5887\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8630 - accuracy: 0.5950 - val_loss: 1.8696 - val_accuracy: 0.5863\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8625 - accuracy: 0.5946 - val_loss: 1.8653 - val_accuracy: 0.5941\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8622 - accuracy: 0.5962 - val_loss: 1.8647 - val_accuracy: 0.5944\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8616 - accuracy: 0.5971 - val_loss: 1.8664 - val_accuracy: 0.5907\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8613 - accuracy: 0.5970 - val_loss: 1.8638 - val_accuracy: 0.5938\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8605 - accuracy: 0.5985 - val_loss: 1.8654 - val_accuracy: 0.5919\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8602 - accuracy: 0.5982 - val_loss: 1.8638 - val_accuracy: 0.5950\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8603 - accuracy: 0.5974 - val_loss: 1.8639 - val_accuracy: 0.5920\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8592 - accuracy: 0.5990 - val_loss: 1.8642 - val_accuracy: 0.5924\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8594 - accuracy: 0.5997 - val_loss: 1.8648 - val_accuracy: 0.5929\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:57, 163.22s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 4\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1568 - accuracy: 0.2964 - val_loss: 2.1235 - val_accuracy: 0.3316\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0901 - accuracy: 0.3803 - val_loss: 2.0554 - val_accuracy: 0.4334\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0271 - accuracy: 0.4543 - val_loss: 2.0141 - val_accuracy: 0.4589\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0022 - accuracy: 0.4674 - val_loss: 2.0013 - val_accuracy: 0.4649\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9940 - accuracy: 0.4708 - val_loss: 1.9959 - val_accuracy: 0.4680\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9897 - accuracy: 0.4742 - val_loss: 1.9927 - val_accuracy: 0.4707\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9868 - accuracy: 0.4766 - val_loss: 1.9895 - val_accuracy: 0.4741\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9846 - accuracy: 0.4782 - val_loss: 1.9871 - val_accuracy: 0.4761\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9827 - accuracy: 0.4807 - val_loss: 1.9851 - val_accuracy: 0.4784\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9810 - accuracy: 0.4813 - val_loss: 1.9834 - val_accuracy: 0.4805\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9797 - accuracy: 0.4829 - val_loss: 1.9826 - val_accuracy: 0.4794\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9788 - accuracy: 0.4841 - val_loss: 1.9818 - val_accuracy: 0.4797\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9778 - accuracy: 0.4841 - val_loss: 1.9806 - val_accuracy: 0.4813\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9772 - accuracy: 0.4850 - val_loss: 1.9796 - val_accuracy: 0.4808\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9765 - accuracy: 0.4853 - val_loss: 1.9794 - val_accuracy: 0.4820\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9758 - accuracy: 0.4862 - val_loss: 1.9791 - val_accuracy: 0.4828\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9752 - accuracy: 0.4865 - val_loss: 1.9785 - val_accuracy: 0.4826\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9747 - accuracy: 0.4869 - val_loss: 1.9775 - val_accuracy: 0.4849\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9741 - accuracy: 0.4876 - val_loss: 1.9781 - val_accuracy: 0.4830\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9736 - accuracy: 0.4879 - val_loss: 1.9775 - val_accuracy: 0.4831\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9732 - accuracy: 0.4882 - val_loss: 1.9765 - val_accuracy: 0.4836\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9728 - accuracy: 0.4883 - val_loss: 1.9768 - val_accuracy: 0.4839\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9724 - accuracy: 0.4884 - val_loss: 1.9756 - val_accuracy: 0.4844\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9721 - accuracy: 0.4892 - val_loss: 1.9758 - val_accuracy: 0.4843\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9716 - accuracy: 0.4896 - val_loss: 1.9751 - val_accuracy: 0.4845\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9714 - accuracy: 0.4895 - val_loss: 1.9746 - val_accuracy: 0.4853\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9710 - accuracy: 0.4897 - val_loss: 1.9738 - val_accuracy: 0.4857\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9709 - accuracy: 0.4902 - val_loss: 1.9735 - val_accuracy: 0.4854\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9705 - accuracy: 0.4901 - val_loss: 1.9734 - val_accuracy: 0.4857\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9703 - accuracy: 0.4899 - val_loss: 1.9740 - val_accuracy: 0.4857\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9702 - accuracy: 0.4905 - val_loss: 1.9733 - val_accuracy: 0.4856\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9698 - accuracy: 0.4908 - val_loss: 1.9731 - val_accuracy: 0.4861\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9696 - accuracy: 0.4911 - val_loss: 1.9730 - val_accuracy: 0.4863\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9694 - accuracy: 0.4911 - val_loss: 1.9735 - val_accuracy: 0.4865\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9691 - accuracy: 0.4910 - val_loss: 1.9726 - val_accuracy: 0.4887\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9691 - accuracy: 0.4918 - val_loss: 1.9726 - val_accuracy: 0.4873\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9689 - accuracy: 0.4918 - val_loss: 1.9725 - val_accuracy: 0.4878\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9686 - accuracy: 0.4922 - val_loss: 1.9726 - val_accuracy: 0.4878\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9684 - accuracy: 0.4922 - val_loss: 1.9713 - val_accuracy: 0.4890\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9683 - accuracy: 0.4922 - val_loss: 1.9714 - val_accuracy: 0.4907\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9681 - accuracy: 0.4925 - val_loss: 1.9715 - val_accuracy: 0.4891\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9678 - accuracy: 0.4926 - val_loss: 1.9709 - val_accuracy: 0.4891\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9678 - accuracy: 0.4928 - val_loss: 1.9708 - val_accuracy: 0.4885\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9675 - accuracy: 0.4927 - val_loss: 1.9704 - val_accuracy: 0.4883\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9673 - accuracy: 0.4936 - val_loss: 1.9702 - val_accuracy: 0.4891\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9672 - accuracy: 0.4938 - val_loss: 1.9706 - val_accuracy: 0.4892\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9670 - accuracy: 0.4936 - val_loss: 1.9713 - val_accuracy: 0.4890\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9669 - accuracy: 0.4938 - val_loss: 1.9701 - val_accuracy: 0.4888\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9668 - accuracy: 0.4930 - val_loss: 1.9697 - val_accuracy: 0.4897\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9665 - accuracy: 0.4942 - val_loss: 1.9699 - val_accuracy: 0.4899\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9663 - accuracy: 0.4939 - val_loss: 1.9698 - val_accuracy: 0.4896\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9661 - accuracy: 0.4945 - val_loss: 1.9693 - val_accuracy: 0.4896\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9660 - accuracy: 0.4945 - val_loss: 1.9693 - val_accuracy: 0.4896\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9660 - accuracy: 0.4942 - val_loss: 1.9690 - val_accuracy: 0.4897\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9657 - accuracy: 0.4951 - val_loss: 1.9700 - val_accuracy: 0.4888\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9656 - accuracy: 0.4951 - val_loss: 1.9695 - val_accuracy: 0.4906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9656 - accuracy: 0.4948 - val_loss: 1.9694 - val_accuracy: 0.4912\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [16:25, 176.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 4\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2334 - accuracy: 0.2188 - val_loss: 2.2020 - val_accuracy: 0.2435\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1812 - accuracy: 0.2600 - val_loss: 2.1686 - val_accuracy: 0.2771\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1604 - accuracy: 0.2815 - val_loss: 2.1627 - val_accuracy: 0.2816\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1578 - accuracy: 0.2822 - val_loss: 2.1607 - val_accuracy: 0.2782\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1560 - accuracy: 0.2866 - val_loss: 2.1587 - val_accuracy: 0.2838\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1548 - accuracy: 0.2861 - val_loss: 2.1570 - val_accuracy: 0.2887\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1537 - accuracy: 0.2882 - val_loss: 2.1565 - val_accuracy: 0.2883\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1529 - accuracy: 0.2868 - val_loss: 2.1553 - val_accuracy: 0.2851\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1521 - accuracy: 0.2880 - val_loss: 2.1551 - val_accuracy: 0.2855\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1513 - accuracy: 0.2892 - val_loss: 2.1539 - val_accuracy: 0.2919\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1476 - accuracy: 0.2957 - val_loss: 2.1424 - val_accuracy: 0.3058\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1372 - accuracy: 0.3074 - val_loss: 2.1418 - val_accuracy: 0.3009\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1362 - accuracy: 0.3088 - val_loss: 2.1414 - val_accuracy: 0.3013\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1354 - accuracy: 0.3091 - val_loss: 2.1403 - val_accuracy: 0.3057\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1343 - accuracy: 0.3091 - val_loss: 2.1389 - val_accuracy: 0.3066\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1329 - accuracy: 0.3098 - val_loss: 2.1375 - val_accuracy: 0.3084\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1318 - accuracy: 0.3108 - val_loss: 2.1369 - val_accuracy: 0.3089\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1310 - accuracy: 0.3119 - val_loss: 2.1365 - val_accuracy: 0.3099\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1304 - accuracy: 0.3121 - val_loss: 2.1354 - val_accuracy: 0.3102\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1299 - accuracy: 0.3113 - val_loss: 2.1355 - val_accuracy: 0.3069\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1295 - accuracy: 0.3120 - val_loss: 2.1341 - val_accuracy: 0.3075\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1290 - accuracy: 0.3138 - val_loss: 2.1340 - val_accuracy: 0.3074\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1284 - accuracy: 0.3155 - val_loss: 2.1327 - val_accuracy: 0.3129\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1281 - accuracy: 0.3138 - val_loss: 2.1322 - val_accuracy: 0.3136\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1276 - accuracy: 0.3153 - val_loss: 2.1319 - val_accuracy: 0.3106\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1272 - accuracy: 0.3143 - val_loss: 2.1313 - val_accuracy: 0.3115\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1266 - accuracy: 0.3165 - val_loss: 2.1318 - val_accuracy: 0.3121\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1260 - accuracy: 0.3177 - val_loss: 2.1298 - val_accuracy: 0.3156\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1254 - accuracy: 0.3168 - val_loss: 2.1289 - val_accuracy: 0.3137\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1244 - accuracy: 0.3171 - val_loss: 2.1269 - val_accuracy: 0.3196\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1232 - accuracy: 0.3197 - val_loss: 2.1265 - val_accuracy: 0.3159\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1228 - accuracy: 0.3203 - val_loss: 2.1264 - val_accuracy: 0.3151\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1225 - accuracy: 0.3200 - val_loss: 2.1260 - val_accuracy: 0.3191\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1222 - accuracy: 0.3200 - val_loss: 2.1259 - val_accuracy: 0.3208\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1220 - accuracy: 0.3209 - val_loss: 2.1255 - val_accuracy: 0.3159\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1218 - accuracy: 0.3198 - val_loss: 2.1252 - val_accuracy: 0.3199\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1216 - accuracy: 0.3210 - val_loss: 2.1250 - val_accuracy: 0.3208\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1213 - accuracy: 0.3209 - val_loss: 2.1251 - val_accuracy: 0.3168\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1212 - accuracy: 0.3230 - val_loss: 2.1248 - val_accuracy: 0.3212\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1211 - accuracy: 0.3206 - val_loss: 2.1247 - val_accuracy: 0.3170\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1208 - accuracy: 0.3219 - val_loss: 2.1245 - val_accuracy: 0.3170\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1207 - accuracy: 0.3236 - val_loss: 2.1243 - val_accuracy: 0.3211\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1206 - accuracy: 0.3229 - val_loss: 2.1247 - val_accuracy: 0.3213\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1205 - accuracy: 0.3220 - val_loss: 2.1242 - val_accuracy: 0.3217\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1204 - accuracy: 0.3212 - val_loss: 2.1240 - val_accuracy: 0.3211\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1203 - accuracy: 0.3217 - val_loss: 2.1242 - val_accuracy: 0.3176\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1203 - accuracy: 0.3222 - val_loss: 2.1240 - val_accuracy: 0.3174\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1201 - accuracy: 0.3234 - val_loss: 2.1236 - val_accuracy: 0.3217\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1200 - accuracy: 0.3230 - val_loss: 2.1237 - val_accuracy: 0.3220\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1199 - accuracy: 0.3227 - val_loss: 2.1238 - val_accuracy: 0.3175\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1200 - accuracy: 0.3230 - val_loss: 2.1235 - val_accuracy: 0.3220\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1199 - accuracy: 0.3232 - val_loss: 2.1235 - val_accuracy: 0.3181\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1197 - accuracy: 0.3231 - val_loss: 2.1233 - val_accuracy: 0.3179\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1196 - accuracy: 0.3238 - val_loss: 2.1244 - val_accuracy: 0.3179\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1196 - accuracy: 0.3237 - val_loss: 2.1236 - val_accuracy: 0.3182\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1194 - accuracy: 0.3247 - val_loss: 2.1236 - val_accuracy: 0.3180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [19:02, 163.28s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [1:34:10<1:35:17, 1143.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5951 - accuracy: 0.8774 - val_loss: 1.5177 - val_accuracy: 0.9469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 5\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5116 - accuracy: 0.9528 - val_loss: 1.5041 - val_accuracy: 0.9600\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4976 - accuracy: 0.9653 - val_loss: 1.4971 - val_accuracy: 0.9646\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4909 - accuracy: 0.9712 - val_loss: 1.4963 - val_accuracy: 0.9657\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4858 - accuracy: 0.9761 - val_loss: 1.4906 - val_accuracy: 0.9708\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4826 - accuracy: 0.9793 - val_loss: 1.4894 - val_accuracy: 0.9724\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4803 - accuracy: 0.9815 - val_loss: 1.4892 - val_accuracy: 0.9719\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4786 - accuracy: 0.9829 - val_loss: 1.4871 - val_accuracy: 0.9748\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4781 - accuracy: 0.9834 - val_loss: 1.4869 - val_accuracy: 0.9743\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4760 - accuracy: 0.9857 - val_loss: 1.4852 - val_accuracy: 0.9763\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4756 - accuracy: 0.9858 - val_loss: 1.4821 - val_accuracy: 0.9793\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4748 - accuracy: 0.9866 - val_loss: 1.4879 - val_accuracy: 0.9729\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4732 - accuracy: 0.9881 - val_loss: 1.4823 - val_accuracy: 0.9791\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4736 - accuracy: 0.9877 - val_loss: 1.4828 - val_accuracy: 0.9783\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:05, 125.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 5\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4888 - accuracy: 0.9745 - val_loss: 1.4914 - val_accuracy: 0.9707\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4790 - accuracy: 0.9837 - val_loss: 1.4871 - val_accuracy: 0.9755\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4759 - accuracy: 0.9864 - val_loss: 1.4876 - val_accuracy: 0.9745\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4741 - accuracy: 0.9882 - val_loss: 1.4874 - val_accuracy: 0.9745\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4727 - accuracy: 0.9893 - val_loss: 1.4860 - val_accuracy: 0.9757\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9906 - val_loss: 1.4877 - val_accuracy: 0.9735\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4709 - accuracy: 0.9910 - val_loss: 1.4859 - val_accuracy: 0.9761\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4704 - accuracy: 0.9913 - val_loss: 1.4858 - val_accuracy: 0.9763\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4703 - accuracy: 0.9914 - val_loss: 1.4840 - val_accuracy: 0.9774\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4695 - accuracy: 0.9922 - val_loss: 1.4844 - val_accuracy: 0.9774\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4694 - accuracy: 0.9920 - val_loss: 1.4863 - val_accuracy: 0.9752\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4689 - accuracy: 0.9926 - val_loss: 1.4846 - val_accuracy: 0.9768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:04, 123.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 5\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5536 - accuracy: 0.9234 - val_loss: 1.5205 - val_accuracy: 0.9457\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5069 - accuracy: 0.9602 - val_loss: 1.5107 - val_accuracy: 0.9541\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4988 - accuracy: 0.9670 - val_loss: 1.5066 - val_accuracy: 0.9581\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4937 - accuracy: 0.9710 - val_loss: 1.5037 - val_accuracy: 0.9599\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4900 - accuracy: 0.9744 - val_loss: 1.5044 - val_accuracy: 0.9588\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4878 - accuracy: 0.9765 - val_loss: 1.5008 - val_accuracy: 0.9628\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4855 - accuracy: 0.9786 - val_loss: 1.5027 - val_accuracy: 0.9592\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4838 - accuracy: 0.9798 - val_loss: 1.5005 - val_accuracy: 0.9619\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4823 - accuracy: 0.9813 - val_loss: 1.4992 - val_accuracy: 0.9634\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4810 - accuracy: 0.9824 - val_loss: 1.5001 - val_accuracy: 0.9628\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4798 - accuracy: 0.9833 - val_loss: 1.4994 - val_accuracy: 0.9629\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4791 - accuracy: 0.9839 - val_loss: 1.4982 - val_accuracy: 0.9638\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4780 - accuracy: 0.9849 - val_loss: 1.4995 - val_accuracy: 0.9617\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4775 - accuracy: 0.9853 - val_loss: 1.4976 - val_accuracy: 0.9640\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4768 - accuracy: 0.9859 - val_loss: 1.4985 - val_accuracy: 0.9627\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4761 - accuracy: 0.9866 - val_loss: 1.4977 - val_accuracy: 0.9638\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9867 - val_loss: 1.4967 - val_accuracy: 0.9653\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9872 - val_loss: 1.4983 - val_accuracy: 0.9626\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4748 - accuracy: 0.9876 - val_loss: 1.4981 - val_accuracy: 0.9635\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4743 - accuracy: 0.9880 - val_loss: 1.4983 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:34, 131.60s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 5\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7415 - accuracy: 0.7398 - val_loss: 1.6200 - val_accuracy: 0.8532\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6068 - accuracy: 0.8647 - val_loss: 1.5960 - val_accuracy: 0.8729\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5890 - accuracy: 0.8803 - val_loss: 1.5841 - val_accuracy: 0.8831\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5797 - accuracy: 0.8890 - val_loss: 1.5787 - val_accuracy: 0.8897\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5736 - accuracy: 0.8935 - val_loss: 1.5751 - val_accuracy: 0.8900\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5689 - accuracy: 0.8978 - val_loss: 1.5728 - val_accuracy: 0.8917\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5653 - accuracy: 0.9007 - val_loss: 1.5702 - val_accuracy: 0.8937\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5623 - accuracy: 0.9040 - val_loss: 1.5675 - val_accuracy: 0.8958\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5597 - accuracy: 0.9066 - val_loss: 1.5672 - val_accuracy: 0.8958\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5577 - accuracy: 0.9083 - val_loss: 1.5652 - val_accuracy: 0.8996\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5558 - accuracy: 0.9100 - val_loss: 1.5674 - val_accuracy: 0.8962\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5542 - accuracy: 0.9115 - val_loss: 1.5640 - val_accuracy: 0.8997\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5528 - accuracy: 0.9128 - val_loss: 1.5639 - val_accuracy: 0.8991\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5514 - accuracy: 0.9139 - val_loss: 1.5625 - val_accuracy: 0.8991\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5502 - accuracy: 0.9147 - val_loss: 1.5629 - val_accuracy: 0.8986\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5493 - accuracy: 0.9157 - val_loss: 1.5624 - val_accuracy: 0.8995\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5484 - accuracy: 0.9167 - val_loss: 1.5616 - val_accuracy: 0.9011\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5473 - accuracy: 0.9175 - val_loss: 1.5620 - val_accuracy: 0.8999\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5464 - accuracy: 0.9179 - val_loss: 1.5617 - val_accuracy: 0.9013\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5458 - accuracy: 0.9189 - val_loss: 1.5611 - val_accuracy: 0.9007\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5450 - accuracy: 0.9200 - val_loss: 1.5611 - val_accuracy: 0.9002\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5443 - accuracy: 0.9205 - val_loss: 1.5615 - val_accuracy: 0.9006\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5435 - accuracy: 0.9212 - val_loss: 1.5619 - val_accuracy: 0.8999\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5433 - accuracy: 0.9214 - val_loss: 1.5613 - val_accuracy: 0.9013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:33, 145.65s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 5\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9194 - accuracy: 0.5627 - val_loss: 1.8135 - val_accuracy: 0.6635\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7963 - accuracy: 0.6763 - val_loss: 1.7743 - val_accuracy: 0.6977\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7733 - accuracy: 0.6952 - val_loss: 1.7593 - val_accuracy: 0.7114\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7613 - accuracy: 0.7060 - val_loss: 1.7511 - val_accuracy: 0.7165\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7536 - accuracy: 0.7136 - val_loss: 1.7455 - val_accuracy: 0.7200\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7477 - accuracy: 0.7182 - val_loss: 1.7420 - val_accuracy: 0.7248\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7428 - accuracy: 0.7232 - val_loss: 1.7375 - val_accuracy: 0.7272\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7388 - accuracy: 0.7270 - val_loss: 1.7330 - val_accuracy: 0.7340\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7352 - accuracy: 0.7305 - val_loss: 1.7316 - val_accuracy: 0.7332\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7325 - accuracy: 0.7328 - val_loss: 1.7269 - val_accuracy: 0.7369\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7299 - accuracy: 0.7353 - val_loss: 1.7253 - val_accuracy: 0.7393\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7277 - accuracy: 0.7376 - val_loss: 1.7230 - val_accuracy: 0.7412\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7257 - accuracy: 0.7400 - val_loss: 1.7226 - val_accuracy: 0.7414\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7240 - accuracy: 0.7417 - val_loss: 1.7222 - val_accuracy: 0.7421\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7226 - accuracy: 0.7424 - val_loss: 1.7213 - val_accuracy: 0.7426\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7209 - accuracy: 0.7448 - val_loss: 1.7190 - val_accuracy: 0.7463\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7199 - accuracy: 0.7447 - val_loss: 1.7189 - val_accuracy: 0.7447\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7189 - accuracy: 0.7456 - val_loss: 1.7182 - val_accuracy: 0.7449\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7176 - accuracy: 0.7469 - val_loss: 1.7180 - val_accuracy: 0.7446\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7167 - accuracy: 0.7476 - val_loss: 1.7173 - val_accuracy: 0.7457\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7159 - accuracy: 0.7480 - val_loss: 1.7183 - val_accuracy: 0.7429\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7151 - accuracy: 0.7490 - val_loss: 1.7154 - val_accuracy: 0.7471\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7143 - accuracy: 0.7501 - val_loss: 1.7162 - val_accuracy: 0.7467\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7137 - accuracy: 0.7504 - val_loss: 1.7158 - val_accuracy: 0.7453\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7131 - accuracy: 0.7510 - val_loss: 1.7143 - val_accuracy: 0.7474\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7126 - accuracy: 0.7509 - val_loss: 1.7134 - val_accuracy: 0.7487\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7119 - accuracy: 0.7515 - val_loss: 1.7147 - val_accuracy: 0.7471\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7113 - accuracy: 0.7521 - val_loss: 1.7131 - val_accuracy: 0.7480\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7110 - accuracy: 0.7524 - val_loss: 1.7142 - val_accuracy: 0.7469\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7104 - accuracy: 0.7532 - val_loss: 1.7126 - val_accuracy: 0.7502\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7101 - accuracy: 0.7535 - val_loss: 1.7120 - val_accuracy: 0.7492\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7096 - accuracy: 0.7542 - val_loss: 1.7130 - val_accuracy: 0.7477\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7092 - accuracy: 0.7545 - val_loss: 1.7124 - val_accuracy: 0.7490\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7089 - accuracy: 0.7544 - val_loss: 1.7112 - val_accuracy: 0.7497\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7085 - accuracy: 0.7545 - val_loss: 1.7102 - val_accuracy: 0.7510\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7082 - accuracy: 0.7550 - val_loss: 1.7125 - val_accuracy: 0.7497\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7077 - accuracy: 0.7554 - val_loss: 1.7100 - val_accuracy: 0.7519\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7078 - accuracy: 0.7552 - val_loss: 1.7104 - val_accuracy: 0.7514\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7072 - accuracy: 0.7563 - val_loss: 1.7110 - val_accuracy: 0.7497\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7068 - accuracy: 0.7570 - val_loss: 1.7103 - val_accuracy: 0.7507\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:45, 159.74s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 5\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2273 - accuracy: 0.2235 - val_loss: 2.1992 - val_accuracy: 0.2505\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1805 - accuracy: 0.2775 - val_loss: 2.1675 - val_accuracy: 0.2870\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1429 - accuracy: 0.3150 - val_loss: 2.1409 - val_accuracy: 0.3139\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1356 - accuracy: 0.3199 - val_loss: 2.1377 - val_accuracy: 0.3162\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1324 - accuracy: 0.3219 - val_loss: 2.1349 - val_accuracy: 0.3204\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1298 - accuracy: 0.3250 - val_loss: 2.1336 - val_accuracy: 0.3197\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1280 - accuracy: 0.3262 - val_loss: 2.1313 - val_accuracy: 0.3217\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1265 - accuracy: 0.3272 - val_loss: 2.1302 - val_accuracy: 0.3224\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1252 - accuracy: 0.3285 - val_loss: 2.1299 - val_accuracy: 0.3224\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1245 - accuracy: 0.3297 - val_loss: 2.1285 - val_accuracy: 0.3236\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1235 - accuracy: 0.3302 - val_loss: 2.1280 - val_accuracy: 0.3233\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1229 - accuracy: 0.3308 - val_loss: 2.1273 - val_accuracy: 0.3237\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1224 - accuracy: 0.3314 - val_loss: 2.1267 - val_accuracy: 0.3253\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1217 - accuracy: 0.3316 - val_loss: 2.1266 - val_accuracy: 0.3253\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1211 - accuracy: 0.3323 - val_loss: 2.1262 - val_accuracy: 0.3263\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1207 - accuracy: 0.3325 - val_loss: 2.1256 - val_accuracy: 0.3266\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1202 - accuracy: 0.3330 - val_loss: 2.1255 - val_accuracy: 0.3258\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1199 - accuracy: 0.3325 - val_loss: 2.1250 - val_accuracy: 0.3269\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1196 - accuracy: 0.3335 - val_loss: 2.1247 - val_accuracy: 0.3266\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1193 - accuracy: 0.3335 - val_loss: 2.1264 - val_accuracy: 0.3269\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1189 - accuracy: 0.3341 - val_loss: 2.1239 - val_accuracy: 0.3286\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1187 - accuracy: 0.3343 - val_loss: 2.1235 - val_accuracy: 0.3285\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1182 - accuracy: 0.3345 - val_loss: 2.1232 - val_accuracy: 0.3289\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1181 - accuracy: 0.3353 - val_loss: 2.1236 - val_accuracy: 0.3283\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1177 - accuracy: 0.3350 - val_loss: 2.1231 - val_accuracy: 0.3285\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1176 - accuracy: 0.3353 - val_loss: 2.1231 - val_accuracy: 0.3289\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1174 - accuracy: 0.3355 - val_loss: 2.1224 - val_accuracy: 0.3297\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1172 - accuracy: 0.3358 - val_loss: 2.1223 - val_accuracy: 0.3293\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1169 - accuracy: 0.3355 - val_loss: 2.1223 - val_accuracy: 0.3293\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1167 - accuracy: 0.3361 - val_loss: 2.1221 - val_accuracy: 0.3292\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1166 - accuracy: 0.3361 - val_loss: 2.1218 - val_accuracy: 0.3301\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1163 - accuracy: 0.3366 - val_loss: 2.1212 - val_accuracy: 0.3305\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1162 - accuracy: 0.3367 - val_loss: 2.1227 - val_accuracy: 0.3301\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1161 - accuracy: 0.3365 - val_loss: 2.1210 - val_accuracy: 0.3314\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1160 - accuracy: 0.3368 - val_loss: 2.1212 - val_accuracy: 0.3308\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1157 - accuracy: 0.3370 - val_loss: 2.1219 - val_accuracy: 0.3298\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1157 - accuracy: 0.3372 - val_loss: 2.1206 - val_accuracy: 0.3316\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1156 - accuracy: 0.3375 - val_loss: 2.1204 - val_accuracy: 0.3316\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1155 - accuracy: 0.3373 - val_loss: 2.1207 - val_accuracy: 0.3310\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1154 - accuracy: 0.3374 - val_loss: 2.1200 - val_accuracy: 0.3318\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1153 - accuracy: 0.3378 - val_loss: 2.1200 - val_accuracy: 0.3320\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1150 - accuracy: 0.3379 - val_loss: 2.1201 - val_accuracy: 0.3310\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1149 - accuracy: 0.3380 - val_loss: 2.1199 - val_accuracy: 0.3317\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1149 - accuracy: 0.3384 - val_loss: 2.1217 - val_accuracy: 0.3311\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1148 - accuracy: 0.3381 - val_loss: 2.1195 - val_accuracy: 0.3327\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1147 - accuracy: 0.3384 - val_loss: 2.1197 - val_accuracy: 0.3320\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1146 - accuracy: 0.3383 - val_loss: 2.1198 - val_accuracy: 0.3327\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1143 - accuracy: 0.3383 - val_loss: 2.1190 - val_accuracy: 0.3329\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1143 - accuracy: 0.3387 - val_loss: 2.1193 - val_accuracy: 0.3337\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1144 - accuracy: 0.3387 - val_loss: 2.1195 - val_accuracy: 0.3323\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1142 - accuracy: 0.3389 - val_loss: 2.1192 - val_accuracy: 0.3317\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [15:30, 161.34s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 5\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2610 - accuracy: 0.1951 - val_loss: 2.2328 - val_accuracy: 0.2197\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2258 - accuracy: 0.2233 - val_loss: 2.2192 - val_accuracy: 0.2236\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2100 - accuracy: 0.2271 - val_loss: 2.2082 - val_accuracy: 0.2242\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2005 - accuracy: 0.2388 - val_loss: 2.2021 - val_accuracy: 0.2424\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1957 - accuracy: 0.2498 - val_loss: 2.1987 - val_accuracy: 0.2457\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1925 - accuracy: 0.2510 - val_loss: 2.1961 - val_accuracy: 0.2460\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1900 - accuracy: 0.2512 - val_loss: 2.1938 - val_accuracy: 0.2464\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1881 - accuracy: 0.2519 - val_loss: 2.1923 - val_accuracy: 0.2472\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1866 - accuracy: 0.2528 - val_loss: 2.1910 - val_accuracy: 0.2471\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1856 - accuracy: 0.2529 - val_loss: 2.1902 - val_accuracy: 0.2470\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1847 - accuracy: 0.2524 - val_loss: 2.1895 - val_accuracy: 0.2474\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1840 - accuracy: 0.2532 - val_loss: 2.1888 - val_accuracy: 0.2474\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1834 - accuracy: 0.2529 - val_loss: 2.1885 - val_accuracy: 0.2471\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1829 - accuracy: 0.2538 - val_loss: 2.1879 - val_accuracy: 0.2482\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1825 - accuracy: 0.2533 - val_loss: 2.1877 - val_accuracy: 0.2452\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1821 - accuracy: 0.2537 - val_loss: 2.1874 - val_accuracy: 0.2479\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1819 - accuracy: 0.2550 - val_loss: 2.1870 - val_accuracy: 0.2485\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1816 - accuracy: 0.2537 - val_loss: 2.1867 - val_accuracy: 0.2484\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1814 - accuracy: 0.2552 - val_loss: 2.1866 - val_accuracy: 0.2487\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1811 - accuracy: 0.2539 - val_loss: 2.1863 - val_accuracy: 0.2484\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1810 - accuracy: 0.2541 - val_loss: 2.1862 - val_accuracy: 0.2490\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1809 - accuracy: 0.2543 - val_loss: 2.1862 - val_accuracy: 0.2491\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1807 - accuracy: 0.2551 - val_loss: 2.1860 - val_accuracy: 0.2490\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1805 - accuracy: 0.2548 - val_loss: 2.1859 - val_accuracy: 0.2495\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1803 - accuracy: 0.2556 - val_loss: 2.1857 - val_accuracy: 0.2486\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1802 - accuracy: 0.2553 - val_loss: 2.1856 - val_accuracy: 0.2494\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1801 - accuracy: 0.2558 - val_loss: 2.1856 - val_accuracy: 0.2494\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1800 - accuracy: 0.2552 - val_loss: 2.1854 - val_accuracy: 0.2490\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1798 - accuracy: 0.2553 - val_loss: 2.1849 - val_accuracy: 0.2495\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1797 - accuracy: 0.2554 - val_loss: 2.1849 - val_accuracy: 0.2468\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1796 - accuracy: 0.2554 - val_loss: 2.1849 - val_accuracy: 0.2497\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1795 - accuracy: 0.2549 - val_loss: 2.1844 - val_accuracy: 0.2506\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1794 - accuracy: 0.2553 - val_loss: 2.1843 - val_accuracy: 0.2506\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1794 - accuracy: 0.2560 - val_loss: 2.1841 - val_accuracy: 0.2513\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1792 - accuracy: 0.2567 - val_loss: 2.1841 - val_accuracy: 0.2482\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1791 - accuracy: 0.2553 - val_loss: 2.1842 - val_accuracy: 0.2512\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1791 - accuracy: 0.2567 - val_loss: 2.1842 - val_accuracy: 0.2484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [17:31, 150.27s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [1:51:44<1:14:26, 1116.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5787 - accuracy: 0.8961 - val_loss: 1.5232 - val_accuracy: 0.9418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 6\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5138 - accuracy: 0.9503 - val_loss: 1.5107 - val_accuracy: 0.9533\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4992 - accuracy: 0.9633 - val_loss: 1.5018 - val_accuracy: 0.9612\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4916 - accuracy: 0.9708 - val_loss: 1.4936 - val_accuracy: 0.9683\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4863 - accuracy: 0.9758 - val_loss: 1.4925 - val_accuracy: 0.9691\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4836 - accuracy: 0.9784 - val_loss: 1.4885 - val_accuracy: 0.9728\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4813 - accuracy: 0.9803 - val_loss: 1.4935 - val_accuracy: 0.9681\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4793 - accuracy: 0.9824 - val_loss: 1.4861 - val_accuracy: 0.9748\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4786 - accuracy: 0.9827 - val_loss: 1.4866 - val_accuracy: 0.9749\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4765 - accuracy: 0.9851 - val_loss: 1.4843 - val_accuracy: 0.9771\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4759 - accuracy: 0.9855 - val_loss: 1.4856 - val_accuracy: 0.9758\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4752 - accuracy: 0.9862 - val_loss: 1.4899 - val_accuracy: 0.9706\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4740 - accuracy: 0.9874 - val_loss: 1.4843 - val_accuracy: 0.9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:56, 116.88s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 6\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4937 - accuracy: 0.9699 - val_loss: 1.4933 - val_accuracy: 0.9692\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4819 - accuracy: 0.9810 - val_loss: 1.4907 - val_accuracy: 0.9718\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4778 - accuracy: 0.9848 - val_loss: 1.4900 - val_accuracy: 0.9722\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4764 - accuracy: 0.9856 - val_loss: 1.4895 - val_accuracy: 0.9724\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4746 - accuracy: 0.9873 - val_loss: 1.4872 - val_accuracy: 0.9752\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4733 - accuracy: 0.9888 - val_loss: 1.4868 - val_accuracy: 0.9749\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4726 - accuracy: 0.9893 - val_loss: 1.4874 - val_accuracy: 0.9747\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.9899 - val_loss: 1.4859 - val_accuracy: 0.9755\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4713 - accuracy: 0.9906 - val_loss: 1.4883 - val_accuracy: 0.9734\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4708 - accuracy: 0.9908 - val_loss: 1.4861 - val_accuracy: 0.9754\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4708 - accuracy: 0.9908 - val_loss: 1.4898 - val_accuracy: 0.9715\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:57, 118.14s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 6\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5723 - accuracy: 0.9058 - val_loss: 1.5379 - val_accuracy: 0.9301\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5254 - accuracy: 0.9419 - val_loss: 1.5260 - val_accuracy: 0.9404\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5155 - accuracy: 0.9507 - val_loss: 1.5220 - val_accuracy: 0.9422\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5099 - accuracy: 0.9559 - val_loss: 1.5176 - val_accuracy: 0.9469\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5057 - accuracy: 0.9594 - val_loss: 1.5172 - val_accuracy: 0.9481\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5021 - accuracy: 0.9627 - val_loss: 1.5158 - val_accuracy: 0.9471\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4995 - accuracy: 0.9649 - val_loss: 1.5136 - val_accuracy: 0.9497\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4972 - accuracy: 0.9672 - val_loss: 1.5145 - val_accuracy: 0.9478\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4955 - accuracy: 0.9685 - val_loss: 1.5119 - val_accuracy: 0.9517\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4944 - accuracy: 0.9693 - val_loss: 1.5118 - val_accuracy: 0.9507\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4925 - accuracy: 0.9714 - val_loss: 1.5118 - val_accuracy: 0.9521\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4909 - accuracy: 0.9729 - val_loss: 1.5109 - val_accuracy: 0.9512\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4895 - accuracy: 0.9740 - val_loss: 1.5095 - val_accuracy: 0.9527\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4888 - accuracy: 0.9747 - val_loss: 1.5089 - val_accuracy: 0.9535\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4878 - accuracy: 0.9755 - val_loss: 1.5102 - val_accuracy: 0.9528\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4871 - accuracy: 0.9762 - val_loss: 1.5101 - val_accuracy: 0.9516\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4866 - accuracy: 0.9767 - val_loss: 1.5091 - val_accuracy: 0.9531\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:17, 124.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 6\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7876 - accuracy: 0.6921 - val_loss: 1.6461 - val_accuracy: 0.8270\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6265 - accuracy: 0.8476 - val_loss: 1.6122 - val_accuracy: 0.8583\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6065 - accuracy: 0.8637 - val_loss: 1.6022 - val_accuracy: 0.8678\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5966 - accuracy: 0.8724 - val_loss: 1.5941 - val_accuracy: 0.8747\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5902 - accuracy: 0.8780 - val_loss: 1.5915 - val_accuracy: 0.8764\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5856 - accuracy: 0.8821 - val_loss: 1.5863 - val_accuracy: 0.8811\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5815 - accuracy: 0.8856 - val_loss: 1.5841 - val_accuracy: 0.8826\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5783 - accuracy: 0.8890 - val_loss: 1.5827 - val_accuracy: 0.8828\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5755 - accuracy: 0.8913 - val_loss: 1.5821 - val_accuracy: 0.8827\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5731 - accuracy: 0.8938 - val_loss: 1.5791 - val_accuracy: 0.8860\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5710 - accuracy: 0.8955 - val_loss: 1.5799 - val_accuracy: 0.8841\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5692 - accuracy: 0.8977 - val_loss: 1.5791 - val_accuracy: 0.8851\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5678 - accuracy: 0.8985 - val_loss: 1.5792 - val_accuracy: 0.8844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [08:49, 132.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 6\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9721 - accuracy: 0.5210 - val_loss: 1.8731 - val_accuracy: 0.6216\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8562 - accuracy: 0.6317 - val_loss: 1.8196 - val_accuracy: 0.6645\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8198 - accuracy: 0.6612 - val_loss: 1.8005 - val_accuracy: 0.6774\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8052 - accuracy: 0.6697 - val_loss: 1.7894 - val_accuracy: 0.6851\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7965 - accuracy: 0.6757 - val_loss: 1.7827 - val_accuracy: 0.6887\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7908 - accuracy: 0.6791 - val_loss: 1.7776 - val_accuracy: 0.6901\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7864 - accuracy: 0.6829 - val_loss: 1.7752 - val_accuracy: 0.6919\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7830 - accuracy: 0.6847 - val_loss: 1.7717 - val_accuracy: 0.6963\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7803 - accuracy: 0.6869 - val_loss: 1.7694 - val_accuracy: 0.6963\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7779 - accuracy: 0.6888 - val_loss: 1.7675 - val_accuracy: 0.6981\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7758 - accuracy: 0.6898 - val_loss: 1.7661 - val_accuracy: 0.7003\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7742 - accuracy: 0.6914 - val_loss: 1.7645 - val_accuracy: 0.7006\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7724 - accuracy: 0.6928 - val_loss: 1.7644 - val_accuracy: 0.6975\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7710 - accuracy: 0.6941 - val_loss: 1.7633 - val_accuracy: 0.6998\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7696 - accuracy: 0.6959 - val_loss: 1.7616 - val_accuracy: 0.7021\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7684 - accuracy: 0.6966 - val_loss: 1.7615 - val_accuracy: 0.7026\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7671 - accuracy: 0.6980 - val_loss: 1.7609 - val_accuracy: 0.7022\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7658 - accuracy: 0.6985 - val_loss: 1.7604 - val_accuracy: 0.7017\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7651 - accuracy: 0.6998 - val_loss: 1.7585 - val_accuracy: 0.7047\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7639 - accuracy: 0.7006 - val_loss: 1.7586 - val_accuracy: 0.7042\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7629 - accuracy: 0.7014 - val_loss: 1.7581 - val_accuracy: 0.7037\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7618 - accuracy: 0.7028 - val_loss: 1.7575 - val_accuracy: 0.7072\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7610 - accuracy: 0.7033 - val_loss: 1.7555 - val_accuracy: 0.7073\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7601 - accuracy: 0.7046 - val_loss: 1.7568 - val_accuracy: 0.7057\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7593 - accuracy: 0.7053 - val_loss: 1.7545 - val_accuracy: 0.7059\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7583 - accuracy: 0.7065 - val_loss: 1.7535 - val_accuracy: 0.7091\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7577 - accuracy: 0.7064 - val_loss: 1.7541 - val_accuracy: 0.7071\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7571 - accuracy: 0.7070 - val_loss: 1.7532 - val_accuracy: 0.7099\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7565 - accuracy: 0.7070 - val_loss: 1.7525 - val_accuracy: 0.7075\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7556 - accuracy: 0.7089 - val_loss: 1.7524 - val_accuracy: 0.7086\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7552 - accuracy: 0.7089 - val_loss: 1.7513 - val_accuracy: 0.7102\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7547 - accuracy: 0.7090 - val_loss: 1.7506 - val_accuracy: 0.7106\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7539 - accuracy: 0.7096 - val_loss: 1.7510 - val_accuracy: 0.7101\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7537 - accuracy: 0.7099 - val_loss: 1.7495 - val_accuracy: 0.7115\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7529 - accuracy: 0.7109 - val_loss: 1.7499 - val_accuracy: 0.7124\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7526 - accuracy: 0.7115 - val_loss: 1.7494 - val_accuracy: 0.7139\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7521 - accuracy: 0.7115 - val_loss: 1.7492 - val_accuracy: 0.7129\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7517 - accuracy: 0.7115 - val_loss: 1.7485 - val_accuracy: 0.7126\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7515 - accuracy: 0.7121 - val_loss: 1.7477 - val_accuracy: 0.7142\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7510 - accuracy: 0.7122 - val_loss: 1.7473 - val_accuracy: 0.7138\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7506 - accuracy: 0.7131 - val_loss: 1.7472 - val_accuracy: 0.7145\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7502 - accuracy: 0.7139 - val_loss: 1.7469 - val_accuracy: 0.7144\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7497 - accuracy: 0.7139 - val_loss: 1.7494 - val_accuracy: 0.7118\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7494 - accuracy: 0.7143 - val_loss: 1.7480 - val_accuracy: 0.7130\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7493 - accuracy: 0.7142 - val_loss: 1.7478 - val_accuracy: 0.7126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [11:55, 148.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 6\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2081 - accuracy: 0.2476 - val_loss: 2.1712 - val_accuracy: 0.2809\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1480 - accuracy: 0.3111 - val_loss: 2.1415 - val_accuracy: 0.3163\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1332 - accuracy: 0.3239 - val_loss: 2.1212 - val_accuracy: 0.3297\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1130 - accuracy: 0.3426 - val_loss: 2.1125 - val_accuracy: 0.3399\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1093 - accuracy: 0.3448 - val_loss: 2.1114 - val_accuracy: 0.3426\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1075 - accuracy: 0.3474 - val_loss: 2.1086 - val_accuracy: 0.3453\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1060 - accuracy: 0.3491 - val_loss: 2.1074 - val_accuracy: 0.3456\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1050 - accuracy: 0.3497 - val_loss: 2.1058 - val_accuracy: 0.3471\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1040 - accuracy: 0.3507 - val_loss: 2.1048 - val_accuracy: 0.3471\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1031 - accuracy: 0.3509 - val_loss: 2.1049 - val_accuracy: 0.3487\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1023 - accuracy: 0.3512 - val_loss: 2.1031 - val_accuracy: 0.3472\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1017 - accuracy: 0.3518 - val_loss: 2.1028 - val_accuracy: 0.3472\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1010 - accuracy: 0.3531 - val_loss: 2.1023 - val_accuracy: 0.3528\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1005 - accuracy: 0.3530 - val_loss: 2.1012 - val_accuracy: 0.3515\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1001 - accuracy: 0.3532 - val_loss: 2.1014 - val_accuracy: 0.3512\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0996 - accuracy: 0.3539 - val_loss: 2.1003 - val_accuracy: 0.3517\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0989 - accuracy: 0.3552 - val_loss: 2.1012 - val_accuracy: 0.3494\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0989 - accuracy: 0.3548 - val_loss: 2.0998 - val_accuracy: 0.3511\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0985 - accuracy: 0.3548 - val_loss: 2.1005 - val_accuracy: 0.3515\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0981 - accuracy: 0.3553 - val_loss: 2.0989 - val_accuracy: 0.3513\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0980 - accuracy: 0.3550 - val_loss: 2.0993 - val_accuracy: 0.3527\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0975 - accuracy: 0.3558 - val_loss: 2.0995 - val_accuracy: 0.3529\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0975 - accuracy: 0.3562 - val_loss: 2.0990 - val_accuracy: 0.3529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [13:45, 137.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 6\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2756 - accuracy: 0.1760 - val_loss: 2.2396 - val_accuracy: 0.2149\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2300 - accuracy: 0.2229 - val_loss: 2.2304 - val_accuracy: 0.2232\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2220 - accuracy: 0.2320 - val_loss: 2.2213 - val_accuracy: 0.2347\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2135 - accuracy: 0.2412 - val_loss: 2.2145 - val_accuracy: 0.2404\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2101 - accuracy: 0.2436 - val_loss: 2.2125 - val_accuracy: 0.2414\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2086 - accuracy: 0.2445 - val_loss: 2.2108 - val_accuracy: 0.2426\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2077 - accuracy: 0.2452 - val_loss: 2.2100 - val_accuracy: 0.2427\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2070 - accuracy: 0.2457 - val_loss: 2.2092 - val_accuracy: 0.2437\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2064 - accuracy: 0.2463 - val_loss: 2.2088 - val_accuracy: 0.2443\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2060 - accuracy: 0.2466 - val_loss: 2.2083 - val_accuracy: 0.2447\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2055 - accuracy: 0.2470 - val_loss: 2.2081 - val_accuracy: 0.2449\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2051 - accuracy: 0.2476 - val_loss: 2.2076 - val_accuracy: 0.2449\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2048 - accuracy: 0.2478 - val_loss: 2.2072 - val_accuracy: 0.2453\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2044 - accuracy: 0.2480 - val_loss: 2.2069 - val_accuracy: 0.2455\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2040 - accuracy: 0.2482 - val_loss: 2.2066 - val_accuracy: 0.2462\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2037 - accuracy: 0.2484 - val_loss: 2.2060 - val_accuracy: 0.2463\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2034 - accuracy: 0.2487 - val_loss: 2.2060 - val_accuracy: 0.2466\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2031 - accuracy: 0.2490 - val_loss: 2.2060 - val_accuracy: 0.2471\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2028 - accuracy: 0.2490 - val_loss: 2.2057 - val_accuracy: 0.2472\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2027 - accuracy: 0.2490 - val_loss: 2.2054 - val_accuracy: 0.2467\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2025 - accuracy: 0.2493 - val_loss: 2.2054 - val_accuracy: 0.2469\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2022 - accuracy: 0.2492 - val_loss: 2.2053 - val_accuracy: 0.2469\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2021 - accuracy: 0.2494 - val_loss: 2.2051 - val_accuracy: 0.2475\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2019 - accuracy: 0.2495 - val_loss: 2.2050 - val_accuracy: 0.2475\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2017 - accuracy: 0.2495 - val_loss: 2.2051 - val_accuracy: 0.2471\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2015 - accuracy: 0.2496 - val_loss: 2.2052 - val_accuracy: 0.2472\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2014 - accuracy: 0.2500 - val_loss: 2.2051 - val_accuracy: 0.2469\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [15:27, 132.45s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [2:07:13<53:01, 1060.49s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5795 - accuracy: 0.8940 - val_loss: 1.5282 - val_accuracy: 0.9363\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 7\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5135 - accuracy: 0.9503 - val_loss: 1.5033 - val_accuracy: 0.9595\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4984 - accuracy: 0.9645 - val_loss: 1.4986 - val_accuracy: 0.9630\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4921 - accuracy: 0.9707 - val_loss: 1.4955 - val_accuracy: 0.9670\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4875 - accuracy: 0.9744 - val_loss: 1.4958 - val_accuracy: 0.9659\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9785 - val_loss: 1.4926 - val_accuracy: 0.9693\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4814 - accuracy: 0.9805 - val_loss: 1.4872 - val_accuracy: 0.9751\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4805 - accuracy: 0.9812 - val_loss: 1.4892 - val_accuracy: 0.9724\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4781 - accuracy: 0.9836 - val_loss: 1.4844 - val_accuracy: 0.9770\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4769 - accuracy: 0.9845 - val_loss: 1.4883 - val_accuracy: 0.9731\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4762 - accuracy: 0.9853 - val_loss: 1.4903 - val_accuracy: 0.9697\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4751 - accuracy: 0.9862 - val_loss: 1.4839 - val_accuracy: 0.9769\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9860 - val_loss: 1.4857 - val_accuracy: 0.9748\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4741 - accuracy: 0.9873 - val_loss: 1.4843 - val_accuracy: 0.9768\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4734 - accuracy: 0.9879 - val_loss: 1.4839 - val_accuracy: 0.9772\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4722 - accuracy: 0.9891 - val_loss: 1.4846 - val_accuracy: 0.9770\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4723 - accuracy: 0.9891 - val_loss: 1.4818 - val_accuracy: 0.9791\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4717 - accuracy: 0.9895 - val_loss: 1.4854 - val_accuracy: 0.9754\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4711 - accuracy: 0.9902 - val_loss: 1.4834 - val_accuracy: 0.9777\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4720 - accuracy: 0.9894 - val_loss: 1.4822 - val_accuracy: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [03:37, 217.07s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 7\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4885 - accuracy: 0.9743 - val_loss: 1.4894 - val_accuracy: 0.9734\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4780 - accuracy: 0.9843 - val_loss: 1.4878 - val_accuracy: 0.9749\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4749 - accuracy: 0.9871 - val_loss: 1.4872 - val_accuracy: 0.9747\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9887 - val_loss: 1.4882 - val_accuracy: 0.9735\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4716 - accuracy: 0.9902 - val_loss: 1.4871 - val_accuracy: 0.9743\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4716 - accuracy: 0.9901 - val_loss: 1.4853 - val_accuracy: 0.9763\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4705 - accuracy: 0.9912 - val_loss: 1.4851 - val_accuracy: 0.9758\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4702 - accuracy: 0.9915 - val_loss: 1.4854 - val_accuracy: 0.9755\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4697 - accuracy: 0.9919 - val_loss: 1.4855 - val_accuracy: 0.9759\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4694 - accuracy: 0.9921 - val_loss: 1.4846 - val_accuracy: 0.9774\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4685 - accuracy: 0.9931 - val_loss: 1.4853 - val_accuracy: 0.9765\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4692 - accuracy: 0.9922 - val_loss: 1.4864 - val_accuracy: 0.9750\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4683 - accuracy: 0.9930 - val_loss: 1.4842 - val_accuracy: 0.9771\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4679 - accuracy: 0.9936 - val_loss: 1.4842 - val_accuracy: 0.9773\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4680 - accuracy: 0.9934 - val_loss: 1.4818 - val_accuracy: 0.9797\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4680 - accuracy: 0.9934 - val_loss: 1.4824 - val_accuracy: 0.9787\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4680 - accuracy: 0.9935 - val_loss: 1.4841 - val_accuracy: 0.9767\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4685 - accuracy: 0.9929 - val_loss: 1.4845 - val_accuracy: 0.9766\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [05:55, 193.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 7\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5806 - accuracy: 0.8947 - val_loss: 1.5342 - val_accuracy: 0.9341\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5205 - accuracy: 0.9463 - val_loss: 1.5226 - val_accuracy: 0.9416\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5107 - accuracy: 0.9549 - val_loss: 1.5181 - val_accuracy: 0.9462\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5046 - accuracy: 0.9607 - val_loss: 1.5155 - val_accuracy: 0.9476\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5004 - accuracy: 0.9641 - val_loss: 1.5145 - val_accuracy: 0.9495\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4971 - accuracy: 0.9673 - val_loss: 1.5121 - val_accuracy: 0.9515\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4943 - accuracy: 0.9702 - val_loss: 1.5119 - val_accuracy: 0.9512\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4920 - accuracy: 0.9721 - val_loss: 1.5110 - val_accuracy: 0.9514\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4903 - accuracy: 0.9738 - val_loss: 1.5086 - val_accuracy: 0.9542\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4885 - accuracy: 0.9753 - val_loss: 1.5104 - val_accuracy: 0.9517\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4872 - accuracy: 0.9768 - val_loss: 1.5089 - val_accuracy: 0.9535\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4857 - accuracy: 0.9777 - val_loss: 1.5091 - val_accuracy: 0.9529\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [08:03, 173.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 7\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7386 - accuracy: 0.7459 - val_loss: 1.6221 - val_accuracy: 0.8541\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6059 - accuracy: 0.8668 - val_loss: 1.5937 - val_accuracy: 0.8765\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5872 - accuracy: 0.8828 - val_loss: 1.5838 - val_accuracy: 0.8858\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5784 - accuracy: 0.8904 - val_loss: 1.5776 - val_accuracy: 0.8901\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5722 - accuracy: 0.8959 - val_loss: 1.5765 - val_accuracy: 0.8885\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5678 - accuracy: 0.8989 - val_loss: 1.5724 - val_accuracy: 0.8927\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5645 - accuracy: 0.9022 - val_loss: 1.5699 - val_accuracy: 0.8940\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5615 - accuracy: 0.9049 - val_loss: 1.5677 - val_accuracy: 0.8957\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5591 - accuracy: 0.9071 - val_loss: 1.5670 - val_accuracy: 0.8980\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5568 - accuracy: 0.9092 - val_loss: 1.5654 - val_accuracy: 0.8987\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5548 - accuracy: 0.9112 - val_loss: 1.5639 - val_accuracy: 0.8985\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5529 - accuracy: 0.9126 - val_loss: 1.5636 - val_accuracy: 0.8983\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5513 - accuracy: 0.9147 - val_loss: 1.5626 - val_accuracy: 0.9002\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5500 - accuracy: 0.9153 - val_loss: 1.5622 - val_accuracy: 0.8996\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5486 - accuracy: 0.9172 - val_loss: 1.5627 - val_accuracy: 0.9016\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5476 - accuracy: 0.9179 - val_loss: 1.5616 - val_accuracy: 0.9007\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5459 - accuracy: 0.9197 - val_loss: 1.5614 - val_accuracy: 0.9032\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5451 - accuracy: 0.9199 - val_loss: 1.5610 - val_accuracy: 0.9012\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5442 - accuracy: 0.9205 - val_loss: 1.5618 - val_accuracy: 0.9000\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5432 - accuracy: 0.9217 - val_loss: 1.5606 - val_accuracy: 0.9023\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5425 - accuracy: 0.9226 - val_loss: 1.5598 - val_accuracy: 0.9012\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5415 - accuracy: 0.9235 - val_loss: 1.5602 - val_accuracy: 0.9020\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5405 - accuracy: 0.9247 - val_loss: 1.5588 - val_accuracy: 0.9027\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5398 - accuracy: 0.9254 - val_loss: 1.5591 - val_accuracy: 0.9029\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5392 - accuracy: 0.9258 - val_loss: 1.5585 - val_accuracy: 0.9029\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5382 - accuracy: 0.9266 - val_loss: 1.5586 - val_accuracy: 0.9036\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5377 - accuracy: 0.9270 - val_loss: 1.5590 - val_accuracy: 0.9029\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5375 - accuracy: 0.9271 - val_loss: 1.5588 - val_accuracy: 0.9028\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [11:00, 174.83s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 7\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9915 - accuracy: 0.4887 - val_loss: 1.8828 - val_accuracy: 0.5950\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8609 - accuracy: 0.6134 - val_loss: 1.8500 - val_accuracy: 0.6205\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8419 - accuracy: 0.6274 - val_loss: 1.8390 - val_accuracy: 0.6300\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8331 - accuracy: 0.6339 - val_loss: 1.8339 - val_accuracy: 0.6330\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8274 - accuracy: 0.6379 - val_loss: 1.8290 - val_accuracy: 0.6365\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8229 - accuracy: 0.6424 - val_loss: 1.8261 - val_accuracy: 0.6379\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8190 - accuracy: 0.6458 - val_loss: 1.8224 - val_accuracy: 0.6440\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8159 - accuracy: 0.6494 - val_loss: 1.8204 - val_accuracy: 0.6433\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8133 - accuracy: 0.6514 - val_loss: 1.8179 - val_accuracy: 0.6442\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8110 - accuracy: 0.6534 - val_loss: 1.8160 - val_accuracy: 0.6470\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8091 - accuracy: 0.6554 - val_loss: 1.8146 - val_accuracy: 0.6477\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8076 - accuracy: 0.6562 - val_loss: 1.8124 - val_accuracy: 0.6494\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8062 - accuracy: 0.6572 - val_loss: 1.8113 - val_accuracy: 0.6513\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8048 - accuracy: 0.6595 - val_loss: 1.8101 - val_accuracy: 0.6529\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8038 - accuracy: 0.6596 - val_loss: 1.8084 - val_accuracy: 0.6540\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8024 - accuracy: 0.6604 - val_loss: 1.8083 - val_accuracy: 0.6534\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8015 - accuracy: 0.6619 - val_loss: 1.8074 - val_accuracy: 0.6545\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8006 - accuracy: 0.6628 - val_loss: 1.8063 - val_accuracy: 0.6545\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7996 - accuracy: 0.6632 - val_loss: 1.8050 - val_accuracy: 0.6570\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7989 - accuracy: 0.6650 - val_loss: 1.8054 - val_accuracy: 0.6559\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7984 - accuracy: 0.6646 - val_loss: 1.8040 - val_accuracy: 0.6581\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7976 - accuracy: 0.6651 - val_loss: 1.8037 - val_accuracy: 0.6578\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7969 - accuracy: 0.6661 - val_loss: 1.8024 - val_accuracy: 0.6596\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7962 - accuracy: 0.6664 - val_loss: 1.8025 - val_accuracy: 0.6589\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7957 - accuracy: 0.6675 - val_loss: 1.8021 - val_accuracy: 0.6598\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7953 - accuracy: 0.6673 - val_loss: 1.8010 - val_accuracy: 0.6599\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7944 - accuracy: 0.6687 - val_loss: 1.8023 - val_accuracy: 0.6602\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7938 - accuracy: 0.6686 - val_loss: 1.8004 - val_accuracy: 0.6605\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7932 - accuracy: 0.6699 - val_loss: 1.7997 - val_accuracy: 0.6603\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7928 - accuracy: 0.6694 - val_loss: 1.7997 - val_accuracy: 0.6617\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7923 - accuracy: 0.6710 - val_loss: 1.7993 - val_accuracy: 0.6631\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7917 - accuracy: 0.6705 - val_loss: 1.7988 - val_accuracy: 0.6631\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7914 - accuracy: 0.6714 - val_loss: 1.7980 - val_accuracy: 0.6638\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7910 - accuracy: 0.6710 - val_loss: 1.7980 - val_accuracy: 0.6622\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7909 - accuracy: 0.6716 - val_loss: 1.7973 - val_accuracy: 0.6639\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7904 - accuracy: 0.6720 - val_loss: 1.7974 - val_accuracy: 0.6632\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7900 - accuracy: 0.6726 - val_loss: 1.7975 - val_accuracy: 0.6634\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7896 - accuracy: 0.6729 - val_loss: 1.7973 - val_accuracy: 0.6640\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [13:44, 171.66s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 7\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2310 - accuracy: 0.2114 - val_loss: 2.1917 - val_accuracy: 0.2418\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1622 - accuracy: 0.3192 - val_loss: 2.1304 - val_accuracy: 0.3446\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1108 - accuracy: 0.3583 - val_loss: 2.0826 - val_accuracy: 0.3850\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0776 - accuracy: 0.3865 - val_loss: 2.0680 - val_accuracy: 0.3974\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0573 - accuracy: 0.4026 - val_loss: 2.0453 - val_accuracy: 0.4208\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0272 - accuracy: 0.4367 - val_loss: 2.0291 - val_accuracy: 0.4361\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0136 - accuracy: 0.4510 - val_loss: 2.0209 - val_accuracy: 0.4474\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0054 - accuracy: 0.4578 - val_loss: 2.0137 - val_accuracy: 0.4505\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9983 - accuracy: 0.4638 - val_loss: 2.0076 - val_accuracy: 0.4559\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9926 - accuracy: 0.4688 - val_loss: 2.0031 - val_accuracy: 0.4604\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9892 - accuracy: 0.4717 - val_loss: 2.0010 - val_accuracy: 0.4596\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9870 - accuracy: 0.4725 - val_loss: 2.0001 - val_accuracy: 0.4605\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9854 - accuracy: 0.4741 - val_loss: 1.9972 - val_accuracy: 0.4627\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9840 - accuracy: 0.4755 - val_loss: 1.9957 - val_accuracy: 0.4641\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9828 - accuracy: 0.4760 - val_loss: 1.9958 - val_accuracy: 0.4662\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9817 - accuracy: 0.4777 - val_loss: 1.9939 - val_accuracy: 0.4653\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9807 - accuracy: 0.4782 - val_loss: 1.9935 - val_accuracy: 0.4644\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9798 - accuracy: 0.4795 - val_loss: 1.9921 - val_accuracy: 0.4659\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9789 - accuracy: 0.4796 - val_loss: 1.9911 - val_accuracy: 0.4668\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9782 - accuracy: 0.4809 - val_loss: 1.9909 - val_accuracy: 0.4685\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9777 - accuracy: 0.4811 - val_loss: 1.9897 - val_accuracy: 0.4679\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9769 - accuracy: 0.4820 - val_loss: 1.9899 - val_accuracy: 0.4698\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9765 - accuracy: 0.4818 - val_loss: 1.9886 - val_accuracy: 0.4700\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9759 - accuracy: 0.4822 - val_loss: 1.9884 - val_accuracy: 0.4718\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9755 - accuracy: 0.4828 - val_loss: 1.9877 - val_accuracy: 0.4695\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9750 - accuracy: 0.4845 - val_loss: 1.9882 - val_accuracy: 0.4715\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9746 - accuracy: 0.4845 - val_loss: 1.9867 - val_accuracy: 0.4737\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9741 - accuracy: 0.4846 - val_loss: 1.9869 - val_accuracy: 0.4742\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9739 - accuracy: 0.4851 - val_loss: 1.9863 - val_accuracy: 0.4713\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9734 - accuracy: 0.4859 - val_loss: 1.9858 - val_accuracy: 0.4707\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9731 - accuracy: 0.4863 - val_loss: 1.9858 - val_accuracy: 0.4716\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9725 - accuracy: 0.4866 - val_loss: 1.9851 - val_accuracy: 0.4716\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9722 - accuracy: 0.4868 - val_loss: 1.9856 - val_accuracy: 0.4742\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9717 - accuracy: 0.4875 - val_loss: 1.9842 - val_accuracy: 0.4750\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9711 - accuracy: 0.4890 - val_loss: 1.9837 - val_accuracy: 0.4740\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9706 - accuracy: 0.4885 - val_loss: 1.9836 - val_accuracy: 0.4750\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9703 - accuracy: 0.4891 - val_loss: 1.9836 - val_accuracy: 0.4753\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9698 - accuracy: 0.4893 - val_loss: 1.9833 - val_accuracy: 0.4756\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9695 - accuracy: 0.4894 - val_loss: 1.9830 - val_accuracy: 0.4760\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9692 - accuracy: 0.4895 - val_loss: 1.9829 - val_accuracy: 0.4782\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9689 - accuracy: 0.4905 - val_loss: 1.9819 - val_accuracy: 0.4778\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9687 - accuracy: 0.4899 - val_loss: 1.9829 - val_accuracy: 0.4766\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9683 - accuracy: 0.4908 - val_loss: 1.9822 - val_accuracy: 0.4772\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9681 - accuracy: 0.4905 - val_loss: 1.9809 - val_accuracy: 0.4767\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9676 - accuracy: 0.4918 - val_loss: 1.9821 - val_accuracy: 0.4777\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9676 - accuracy: 0.4917 - val_loss: 1.9805 - val_accuracy: 0.4783\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9671 - accuracy: 0.4918 - val_loss: 1.9817 - val_accuracy: 0.4798\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9670 - accuracy: 0.4924 - val_loss: 1.9807 - val_accuracy: 0.4803\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9668 - accuracy: 0.4923 - val_loss: 1.9811 - val_accuracy: 0.4797\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [17:01, 179.16s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 7\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2856 - accuracy: 0.1718 - val_loss: 2.2842 - val_accuracy: 0.1718\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2808 - accuracy: 0.1755 - val_loss: 2.2738 - val_accuracy: 0.1812\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2594 - accuracy: 0.1979 - val_loss: 2.2433 - val_accuracy: 0.2182\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2232 - accuracy: 0.2427 - val_loss: 2.2269 - val_accuracy: 0.2317\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2116 - accuracy: 0.2480 - val_loss: 2.2199 - val_accuracy: 0.2337\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2054 - accuracy: 0.2490 - val_loss: 2.2148 - val_accuracy: 0.2348\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1998 - accuracy: 0.2581 - val_loss: 2.2100 - val_accuracy: 0.2496\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1942 - accuracy: 0.2664 - val_loss: 2.2046 - val_accuracy: 0.2525\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1883 - accuracy: 0.2695 - val_loss: 2.1989 - val_accuracy: 0.2542\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1841 - accuracy: 0.2703 - val_loss: 2.1956 - val_accuracy: 0.2558\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1815 - accuracy: 0.2715 - val_loss: 2.1933 - val_accuracy: 0.2573\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1797 - accuracy: 0.2724 - val_loss: 2.1927 - val_accuracy: 0.2572\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1785 - accuracy: 0.2729 - val_loss: 2.1911 - val_accuracy: 0.2579\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1777 - accuracy: 0.2730 - val_loss: 2.1908 - val_accuracy: 0.2581\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1768 - accuracy: 0.2736 - val_loss: 2.1898 - val_accuracy: 0.2584\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1763 - accuracy: 0.2736 - val_loss: 2.1894 - val_accuracy: 0.2584\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1757 - accuracy: 0.2739 - val_loss: 2.1889 - val_accuracy: 0.2584\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1750 - accuracy: 0.2743 - val_loss: 2.1880 - val_accuracy: 0.2599\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1744 - accuracy: 0.2749 - val_loss: 2.1871 - val_accuracy: 0.2606\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1739 - accuracy: 0.2753 - val_loss: 2.1867 - val_accuracy: 0.2608\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1736 - accuracy: 0.2756 - val_loss: 2.1877 - val_accuracy: 0.2614\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1732 - accuracy: 0.2758 - val_loss: 2.1863 - val_accuracy: 0.2608\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1727 - accuracy: 0.2757 - val_loss: 2.1857 - val_accuracy: 0.2622\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1723 - accuracy: 0.2763 - val_loss: 2.1862 - val_accuracy: 0.2615\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1722 - accuracy: 0.2763 - val_loss: 2.1854 - val_accuracy: 0.2612\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1719 - accuracy: 0.2767 - val_loss: 2.1849 - val_accuracy: 0.2616\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1718 - accuracy: 0.2766 - val_loss: 2.1851 - val_accuracy: 0.2621\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1717 - accuracy: 0.2767 - val_loss: 2.1846 - val_accuracy: 0.2619\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1715 - accuracy: 0.2769 - val_loss: 2.1845 - val_accuracy: 0.2622\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1714 - accuracy: 0.2769 - val_loss: 2.1844 - val_accuracy: 0.2625\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1710 - accuracy: 0.2772 - val_loss: 2.1847 - val_accuracy: 0.2624\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1710 - accuracy: 0.2771 - val_loss: 2.1846 - val_accuracy: 0.2619\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1708 - accuracy: 0.2774 - val_loss: 2.1841 - val_accuracy: 0.2625\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1707 - accuracy: 0.2773 - val_loss: 2.1840 - val_accuracy: 0.2624\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1705 - accuracy: 0.2776 - val_loss: 2.1849 - val_accuracy: 0.2631\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1706 - accuracy: 0.2776 - val_loss: 2.1838 - val_accuracy: 0.2626\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1703 - accuracy: 0.2777 - val_loss: 2.1838 - val_accuracy: 0.2629\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1703 - accuracy: 0.2777 - val_loss: 2.1837 - val_accuracy: 0.2623\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1703 - accuracy: 0.2778 - val_loss: 2.1836 - val_accuracy: 0.2629\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1700 - accuracy: 0.2780 - val_loss: 2.1835 - val_accuracy: 0.2628\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1701 - accuracy: 0.2779 - val_loss: 2.1840 - val_accuracy: 0.2628\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1700 - accuracy: 0.2779 - val_loss: 2.1833 - val_accuracy: 0.2630\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1699 - accuracy: 0.2780 - val_loss: 2.1832 - val_accuracy: 0.2633\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1698 - accuracy: 0.2781 - val_loss: 2.1837 - val_accuracy: 0.2632\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1699 - accuracy: 0.2780 - val_loss: 2.1831 - val_accuracy: 0.2634\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1697 - accuracy: 0.2781 - val_loss: 2.1832 - val_accuracy: 0.2631\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1696 - accuracy: 0.2780 - val_loss: 2.1834 - val_accuracy: 0.2640\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1696 - accuracy: 0.2782 - val_loss: 2.1829 - val_accuracy: 0.2633\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1695 - accuracy: 0.2783 - val_loss: 2.1832 - val_accuracy: 0.2637\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1695 - accuracy: 0.2785 - val_loss: 2.1828 - val_accuracy: 0.2638\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1695 - accuracy: 0.2784 - val_loss: 2.1828 - val_accuracy: 0.2641\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1694 - accuracy: 0.2783 - val_loss: 2.1829 - val_accuracy: 0.2642\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1693 - accuracy: 0.2782 - val_loss: 2.1828 - val_accuracy: 0.2643\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1693 - accuracy: 0.2786 - val_loss: 2.1831 - val_accuracy: 0.2640\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1692 - accuracy: 0.2786 - val_loss: 2.1828 - val_accuracy: 0.2643\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1691 - accuracy: 0.2786 - val_loss: 2.1829 - val_accuracy: 0.2641\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [19:34, 167.77s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [2:26:50<36:30, 1095.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5744 - accuracy: 0.9006 - val_loss: 1.5157 - val_accuracy: 0.9491\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 8\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5104 - accuracy: 0.9532 - val_loss: 1.5031 - val_accuracy: 0.9597\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4977 - accuracy: 0.9653 - val_loss: 1.5021 - val_accuracy: 0.9615\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4904 - accuracy: 0.9718 - val_loss: 1.4907 - val_accuracy: 0.9719\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4858 - accuracy: 0.9761 - val_loss: 1.4960 - val_accuracy: 0.9659\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4828 - accuracy: 0.9790 - val_loss: 1.4871 - val_accuracy: 0.9741\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4800 - accuracy: 0.9819 - val_loss: 1.4900 - val_accuracy: 0.9710\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4797 - accuracy: 0.9821 - val_loss: 1.4863 - val_accuracy: 0.9750\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4781 - accuracy: 0.9834 - val_loss: 1.4878 - val_accuracy: 0.9733\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4770 - accuracy: 0.9846 - val_loss: 1.4838 - val_accuracy: 0.9769\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4755 - accuracy: 0.9859 - val_loss: 1.4849 - val_accuracy: 0.9765\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4753 - accuracy: 0.9861 - val_loss: 1.4846 - val_accuracy: 0.9771\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4739 - accuracy: 0.9874 - val_loss: 1.4824 - val_accuracy: 0.9790\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4738 - accuracy: 0.9875 - val_loss: 1.4881 - val_accuracy: 0.9726\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4741 - accuracy: 0.9872 - val_loss: 1.4845 - val_accuracy: 0.9766\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4730 - accuracy: 0.9882 - val_loss: 1.4872 - val_accuracy: 0.9740\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:33, 153.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 8\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4895 - accuracy: 0.9739 - val_loss: 1.4903 - val_accuracy: 0.9715\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4787 - accuracy: 0.9839 - val_loss: 1.4868 - val_accuracy: 0.9760\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9867 - val_loss: 1.4879 - val_accuracy: 0.9739\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4741 - accuracy: 0.9880 - val_loss: 1.4866 - val_accuracy: 0.9756\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4734 - accuracy: 0.9885 - val_loss: 1.4854 - val_accuracy: 0.9760\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4718 - accuracy: 0.9901 - val_loss: 1.4844 - val_accuracy: 0.9775\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4715 - accuracy: 0.9904 - val_loss: 1.4865 - val_accuracy: 0.9751\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4710 - accuracy: 0.9906 - val_loss: 1.4858 - val_accuracy: 0.9756\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4702 - accuracy: 0.9914 - val_loss: 1.4850 - val_accuracy: 0.9760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:27, 141.36s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 8\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5533 - accuracy: 0.9253 - val_loss: 1.5241 - val_accuracy: 0.9442\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5100 - accuracy: 0.9569 - val_loss: 1.5122 - val_accuracy: 0.9534\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5013 - accuracy: 0.9645 - val_loss: 1.5073 - val_accuracy: 0.9573\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4962 - accuracy: 0.9689 - val_loss: 1.5060 - val_accuracy: 0.9570\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4921 - accuracy: 0.9730 - val_loss: 1.5032 - val_accuracy: 0.9606\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4895 - accuracy: 0.9751 - val_loss: 1.5022 - val_accuracy: 0.9604\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4872 - accuracy: 0.9769 - val_loss: 1.5017 - val_accuracy: 0.9611\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4850 - accuracy: 0.9790 - val_loss: 1.5007 - val_accuracy: 0.9619\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9801 - val_loss: 1.5004 - val_accuracy: 0.9633\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4826 - accuracy: 0.9809 - val_loss: 1.4999 - val_accuracy: 0.9626\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4812 - accuracy: 0.9822 - val_loss: 1.5003 - val_accuracy: 0.9618\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4801 - accuracy: 0.9831 - val_loss: 1.5004 - val_accuracy: 0.9618\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4793 - accuracy: 0.9839 - val_loss: 1.4997 - val_accuracy: 0.9629\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4788 - accuracy: 0.9843 - val_loss: 1.4997 - val_accuracy: 0.9614\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4779 - accuracy: 0.9849 - val_loss: 1.4996 - val_accuracy: 0.9630\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4770 - accuracy: 0.9858 - val_loss: 1.5028 - val_accuracy: 0.9590\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4767 - accuracy: 0.9857 - val_loss: 1.4989 - val_accuracy: 0.9637\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4761 - accuracy: 0.9864 - val_loss: 1.4986 - val_accuracy: 0.9631\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4756 - accuracy: 0.9868 - val_loss: 1.4994 - val_accuracy: 0.9627\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4753 - accuracy: 0.9869 - val_loss: 1.4993 - val_accuracy: 0.9624\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4751 - accuracy: 0.9873 - val_loss: 1.4991 - val_accuracy: 0.9632\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [06:51, 142.37s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 8\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6901 - accuracy: 0.7934 - val_loss: 1.6092 - val_accuracy: 0.8617\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5919 - accuracy: 0.8796 - val_loss: 1.5877 - val_accuracy: 0.8807\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5766 - accuracy: 0.8922 - val_loss: 1.5782 - val_accuracy: 0.8885\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5687 - accuracy: 0.8989 - val_loss: 1.5741 - val_accuracy: 0.8919\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5634 - accuracy: 0.9039 - val_loss: 1.5695 - val_accuracy: 0.8957\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5592 - accuracy: 0.9077 - val_loss: 1.5676 - val_accuracy: 0.8961\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5562 - accuracy: 0.9094 - val_loss: 1.5640 - val_accuracy: 0.9010\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5538 - accuracy: 0.9125 - val_loss: 1.5628 - val_accuracy: 0.9024\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5513 - accuracy: 0.9147 - val_loss: 1.5610 - val_accuracy: 0.9042\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5494 - accuracy: 0.9165 - val_loss: 1.5602 - val_accuracy: 0.9040\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5474 - accuracy: 0.9180 - val_loss: 1.5604 - val_accuracy: 0.9042\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5462 - accuracy: 0.9190 - val_loss: 1.5597 - val_accuracy: 0.9036\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5446 - accuracy: 0.9211 - val_loss: 1.5587 - val_accuracy: 0.9046\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5436 - accuracy: 0.9218 - val_loss: 1.5578 - val_accuracy: 0.9056\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5424 - accuracy: 0.9227 - val_loss: 1.5577 - val_accuracy: 0.9050\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5413 - accuracy: 0.9236 - val_loss: 1.5558 - val_accuracy: 0.9070\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5403 - accuracy: 0.9245 - val_loss: 1.5564 - val_accuracy: 0.9062\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5397 - accuracy: 0.9253 - val_loss: 1.5561 - val_accuracy: 0.9063\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5387 - accuracy: 0.9262 - val_loss: 1.5550 - val_accuracy: 0.9079\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5380 - accuracy: 0.9268 - val_loss: 1.5565 - val_accuracy: 0.9061\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5374 - accuracy: 0.9274 - val_loss: 1.5541 - val_accuracy: 0.9078\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5366 - accuracy: 0.9280 - val_loss: 1.5551 - val_accuracy: 0.9069\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5357 - accuracy: 0.9286 - val_loss: 1.5554 - val_accuracy: 0.9069\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5351 - accuracy: 0.9293 - val_loss: 1.5548 - val_accuracy: 0.9073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:20, 144.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 8\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9898 - accuracy: 0.4854 - val_loss: 1.9058 - val_accuracy: 0.5673\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8968 - accuracy: 0.5709 - val_loss: 1.8758 - val_accuracy: 0.5879\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8344 - accuracy: 0.6351 - val_loss: 1.8067 - val_accuracy: 0.6616\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8040 - accuracy: 0.6635 - val_loss: 1.7936 - val_accuracy: 0.6724\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7930 - accuracy: 0.6740 - val_loss: 1.7853 - val_accuracy: 0.6822\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7810 - accuracy: 0.6858 - val_loss: 1.7760 - val_accuracy: 0.6902\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7745 - accuracy: 0.6911 - val_loss: 1.7727 - val_accuracy: 0.6911\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7702 - accuracy: 0.6954 - val_loss: 1.7709 - val_accuracy: 0.6911\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7670 - accuracy: 0.6988 - val_loss: 1.7680 - val_accuracy: 0.6956\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7644 - accuracy: 0.7005 - val_loss: 1.7655 - val_accuracy: 0.6994\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7618 - accuracy: 0.7031 - val_loss: 1.7648 - val_accuracy: 0.6979\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7596 - accuracy: 0.7053 - val_loss: 1.7634 - val_accuracy: 0.7003\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7578 - accuracy: 0.7066 - val_loss: 1.7619 - val_accuracy: 0.7027\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7562 - accuracy: 0.7083 - val_loss: 1.7624 - val_accuracy: 0.7009\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7546 - accuracy: 0.7101 - val_loss: 1.7611 - val_accuracy: 0.7021\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7533 - accuracy: 0.7107 - val_loss: 1.7586 - val_accuracy: 0.7043\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7520 - accuracy: 0.7128 - val_loss: 1.7589 - val_accuracy: 0.7037\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7510 - accuracy: 0.7131 - val_loss: 1.7580 - val_accuracy: 0.7042\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7501 - accuracy: 0.7144 - val_loss: 1.7584 - val_accuracy: 0.7041\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7490 - accuracy: 0.7157 - val_loss: 1.7569 - val_accuracy: 0.7059\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7483 - accuracy: 0.7160 - val_loss: 1.7560 - val_accuracy: 0.7056\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7474 - accuracy: 0.7167 - val_loss: 1.7545 - val_accuracy: 0.7090\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7467 - accuracy: 0.7178 - val_loss: 1.7540 - val_accuracy: 0.7086\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7459 - accuracy: 0.7184 - val_loss: 1.7539 - val_accuracy: 0.7059\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7449 - accuracy: 0.7191 - val_loss: 1.7532 - val_accuracy: 0.7094\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7439 - accuracy: 0.7196 - val_loss: 1.7514 - val_accuracy: 0.7108\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7429 - accuracy: 0.7209 - val_loss: 1.7525 - val_accuracy: 0.7097\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7419 - accuracy: 0.7212 - val_loss: 1.7513 - val_accuracy: 0.7105\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7413 - accuracy: 0.7215 - val_loss: 1.7513 - val_accuracy: 0.7110\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7408 - accuracy: 0.7224 - val_loss: 1.7505 - val_accuracy: 0.7125\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7400 - accuracy: 0.7231 - val_loss: 1.7501 - val_accuracy: 0.7129\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7392 - accuracy: 0.7240 - val_loss: 1.7498 - val_accuracy: 0.7132\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7391 - accuracy: 0.7239 - val_loss: 1.7486 - val_accuracy: 0.7146\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7382 - accuracy: 0.7255 - val_loss: 1.7504 - val_accuracy: 0.7092\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7377 - accuracy: 0.7257 - val_loss: 1.7485 - val_accuracy: 0.7126\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7373 - accuracy: 0.7258 - val_loss: 1.7480 - val_accuracy: 0.7152\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7371 - accuracy: 0.7260 - val_loss: 1.7489 - val_accuracy: 0.7127\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7363 - accuracy: 0.7270 - val_loss: 1.7485 - val_accuracy: 0.7124\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7360 - accuracy: 0.7273 - val_loss: 1.7495 - val_accuracy: 0.7106\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:27, 157.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 8\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0770 - accuracy: 0.3894 - val_loss: 2.0552 - val_accuracy: 0.4057\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0068 - accuracy: 0.4615 - val_loss: 2.0024 - val_accuracy: 0.4657\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9774 - accuracy: 0.4910 - val_loss: 1.9864 - val_accuracy: 0.4790\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9626 - accuracy: 0.5039 - val_loss: 1.9731 - val_accuracy: 0.4895\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9518 - accuracy: 0.5128 - val_loss: 1.9615 - val_accuracy: 0.5026\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9395 - accuracy: 0.5264 - val_loss: 1.9470 - val_accuracy: 0.5182\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9275 - accuracy: 0.5375 - val_loss: 1.9397 - val_accuracy: 0.5233\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9216 - accuracy: 0.5429 - val_loss: 1.9361 - val_accuracy: 0.5251\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9178 - accuracy: 0.5457 - val_loss: 1.9336 - val_accuracy: 0.5272\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9153 - accuracy: 0.5478 - val_loss: 1.9316 - val_accuracy: 0.5300\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9133 - accuracy: 0.5490 - val_loss: 1.9303 - val_accuracy: 0.5311\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9117 - accuracy: 0.5508 - val_loss: 1.9288 - val_accuracy: 0.5327\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9099 - accuracy: 0.5524 - val_loss: 1.9274 - val_accuracy: 0.5330\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9083 - accuracy: 0.5544 - val_loss: 1.9255 - val_accuracy: 0.5354\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9069 - accuracy: 0.5558 - val_loss: 1.9237 - val_accuracy: 0.5388\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9059 - accuracy: 0.5566 - val_loss: 1.9232 - val_accuracy: 0.5380\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9048 - accuracy: 0.5575 - val_loss: 1.9219 - val_accuracy: 0.5385\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9039 - accuracy: 0.5584 - val_loss: 1.9216 - val_accuracy: 0.5381\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9031 - accuracy: 0.5595 - val_loss: 1.9207 - val_accuracy: 0.5398\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9024 - accuracy: 0.5601 - val_loss: 1.9198 - val_accuracy: 0.5419\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9016 - accuracy: 0.5602 - val_loss: 1.9193 - val_accuracy: 0.5418\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9009 - accuracy: 0.5613 - val_loss: 1.9185 - val_accuracy: 0.5425\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.9004 - accuracy: 0.5612 - val_loss: 1.9177 - val_accuracy: 0.5452\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8995 - accuracy: 0.5625 - val_loss: 1.9175 - val_accuracy: 0.5435\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8990 - accuracy: 0.5629 - val_loss: 1.9159 - val_accuracy: 0.5464\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8986 - accuracy: 0.5633 - val_loss: 1.9162 - val_accuracy: 0.5458\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8982 - accuracy: 0.5639 - val_loss: 1.9153 - val_accuracy: 0.5469\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8976 - accuracy: 0.5642 - val_loss: 1.9159 - val_accuracy: 0.5459\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8973 - accuracy: 0.5646 - val_loss: 1.9162 - val_accuracy: 0.5452\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8969 - accuracy: 0.5646 - val_loss: 1.9151 - val_accuracy: 0.5449\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8966 - accuracy: 0.5654 - val_loss: 1.9146 - val_accuracy: 0.5465\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8963 - accuracy: 0.5655 - val_loss: 1.9140 - val_accuracy: 0.5475\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8959 - accuracy: 0.5661 - val_loss: 1.9134 - val_accuracy: 0.5475\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8956 - accuracy: 0.5663 - val_loss: 1.9131 - val_accuracy: 0.5476\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8951 - accuracy: 0.5667 - val_loss: 1.9121 - val_accuracy: 0.5489\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8950 - accuracy: 0.5671 - val_loss: 1.9117 - val_accuracy: 0.5494\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8947 - accuracy: 0.5672 - val_loss: 1.9129 - val_accuracy: 0.5471\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8944 - accuracy: 0.5672 - val_loss: 1.9125 - val_accuracy: 0.5485\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8941 - accuracy: 0.5676 - val_loss: 1.9116 - val_accuracy: 0.5489\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8938 - accuracy: 0.5677 - val_loss: 1.9120 - val_accuracy: 0.5477\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8935 - accuracy: 0.5682 - val_loss: 1.9117 - val_accuracy: 0.5485\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8930 - accuracy: 0.5691 - val_loss: 1.9106 - val_accuracy: 0.5498\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8928 - accuracy: 0.5688 - val_loss: 1.9107 - val_accuracy: 0.5515\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8925 - accuracy: 0.5695 - val_loss: 1.9099 - val_accuracy: 0.5512\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8925 - accuracy: 0.5686 - val_loss: 1.9093 - val_accuracy: 0.5519\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8922 - accuracy: 0.5701 - val_loss: 1.9105 - val_accuracy: 0.5500\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8920 - accuracy: 0.5697 - val_loss: 1.9101 - val_accuracy: 0.5500\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8919 - accuracy: 0.5699 - val_loss: 1.9093 - val_accuracy: 0.5511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [16:03, 174.69s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 8\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2621 - accuracy: 0.1784 - val_loss: 2.2385 - val_accuracy: 0.1893\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1894 - accuracy: 0.2763 - val_loss: 2.1743 - val_accuracy: 0.2915\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1591 - accuracy: 0.2992 - val_loss: 2.1644 - val_accuracy: 0.2941\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1535 - accuracy: 0.3035 - val_loss: 2.1608 - val_accuracy: 0.2981\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1381 - accuracy: 0.3170 - val_loss: 2.1384 - val_accuracy: 0.3126\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1234 - accuracy: 0.3375 - val_loss: 2.1297 - val_accuracy: 0.3339\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1167 - accuracy: 0.3458 - val_loss: 2.1243 - val_accuracy: 0.3345\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1123 - accuracy: 0.3455 - val_loss: 2.1207 - val_accuracy: 0.3349\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1088 - accuracy: 0.3470 - val_loss: 2.1171 - val_accuracy: 0.3380\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1057 - accuracy: 0.3493 - val_loss: 2.1143 - val_accuracy: 0.3392\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1032 - accuracy: 0.3510 - val_loss: 2.1121 - val_accuracy: 0.3408\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1013 - accuracy: 0.3530 - val_loss: 2.1103 - val_accuracy: 0.3436\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0999 - accuracy: 0.3557 - val_loss: 2.1090 - val_accuracy: 0.3440\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0986 - accuracy: 0.3565 - val_loss: 2.1079 - val_accuracy: 0.3458\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0976 - accuracy: 0.3576 - val_loss: 2.1070 - val_accuracy: 0.3471\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0965 - accuracy: 0.3586 - val_loss: 2.1057 - val_accuracy: 0.3483\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0955 - accuracy: 0.3593 - val_loss: 2.1049 - val_accuracy: 0.3481\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0947 - accuracy: 0.3597 - val_loss: 2.1041 - val_accuracy: 0.3494\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0939 - accuracy: 0.3603 - val_loss: 2.1034 - val_accuracy: 0.3493\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0931 - accuracy: 0.3606 - val_loss: 2.1028 - val_accuracy: 0.3504\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0925 - accuracy: 0.3610 - val_loss: 2.1021 - val_accuracy: 0.3505\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0919 - accuracy: 0.3616 - val_loss: 2.1016 - val_accuracy: 0.3511\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0912 - accuracy: 0.3622 - val_loss: 2.1010 - val_accuracy: 0.3520\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0907 - accuracy: 0.3623 - val_loss: 2.1005 - val_accuracy: 0.3524\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0902 - accuracy: 0.3628 - val_loss: 2.1000 - val_accuracy: 0.3526\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0897 - accuracy: 0.3629 - val_loss: 2.0995 - val_accuracy: 0.3522\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0892 - accuracy: 0.3634 - val_loss: 2.0989 - val_accuracy: 0.3536\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0887 - accuracy: 0.3641 - val_loss: 2.0980 - val_accuracy: 0.3543\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0882 - accuracy: 0.3649 - val_loss: 2.0975 - val_accuracy: 0.3542\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0878 - accuracy: 0.3648 - val_loss: 2.0969 - val_accuracy: 0.3551\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0874 - accuracy: 0.3650 - val_loss: 2.0965 - val_accuracy: 0.3562\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0870 - accuracy: 0.3653 - val_loss: 2.0961 - val_accuracy: 0.3567\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0866 - accuracy: 0.3658 - val_loss: 2.0956 - val_accuracy: 0.3569\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0863 - accuracy: 0.3660 - val_loss: 2.0956 - val_accuracy: 0.3568\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0860 - accuracy: 0.3663 - val_loss: 2.0955 - val_accuracy: 0.3557\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0857 - accuracy: 0.3667 - val_loss: 2.0949 - val_accuracy: 0.3580\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0854 - accuracy: 0.3668 - val_loss: 2.0947 - val_accuracy: 0.3582\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0852 - accuracy: 0.3671 - val_loss: 2.0942 - val_accuracy: 0.3583\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0850 - accuracy: 0.3673 - val_loss: 2.0939 - val_accuracy: 0.3584\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0847 - accuracy: 0.3677 - val_loss: 2.0937 - val_accuracy: 0.3583\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0845 - accuracy: 0.3677 - val_loss: 2.0935 - val_accuracy: 0.3582\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0843 - accuracy: 0.3681 - val_loss: 2.0935 - val_accuracy: 0.3589\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0841 - accuracy: 0.3683 - val_loss: 2.0931 - val_accuracy: 0.3598\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0838 - accuracy: 0.3683 - val_loss: 2.0931 - val_accuracy: 0.3583\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0837 - accuracy: 0.3685 - val_loss: 2.0927 - val_accuracy: 0.3598\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0834 - accuracy: 0.3685 - val_loss: 2.0927 - val_accuracy: 0.3579\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0833 - accuracy: 0.3695 - val_loss: 2.0925 - val_accuracy: 0.3589\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0830 - accuracy: 0.3690 - val_loss: 2.0923 - val_accuracy: 0.3583\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0829 - accuracy: 0.3695 - val_loss: 2.0922 - val_accuracy: 0.3591\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0828 - accuracy: 0.3696 - val_loss: 2.0921 - val_accuracy: 0.3581\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0826 - accuracy: 0.3696 - val_loss: 2.0923 - val_accuracy: 0.3582\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0825 - accuracy: 0.3697 - val_loss: 2.0928 - val_accuracy: 0.3584\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0823 - accuracy: 0.3700 - val_loss: 2.0916 - val_accuracy: 0.3596\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0821 - accuracy: 0.3701 - val_loss: 2.0913 - val_accuracy: 0.3594\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0820 - accuracy: 0.3703 - val_loss: 2.0910 - val_accuracy: 0.3599\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0818 - accuracy: 0.3704 - val_loss: 2.0911 - val_accuracy: 0.3601\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0816 - accuracy: 0.3705 - val_loss: 2.0910 - val_accuracy: 0.3599\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0814 - accuracy: 0.3707 - val_loss: 2.0908 - val_accuracy: 0.3605\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0813 - accuracy: 0.3710 - val_loss: 2.0904 - val_accuracy: 0.3612\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0812 - accuracy: 0.3706 - val_loss: 2.0904 - val_accuracy: 0.3607\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0811 - accuracy: 0.3708 - val_loss: 2.0902 - val_accuracy: 0.3609\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0810 - accuracy: 0.3707 - val_loss: 2.0900 - val_accuracy: 0.3616\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0809 - accuracy: 0.3707 - val_loss: 2.0903 - val_accuracy: 0.3595\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0806 - accuracy: 0.3712 - val_loss: 2.0896 - val_accuracy: 0.3609\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0806 - accuracy: 0.3713 - val_loss: 2.0900 - val_accuracy: 0.3595\n",
      "Epoch 66/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0804 - accuracy: 0.3711 - val_loss: 2.0892 - val_accuracy: 0.3617\n",
      "Epoch 67/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0804 - accuracy: 0.3713 - val_loss: 2.0893 - val_accuracy: 0.3620\n",
      "Epoch 68/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0802 - accuracy: 0.3715 - val_loss: 2.0895 - val_accuracy: 0.3616\n",
      "Epoch 69/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0801 - accuracy: 0.3715 - val_loss: 2.0891 - val_accuracy: 0.3619\n",
      "Epoch 70/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0800 - accuracy: 0.3714 - val_loss: 2.0898 - val_accuracy: 0.3601\n",
      "Epoch 71/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0798 - accuracy: 0.3717 - val_loss: 2.0892 - val_accuracy: 0.3622\n",
      "Epoch 72/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0798 - accuracy: 0.3715 - val_loss: 2.0893 - val_accuracy: 0.3620\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [19:23, 166.24s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [2:46:16<18:36, 1116.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5809 - accuracy: 0.8941 - val_loss: 1.5194 - val_accuracy: 0.9456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 9\n",
      "rows to prune in layer 0 : 0\n",
      "rows to prune in layer 3 : 0\n",
      "rows to prune in layer 6 : 0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5141 - accuracy: 0.9497 - val_loss: 1.5025 - val_accuracy: 0.9604\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5003 - accuracy: 0.9621 - val_loss: 1.5020 - val_accuracy: 0.9595\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4921 - accuracy: 0.9704 - val_loss: 1.4933 - val_accuracy: 0.9691\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4877 - accuracy: 0.9743 - val_loss: 1.4957 - val_accuracy: 0.9671\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4846 - accuracy: 0.9774 - val_loss: 1.4894 - val_accuracy: 0.9723\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4816 - accuracy: 0.9803 - val_loss: 1.4861 - val_accuracy: 0.9754\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4802 - accuracy: 0.9817 - val_loss: 1.4867 - val_accuracy: 0.9743\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4790 - accuracy: 0.9825 - val_loss: 1.4860 - val_accuracy: 0.9763\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4767 - accuracy: 0.9848 - val_loss: 1.4890 - val_accuracy: 0.9725\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4755 - accuracy: 0.9860 - val_loss: 1.4838 - val_accuracy: 0.9772\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9857 - val_loss: 1.4855 - val_accuracy: 0.9761\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4738 - accuracy: 0.9875 - val_loss: 1.4825 - val_accuracy: 0.9787\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4733 - accuracy: 0.9882 - val_loss: 1.4828 - val_accuracy: 0.9785\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4734 - accuracy: 0.9880 - val_loss: 1.4814 - val_accuracy: 0.9801\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4729 - accuracy: 0.9883 - val_loss: 1.4828 - val_accuracy: 0.9786\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4719 - accuracy: 0.9895 - val_loss: 1.4842 - val_accuracy: 0.9770\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4718 - accuracy: 0.9895 - val_loss: 1.4827 - val_accuracy: 0.9785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:59, 179.70s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 9\n",
      "rows to prune in layer 0 : 392\n",
      "rows to prune in layer 3 : 150\n",
      "rows to prune in layer 6 : 50\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4883 - accuracy: 0.9746 - val_loss: 1.4900 - val_accuracy: 0.9722\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4780 - accuracy: 0.9843 - val_loss: 1.4898 - val_accuracy: 0.9721\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4747 - accuracy: 0.9875 - val_loss: 1.4860 - val_accuracy: 0.9755\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4726 - accuracy: 0.9897 - val_loss: 1.4847 - val_accuracy: 0.9767\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4714 - accuracy: 0.9906 - val_loss: 1.4841 - val_accuracy: 0.9776\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4707 - accuracy: 0.9912 - val_loss: 1.4875 - val_accuracy: 0.9742\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4701 - accuracy: 0.9917 - val_loss: 1.4853 - val_accuracy: 0.9758\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4695 - accuracy: 0.9923 - val_loss: 1.4850 - val_accuracy: 0.9761\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:52, 159.61s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 9\n",
      "rows to prune in layer 0 : 588\n",
      "rows to prune in layer 3 : 225\n",
      "rows to prune in layer 6 : 75\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5566 - accuracy: 0.9232 - val_loss: 1.5226 - val_accuracy: 0.9455\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5119 - accuracy: 0.9550 - val_loss: 1.5145 - val_accuracy: 0.9504\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5039 - accuracy: 0.9625 - val_loss: 1.5100 - val_accuracy: 0.9555\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4986 - accuracy: 0.9665 - val_loss: 1.5098 - val_accuracy: 0.9535\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4953 - accuracy: 0.9695 - val_loss: 1.5068 - val_accuracy: 0.9574\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4923 - accuracy: 0.9722 - val_loss: 1.5059 - val_accuracy: 0.9572\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4900 - accuracy: 0.9743 - val_loss: 1.5064 - val_accuracy: 0.9563\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4882 - accuracy: 0.9757 - val_loss: 1.5045 - val_accuracy: 0.9582\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4866 - accuracy: 0.9774 - val_loss: 1.5028 - val_accuracy: 0.9597\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4853 - accuracy: 0.9782 - val_loss: 1.5020 - val_accuracy: 0.9602\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4837 - accuracy: 0.9797 - val_loss: 1.5024 - val_accuracy: 0.9596\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4831 - accuracy: 0.9802 - val_loss: 1.5034 - val_accuracy: 0.9581\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4818 - accuracy: 0.9815 - val_loss: 1.5015 - val_accuracy: 0.9607\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4808 - accuracy: 0.9824 - val_loss: 1.5013 - val_accuracy: 0.9608\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4804 - accuracy: 0.9825 - val_loss: 1.5018 - val_accuracy: 0.9596\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4797 - accuracy: 0.9829 - val_loss: 1.5024 - val_accuracy: 0.9589\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4788 - accuracy: 0.9840 - val_loss: 1.5003 - val_accuracy: 0.9619\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4787 - accuracy: 0.9840 - val_loss: 1.5026 - val_accuracy: 0.9584\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4779 - accuracy: 0.9846 - val_loss: 1.5014 - val_accuracy: 0.9596\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4777 - accuracy: 0.9848 - val_loss: 1.5017 - val_accuracy: 0.9600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [07:10, 153.11s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 9\n",
      "rows to prune in layer 0 : 686\n",
      "rows to prune in layer 3 : 262\n",
      "rows to prune in layer 6 : 87\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7285 - accuracy: 0.7578 - val_loss: 1.6389 - val_accuracy: 0.8350\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6276 - accuracy: 0.8450 - val_loss: 1.6123 - val_accuracy: 0.8576\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6087 - accuracy: 0.8610 - val_loss: 1.6018 - val_accuracy: 0.8668\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5986 - accuracy: 0.8701 - val_loss: 1.5967 - val_accuracy: 0.8703\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5917 - accuracy: 0.8771 - val_loss: 1.5919 - val_accuracy: 0.8751\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5873 - accuracy: 0.8805 - val_loss: 1.5889 - val_accuracy: 0.8772\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5830 - accuracy: 0.8845 - val_loss: 1.5890 - val_accuracy: 0.8782\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5802 - accuracy: 0.8864 - val_loss: 1.5847 - val_accuracy: 0.8797\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5776 - accuracy: 0.8887 - val_loss: 1.5851 - val_accuracy: 0.8801\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5751 - accuracy: 0.8911 - val_loss: 1.5835 - val_accuracy: 0.8812\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5731 - accuracy: 0.8935 - val_loss: 1.5820 - val_accuracy: 0.8821\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5712 - accuracy: 0.8953 - val_loss: 1.5819 - val_accuracy: 0.8839\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5700 - accuracy: 0.8958 - val_loss: 1.5804 - val_accuracy: 0.8842\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5684 - accuracy: 0.8973 - val_loss: 1.5802 - val_accuracy: 0.8843\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5669 - accuracy: 0.8992 - val_loss: 1.5800 - val_accuracy: 0.8825\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5658 - accuracy: 0.8997 - val_loss: 1.5793 - val_accuracy: 0.8841\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5650 - accuracy: 0.9004 - val_loss: 1.5782 - val_accuracy: 0.8856\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5638 - accuracy: 0.9014 - val_loss: 1.5791 - val_accuracy: 0.8839\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5625 - accuracy: 0.9031 - val_loss: 1.5773 - val_accuracy: 0.8840\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5617 - accuracy: 0.9038 - val_loss: 1.5765 - val_accuracy: 0.8861\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5609 - accuracy: 0.9046 - val_loss: 1.5767 - val_accuracy: 0.8852\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5600 - accuracy: 0.9054 - val_loss: 1.5773 - val_accuracy: 0.8848\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5593 - accuracy: 0.9054 - val_loss: 1.5769 - val_accuracy: 0.8848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [09:52, 155.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 9\n",
      "rows to prune in layer 0 : 735\n",
      "rows to prune in layer 3 : 281\n",
      "rows to prune in layer 6 : 93\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0082 - accuracy: 0.4649 - val_loss: 1.9019 - val_accuracy: 0.5661\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8801 - accuracy: 0.5899 - val_loss: 1.8568 - val_accuracy: 0.6157\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8514 - accuracy: 0.6152 - val_loss: 1.8400 - val_accuracy: 0.6273\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8373 - accuracy: 0.6278 - val_loss: 1.8193 - val_accuracy: 0.6525\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.8045 - accuracy: 0.6686 - val_loss: 1.7791 - val_accuracy: 0.6938\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7789 - accuracy: 0.6916 - val_loss: 1.7650 - val_accuracy: 0.7043\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7683 - accuracy: 0.7008 - val_loss: 1.7579 - val_accuracy: 0.7108\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7617 - accuracy: 0.7060 - val_loss: 1.7521 - val_accuracy: 0.7146\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7569 - accuracy: 0.7099 - val_loss: 1.7486 - val_accuracy: 0.7192\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7532 - accuracy: 0.7127 - val_loss: 1.7458 - val_accuracy: 0.7180\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7504 - accuracy: 0.7153 - val_loss: 1.7437 - val_accuracy: 0.7211\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7477 - accuracy: 0.7182 - val_loss: 1.7420 - val_accuracy: 0.7245\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7457 - accuracy: 0.7186 - val_loss: 1.7401 - val_accuracy: 0.7253\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7436 - accuracy: 0.7215 - val_loss: 1.7409 - val_accuracy: 0.7240\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7420 - accuracy: 0.7231 - val_loss: 1.7376 - val_accuracy: 0.7259\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7404 - accuracy: 0.7249 - val_loss: 1.7370 - val_accuracy: 0.7272\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7393 - accuracy: 0.7253 - val_loss: 1.7384 - val_accuracy: 0.7259\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7382 - accuracy: 0.7268 - val_loss: 1.7345 - val_accuracy: 0.7305\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7369 - accuracy: 0.7278 - val_loss: 1.7342 - val_accuracy: 0.7313\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7359 - accuracy: 0.7283 - val_loss: 1.7331 - val_accuracy: 0.7315\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7349 - accuracy: 0.7297 - val_loss: 1.7328 - val_accuracy: 0.7320\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7342 - accuracy: 0.7300 - val_loss: 1.7321 - val_accuracy: 0.7323\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7334 - accuracy: 0.7310 - val_loss: 1.7312 - val_accuracy: 0.7332\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7325 - accuracy: 0.7326 - val_loss: 1.7302 - val_accuracy: 0.7329\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7319 - accuracy: 0.7320 - val_loss: 1.7295 - val_accuracy: 0.7339\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7315 - accuracy: 0.7324 - val_loss: 1.7299 - val_accuracy: 0.7344\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7305 - accuracy: 0.7330 - val_loss: 1.7294 - val_accuracy: 0.7327\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7304 - accuracy: 0.7334 - val_loss: 1.7291 - val_accuracy: 0.7334\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7296 - accuracy: 0.7343 - val_loss: 1.7292 - val_accuracy: 0.7334\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7291 - accuracy: 0.7341 - val_loss: 1.7293 - val_accuracy: 0.7340\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.7284 - accuracy: 0.7356 - val_loss: 1.7294 - val_accuracy: 0.7345\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [12:41, 159.81s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 9\n",
      "rows to prune in layer 0 : 759\n",
      "rows to prune in layer 3 : 290\n",
      "rows to prune in layer 6 : 96\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1501 - accuracy: 0.3282 - val_loss: 2.1092 - val_accuracy: 0.3674\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0984 - accuracy: 0.3713 - val_loss: 2.0940 - val_accuracy: 0.3703\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0885 - accuracy: 0.3750 - val_loss: 2.0872 - val_accuracy: 0.3740\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0817 - accuracy: 0.3799 - val_loss: 2.0811 - val_accuracy: 0.3796\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0756 - accuracy: 0.3856 - val_loss: 2.0771 - val_accuracy: 0.3832\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0708 - accuracy: 0.3905 - val_loss: 2.0730 - val_accuracy: 0.3883\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0658 - accuracy: 0.3980 - val_loss: 2.0698 - val_accuracy: 0.3904\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0618 - accuracy: 0.4027 - val_loss: 2.0651 - val_accuracy: 0.3982\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0573 - accuracy: 0.4084 - val_loss: 2.0612 - val_accuracy: 0.4035\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0529 - accuracy: 0.4124 - val_loss: 2.0577 - val_accuracy: 0.4084\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0488 - accuracy: 0.4185 - val_loss: 2.0541 - val_accuracy: 0.4108\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0418 - accuracy: 0.4252 - val_loss: 2.0454 - val_accuracy: 0.4190\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0362 - accuracy: 0.4297 - val_loss: 2.0423 - val_accuracy: 0.4221\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0330 - accuracy: 0.4316 - val_loss: 2.0400 - val_accuracy: 0.4237\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0306 - accuracy: 0.4336 - val_loss: 2.0380 - val_accuracy: 0.4258\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0291 - accuracy: 0.4345 - val_loss: 2.0370 - val_accuracy: 0.4251\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0278 - accuracy: 0.4354 - val_loss: 2.0357 - val_accuracy: 0.4259\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0270 - accuracy: 0.4358 - val_loss: 2.0357 - val_accuracy: 0.4260\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0263 - accuracy: 0.4361 - val_loss: 2.0343 - val_accuracy: 0.4267\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0256 - accuracy: 0.4372 - val_loss: 2.0338 - val_accuracy: 0.4270\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0250 - accuracy: 0.4374 - val_loss: 2.0334 - val_accuracy: 0.4276\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0243 - accuracy: 0.4377 - val_loss: 2.0334 - val_accuracy: 0.4277\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0237 - accuracy: 0.4380 - val_loss: 2.0324 - val_accuracy: 0.4280\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0234 - accuracy: 0.4388 - val_loss: 2.0320 - val_accuracy: 0.4285\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0230 - accuracy: 0.4387 - val_loss: 2.0318 - val_accuracy: 0.4292\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0225 - accuracy: 0.4390 - val_loss: 2.0321 - val_accuracy: 0.4306\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0224 - accuracy: 0.4388 - val_loss: 2.0308 - val_accuracy: 0.4288\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0220 - accuracy: 0.4397 - val_loss: 2.0307 - val_accuracy: 0.4298\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0217 - accuracy: 0.4400 - val_loss: 2.0309 - val_accuracy: 0.4287\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0214 - accuracy: 0.4399 - val_loss: 2.0308 - val_accuracy: 0.4291\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0210 - accuracy: 0.4401 - val_loss: 2.0298 - val_accuracy: 0.4298\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0209 - accuracy: 0.4406 - val_loss: 2.0302 - val_accuracy: 0.4298\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0204 - accuracy: 0.4408 - val_loss: 2.0297 - val_accuracy: 0.4296\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0200 - accuracy: 0.4410 - val_loss: 2.0285 - val_accuracy: 0.4323\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0200 - accuracy: 0.4408 - val_loss: 2.0283 - val_accuracy: 0.4311\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0196 - accuracy: 0.4413 - val_loss: 2.0286 - val_accuracy: 0.4313\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0193 - accuracy: 0.4415 - val_loss: 2.0278 - val_accuracy: 0.4313\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0190 - accuracy: 0.4415 - val_loss: 2.0281 - val_accuracy: 0.4307\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0184 - accuracy: 0.4422 - val_loss: 2.0266 - val_accuracy: 0.4323\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0179 - accuracy: 0.4430 - val_loss: 2.0253 - val_accuracy: 0.4346\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0173 - accuracy: 0.4435 - val_loss: 2.0249 - val_accuracy: 0.4352\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0169 - accuracy: 0.4444 - val_loss: 2.0249 - val_accuracy: 0.4348\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0165 - accuracy: 0.4448 - val_loss: 2.0239 - val_accuracy: 0.4358\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0161 - accuracy: 0.4451 - val_loss: 2.0241 - val_accuracy: 0.4358\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0158 - accuracy: 0.4454 - val_loss: 2.0229 - val_accuracy: 0.4371\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0155 - accuracy: 0.4457 - val_loss: 2.0226 - val_accuracy: 0.4378\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0152 - accuracy: 0.4460 - val_loss: 2.0225 - val_accuracy: 0.4380\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0152 - accuracy: 0.4456 - val_loss: 2.0225 - val_accuracy: 0.4376\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0148 - accuracy: 0.4466 - val_loss: 2.0216 - val_accuracy: 0.4386\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0122 - accuracy: 0.4492 - val_loss: 2.0166 - val_accuracy: 0.4441\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0087 - accuracy: 0.4527 - val_loss: 2.0149 - val_accuracy: 0.4466\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0073 - accuracy: 0.4543 - val_loss: 2.0135 - val_accuracy: 0.4475\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0067 - accuracy: 0.4544 - val_loss: 2.0140 - val_accuracy: 0.4468\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0064 - accuracy: 0.4542 - val_loss: 2.0128 - val_accuracy: 0.4474\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0062 - accuracy: 0.4547 - val_loss: 2.0127 - val_accuracy: 0.4482\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0058 - accuracy: 0.4554 - val_loss: 2.0128 - val_accuracy: 0.4482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0054 - accuracy: 0.4556 - val_loss: 2.0125 - val_accuracy: 0.4481\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0052 - accuracy: 0.4564 - val_loss: 2.0124 - val_accuracy: 0.4468\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0051 - accuracy: 0.4557 - val_loss: 2.0130 - val_accuracy: 0.4477\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0050 - accuracy: 0.4564 - val_loss: 2.0117 - val_accuracy: 0.4486\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0047 - accuracy: 0.4567 - val_loss: 2.0123 - val_accuracy: 0.4476\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0043 - accuracy: 0.4567 - val_loss: 2.0107 - val_accuracy: 0.4508\n",
      "Epoch 63/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0043 - accuracy: 0.4562 - val_loss: 2.0128 - val_accuracy: 0.4478\n",
      "Epoch 64/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0041 - accuracy: 0.4570 - val_loss: 2.0121 - val_accuracy: 0.4483\n",
      "Epoch 65/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.0041 - accuracy: 0.4567 - val_loss: 2.0108 - val_accuracy: 0.4486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [16:23, 178.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 9\n",
      "rows to prune in layer 0 : 771\n",
      "rows to prune in layer 3 : 295\n",
      "rows to prune in layer 6 : 98\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2901 - accuracy: 0.1540 - val_loss: 2.2809 - val_accuracy: 0.1597\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2782 - accuracy: 0.1574 - val_loss: 2.2739 - val_accuracy: 0.1386\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2661 - accuracy: 0.1654 - val_loss: 2.2610 - val_accuracy: 0.1771\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2619 - accuracy: 0.1742 - val_loss: 2.2593 - val_accuracy: 0.1792\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2547 - accuracy: 0.1821 - val_loss: 2.2401 - val_accuracy: 0.2001\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2403 - accuracy: 0.1970 - val_loss: 2.2341 - val_accuracy: 0.2050\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2372 - accuracy: 0.2004 - val_loss: 2.2319 - val_accuracy: 0.2080\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2358 - accuracy: 0.2018 - val_loss: 2.2310 - val_accuracy: 0.2083\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2348 - accuracy: 0.2027 - val_loss: 2.2302 - val_accuracy: 0.2091\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2340 - accuracy: 0.2031 - val_loss: 2.2293 - val_accuracy: 0.2104\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2334 - accuracy: 0.2035 - val_loss: 2.2291 - val_accuracy: 0.2097\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2330 - accuracy: 0.2040 - val_loss: 2.2289 - val_accuracy: 0.2105\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2325 - accuracy: 0.2042 - val_loss: 2.2283 - val_accuracy: 0.2111\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2319 - accuracy: 0.2044 - val_loss: 2.2277 - val_accuracy: 0.2108\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2315 - accuracy: 0.2050 - val_loss: 2.2276 - val_accuracy: 0.2116\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2312 - accuracy: 0.2053 - val_loss: 2.2273 - val_accuracy: 0.2127\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2307 - accuracy: 0.2058 - val_loss: 2.2269 - val_accuracy: 0.2113\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2302 - accuracy: 0.2060 - val_loss: 2.2266 - val_accuracy: 0.2107\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2212 - accuracy: 0.2168 - val_loss: 2.2067 - val_accuracy: 0.2357\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2102 - accuracy: 0.2297 - val_loss: 2.2032 - val_accuracy: 0.2390\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2077 - accuracy: 0.2324 - val_loss: 2.2020 - val_accuracy: 0.2402\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.2059 - accuracy: 0.2351 - val_loss: 2.2003 - val_accuracy: 0.2425\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1966 - accuracy: 0.2456 - val_loss: 2.1885 - val_accuracy: 0.2528\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1907 - accuracy: 0.2534 - val_loss: 2.1876 - val_accuracy: 0.2531\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1896 - accuracy: 0.2542 - val_loss: 2.1868 - val_accuracy: 0.2532\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1890 - accuracy: 0.2547 - val_loss: 2.1863 - val_accuracy: 0.2541\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1886 - accuracy: 0.2553 - val_loss: 2.1862 - val_accuracy: 0.2536\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1881 - accuracy: 0.2560 - val_loss: 2.1859 - val_accuracy: 0.2543\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1876 - accuracy: 0.2565 - val_loss: 2.1856 - val_accuracy: 0.2538\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1870 - accuracy: 0.2568 - val_loss: 2.1854 - val_accuracy: 0.2552\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1864 - accuracy: 0.2573 - val_loss: 2.1839 - val_accuracy: 0.2549\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1818 - accuracy: 0.2573 - val_loss: 2.1705 - val_accuracy: 0.2567\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1587 - accuracy: 0.2867 - val_loss: 2.1483 - val_accuracy: 0.3021\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1458 - accuracy: 0.3059 - val_loss: 2.1432 - val_accuracy: 0.3023\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1421 - accuracy: 0.3070 - val_loss: 2.1410 - val_accuracy: 0.3031\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1402 - accuracy: 0.3083 - val_loss: 2.1397 - val_accuracy: 0.3040\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1389 - accuracy: 0.3090 - val_loss: 2.1391 - val_accuracy: 0.3050\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1380 - accuracy: 0.3089 - val_loss: 2.1378 - val_accuracy: 0.3056\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1372 - accuracy: 0.3095 - val_loss: 2.1371 - val_accuracy: 0.3063\n",
      "Epoch 40/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1365 - accuracy: 0.3102 - val_loss: 2.1366 - val_accuracy: 0.3053\n",
      "Epoch 41/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1361 - accuracy: 0.3101 - val_loss: 2.1364 - val_accuracy: 0.3057\n",
      "Epoch 42/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1357 - accuracy: 0.3100 - val_loss: 2.1362 - val_accuracy: 0.3057\n",
      "Epoch 43/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1354 - accuracy: 0.3108 - val_loss: 2.1360 - val_accuracy: 0.3063\n",
      "Epoch 44/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1352 - accuracy: 0.3111 - val_loss: 2.1359 - val_accuracy: 0.3067\n",
      "Epoch 45/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1349 - accuracy: 0.3111 - val_loss: 2.1356 - val_accuracy: 0.3062\n",
      "Epoch 46/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1348 - accuracy: 0.3115 - val_loss: 2.1357 - val_accuracy: 0.3060\n",
      "Epoch 47/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1346 - accuracy: 0.3112 - val_loss: 2.1362 - val_accuracy: 0.3058\n",
      "Epoch 48/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1345 - accuracy: 0.3109 - val_loss: 2.1354 - val_accuracy: 0.3065\n",
      "Epoch 49/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1343 - accuracy: 0.3115 - val_loss: 2.1352 - val_accuracy: 0.3057\n",
      "Epoch 50/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1342 - accuracy: 0.3113 - val_loss: 2.1351 - val_accuracy: 0.3064\n",
      "Epoch 51/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1341 - accuracy: 0.3117 - val_loss: 2.1353 - val_accuracy: 0.3067\n",
      "Epoch 52/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1340 - accuracy: 0.3115 - val_loss: 2.1348 - val_accuracy: 0.3066\n",
      "Epoch 53/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1339 - accuracy: 0.3119 - val_loss: 2.1349 - val_accuracy: 0.3071\n",
      "Epoch 54/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1337 - accuracy: 0.3122 - val_loss: 2.1347 - val_accuracy: 0.3061\n",
      "Epoch 55/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1337 - accuracy: 0.3117 - val_loss: 2.1347 - val_accuracy: 0.3077\n",
      "Epoch 56/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1336 - accuracy: 0.3122 - val_loss: 2.1349 - val_accuracy: 0.3069\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1335 - accuracy: 0.3123 - val_loss: 2.1347 - val_accuracy: 0.3066\n",
      "Epoch 58/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1334 - accuracy: 0.3119 - val_loss: 2.1346 - val_accuracy: 0.3069\n",
      "Epoch 59/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1334 - accuracy: 0.3119 - val_loss: 2.1345 - val_accuracy: 0.3064\n",
      "Epoch 60/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1333 - accuracy: 0.3120 - val_loss: 2.1346 - val_accuracy: 0.3076\n",
      "Epoch 61/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1331 - accuracy: 0.3124 - val_loss: 2.1346 - val_accuracy: 0.3074\n",
      "Epoch 62/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.1331 - accuracy: 0.3126 - val_loss: 2.1345 - val_accuracy: 0.3071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [19:20, 165.73s/it]\u001b[A\n",
      "100%|██████████| 10/10 [3:05:38<00:00, 1113.86s/it]\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='magnitude', \n",
    "    scope='local', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5760 - accuracy: 0.8972 - val_loss: 1.5142 - val_accuracy: 0.9512\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 0\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5118 - accuracy: 0.9520 - val_loss: 1.5047 - val_accuracy: 0.9579\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4988 - accuracy: 0.9642 - val_loss: 1.4956 - val_accuracy: 0.9663\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4916 - accuracy: 0.9707 - val_loss: 1.4893 - val_accuracy: 0.9733\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4864 - accuracy: 0.9757 - val_loss: 1.4885 - val_accuracy: 0.9735\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4832 - accuracy: 0.9787 - val_loss: 1.4872 - val_accuracy: 0.9746\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4813 - accuracy: 0.9806 - val_loss: 1.4900 - val_accuracy: 0.9713\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4800 - accuracy: 0.9818 - val_loss: 1.4849 - val_accuracy: 0.9761\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4777 - accuracy: 0.9839 - val_loss: 1.4879 - val_accuracy: 0.9736\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4766 - accuracy: 0.9850 - val_loss: 1.4859 - val_accuracy: 0.9755\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9858 - val_loss: 1.4871 - val_accuracy: 0.9741\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:04, 124.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 0\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3026 - accuracy: 0.1000 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3017 - accuracy: 0.1124 - val_loss: 2.3015 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:37, 115.12s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 0\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:21, 93.77s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 0\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:00, 77.40s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 0\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:03, 72.96s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 0\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:45, 63.57s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 0\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:30, 64.36s/it]\u001b[A\n",
      " 10%|█         | 1/10 [07:32<1:07:55, 452.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5940 - accuracy: 0.8803 - val_loss: 1.5253 - val_accuracy: 0.9405\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 1\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5116 - accuracy: 0.9523 - val_loss: 1.4996 - val_accuracy: 0.9632\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4971 - accuracy: 0.9658 - val_loss: 1.5024 - val_accuracy: 0.9600\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4905 - accuracy: 0.9717 - val_loss: 1.4946 - val_accuracy: 0.9671\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4861 - accuracy: 0.9761 - val_loss: 1.4908 - val_accuracy: 0.9703\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4830 - accuracy: 0.9789 - val_loss: 1.4898 - val_accuracy: 0.9720\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4807 - accuracy: 0.9810 - val_loss: 1.4878 - val_accuracy: 0.9734\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4793 - accuracy: 0.9823 - val_loss: 1.4925 - val_accuracy: 0.9690\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4776 - accuracy: 0.9839 - val_loss: 1.4858 - val_accuracy: 0.9757\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4768 - accuracy: 0.9848 - val_loss: 1.4876 - val_accuracy: 0.9737\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4750 - accuracy: 0.9865 - val_loss: 1.4869 - val_accuracy: 0.9741\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4748 - accuracy: 0.9867 - val_loss: 1.4868 - val_accuracy: 0.9747\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:54, 114.79s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 1\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3026 - accuracy: 0.1001 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3017 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 39/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:40, 112.03s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 1\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:33, 94.30s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 1\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:17, 79.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 1\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:57, 67.52s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 1\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:38, 59.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 1\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:19, 62.83s/it]\u001b[A\n",
      " 20%|██        | 2/10 [14:54<59:56, 449.60s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5758 - accuracy: 0.9005 - val_loss: 1.5204 - val_accuracy: 0.9433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 2\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5115 - accuracy: 0.9519 - val_loss: 1.4993 - val_accuracy: 0.9633\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4981 - accuracy: 0.9647 - val_loss: 1.4955 - val_accuracy: 0.9664\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4912 - accuracy: 0.9710 - val_loss: 1.4952 - val_accuracy: 0.9665\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4869 - accuracy: 0.9754 - val_loss: 1.4971 - val_accuracy: 0.9650\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4836 - accuracy: 0.9782 - val_loss: 1.4898 - val_accuracy: 0.9722\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4807 - accuracy: 0.9811 - val_loss: 1.4880 - val_accuracy: 0.9738\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4788 - accuracy: 0.9829 - val_loss: 1.4864 - val_accuracy: 0.9749\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4775 - accuracy: 0.9839 - val_loss: 1.4863 - val_accuracy: 0.9749\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4767 - accuracy: 0.9848 - val_loss: 1.4853 - val_accuracy: 0.9758\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9856 - val_loss: 1.4888 - val_accuracy: 0.9724\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4749 - accuracy: 0.9864 - val_loss: 1.4864 - val_accuracy: 0.9751\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4747 - accuracy: 0.9866 - val_loss: 1.4857 - val_accuracy: 0.9758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:11, 131.08s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 2\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3025 - accuracy: 0.1002 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3017 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:34, 116.80s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 2\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:28, 97.87s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 2\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:14, 82.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 2\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:59, 71.24s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 2\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:52, 65.58s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 2\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:31, 64.55s/it]\u001b[A\n",
      " 30%|███       | 3/10 [22:28<52:36, 450.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5757 - accuracy: 0.9001 - val_loss: 1.5229 - val_accuracy: 0.9403\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 3\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5126 - accuracy: 0.9512 - val_loss: 1.5045 - val_accuracy: 0.9574\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4992 - accuracy: 0.9635 - val_loss: 1.4965 - val_accuracy: 0.9659\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4920 - accuracy: 0.9703 - val_loss: 1.4901 - val_accuracy: 0.9723\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4870 - accuracy: 0.9751 - val_loss: 1.4891 - val_accuracy: 0.9730\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4840 - accuracy: 0.9778 - val_loss: 1.4911 - val_accuracy: 0.9702\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4811 - accuracy: 0.9807 - val_loss: 1.4948 - val_accuracy: 0.9665\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4806 - accuracy: 0.9808 - val_loss: 1.4879 - val_accuracy: 0.9738\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4784 - accuracy: 0.9832 - val_loss: 1.4834 - val_accuracy: 0.9781\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4766 - accuracy: 0.9850 - val_loss: 1.4881 - val_accuracy: 0.9741\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4760 - accuracy: 0.9853 - val_loss: 1.4850 - val_accuracy: 0.9764\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4754 - accuracy: 0.9862 - val_loss: 1.4869 - val_accuracy: 0.9738\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:03, 123.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 3\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3024 - accuracy: 0.1046 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3019 - accuracy: 0.1124 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3016 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 35/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 36/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 37/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 38/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:45, 116.93s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 3\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:32, 96.06s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 3\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:28, 83.87s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 3\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:22, 75.02s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 3\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [07:11, 67.26s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 3\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:52, 67.45s/it]\u001b[A\n",
      " 40%|████      | 4/10 [30:23<45:47, 457.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5756 - accuracy: 0.9007 - val_loss: 1.5162 - val_accuracy: 0.9492\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 4\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5117 - accuracy: 0.9524 - val_loss: 1.5034 - val_accuracy: 0.9596\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4972 - accuracy: 0.9655 - val_loss: 1.4950 - val_accuracy: 0.9677\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4904 - accuracy: 0.9723 - val_loss: 1.4891 - val_accuracy: 0.9726\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4856 - accuracy: 0.9765 - val_loss: 1.4900 - val_accuracy: 0.9721\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4823 - accuracy: 0.9794 - val_loss: 1.4896 - val_accuracy: 0.9730\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4810 - accuracy: 0.9808 - val_loss: 1.4850 - val_accuracy: 0.9762\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4785 - accuracy: 0.9833 - val_loss: 1.4900 - val_accuracy: 0.9714\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4779 - accuracy: 0.9836 - val_loss: 1.4857 - val_accuracy: 0.9767\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4761 - accuracy: 0.9854 - val_loss: 1.4889 - val_accuracy: 0.9722\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:49, 109.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 4\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3025 - accuracy: 0.1032 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3016 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:15, 102.38s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 4\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:06, 86.96s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 4\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:05, 78.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 4\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:50, 68.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 4\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:33, 61.05s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 4\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:21, 63.10s/it]\u001b[A\n",
      " 50%|█████     | 5/10 [37:47<37:48, 453.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5977 - accuracy: 0.8754 - val_loss: 1.5251 - val_accuracy: 0.9386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 5\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5113 - accuracy: 0.9522 - val_loss: 1.5052 - val_accuracy: 0.9583\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4979 - accuracy: 0.9649 - val_loss: 1.4993 - val_accuracy: 0.9630\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4912 - accuracy: 0.9711 - val_loss: 1.4927 - val_accuracy: 0.9694\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4863 - accuracy: 0.9758 - val_loss: 1.4919 - val_accuracy: 0.9697\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4834 - accuracy: 0.9785 - val_loss: 1.4883 - val_accuracy: 0.9740\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4815 - accuracy: 0.9801 - val_loss: 1.4874 - val_accuracy: 0.9736\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4791 - accuracy: 0.9827 - val_loss: 1.4872 - val_accuracy: 0.9742\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4785 - accuracy: 0.9831 - val_loss: 1.4871 - val_accuracy: 0.9743\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4768 - accuracy: 0.9847 - val_loss: 1.4868 - val_accuracy: 0.9750\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9857 - val_loss: 1.4825 - val_accuracy: 0.9790\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4749 - accuracy: 0.9865 - val_loss: 1.4857 - val_accuracy: 0.9753\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4744 - accuracy: 0.9872 - val_loss: 1.4840 - val_accuracy: 0.9777\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4743 - accuracy: 0.9871 - val_loss: 1.4836 - val_accuracy: 0.9779\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:27, 147.04s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 5\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3025 - accuracy: 0.1002 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3016 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 34/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:58, 130.44s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 5\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:44, 104.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 5\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:34, 88.68s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 5\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:19, 75.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 5\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:56, 64.01s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 5\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:55, 67.86s/it]\u001b[A\n",
      " 60%|██████    | 6/10 [45:44<30:43, 460.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6458 - accuracy: 0.8267 - val_loss: 1.5997 - val_accuracy: 0.8617\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 6\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5166 - accuracy: 0.9480 - val_loss: 1.5030 - val_accuracy: 0.9607\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4980 - accuracy: 0.9648 - val_loss: 1.5054 - val_accuracy: 0.9567\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4909 - accuracy: 0.9714 - val_loss: 1.4957 - val_accuracy: 0.9667\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4866 - accuracy: 0.9754 - val_loss: 1.4918 - val_accuracy: 0.9704\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4831 - accuracy: 0.9788 - val_loss: 1.4899 - val_accuracy: 0.9718\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4806 - accuracy: 0.9812 - val_loss: 1.4873 - val_accuracy: 0.9749\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4795 - accuracy: 0.9822 - val_loss: 1.4882 - val_accuracy: 0.9735\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4781 - accuracy: 0.9837 - val_loss: 1.4880 - val_accuracy: 0.9733\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4770 - accuracy: 0.9845 - val_loss: 1.4855 - val_accuracy: 0.9760\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4759 - accuracy: 0.9855 - val_loss: 1.4844 - val_accuracy: 0.9770\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4748 - accuracy: 0.9866 - val_loss: 1.4858 - val_accuracy: 0.9757\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4744 - accuracy: 0.9870 - val_loss: 1.4817 - val_accuracy: 0.9796\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4733 - accuracy: 0.9880 - val_loss: 1.4859 - val_accuracy: 0.9759\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4729 - accuracy: 0.9884 - val_loss: 1.4837 - val_accuracy: 0.9772\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4727 - accuracy: 0.9887 - val_loss: 1.4846 - val_accuracy: 0.9764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:34, 154.90s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 6\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3025 - accuracy: 0.1010 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3016 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:58, 133.45s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 6\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:45, 107.67s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 6\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:27, 88.02s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 6\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:19, 77.10s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 6\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [07:11, 69.39s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 6\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:53, 67.66s/it]\u001b[A\n",
      " 70%|███████   | 7/10 [53:40<23:16, 465.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.6265 - accuracy: 0.8455 - val_loss: 1.5258 - val_accuracy: 0.9401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 7\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5132 - accuracy: 0.9507 - val_loss: 1.5053 - val_accuracy: 0.9575\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4988 - accuracy: 0.9639 - val_loss: 1.4984 - val_accuracy: 0.9637\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4914 - accuracy: 0.9711 - val_loss: 1.4938 - val_accuracy: 0.9686\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4870 - accuracy: 0.9748 - val_loss: 1.4927 - val_accuracy: 0.9696\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4831 - accuracy: 0.9789 - val_loss: 1.4904 - val_accuracy: 0.9701\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4806 - accuracy: 0.9814 - val_loss: 1.4923 - val_accuracy: 0.9685\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4799 - accuracy: 0.9818 - val_loss: 1.4885 - val_accuracy: 0.9727\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4777 - accuracy: 0.9840 - val_loss: 1.4858 - val_accuracy: 0.9763\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4762 - accuracy: 0.9852 - val_loss: 1.4888 - val_accuracy: 0.9728\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4757 - accuracy: 0.9859 - val_loss: 1.4840 - val_accuracy: 0.9773\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4743 - accuracy: 0.9873 - val_loss: 1.4829 - val_accuracy: 0.9787\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4738 - accuracy: 0.9876 - val_loss: 1.4869 - val_accuracy: 0.9740\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4725 - accuracy: 0.9890 - val_loss: 1.4822 - val_accuracy: 0.9787\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4724 - accuracy: 0.9889 - val_loss: 1.4804 - val_accuracy: 0.9808\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4726 - accuracy: 0.9886 - val_loss: 1.4838 - val_accuracy: 0.9775\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4718 - accuracy: 0.9894 - val_loss: 1.4851 - val_accuracy: 0.9759\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4717 - accuracy: 0.9896 - val_loss: 1.4822 - val_accuracy: 0.9793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:38, 158.30s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 7\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3025 - accuracy: 0.1002 - val_loss: 2.3021 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3019 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3016 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 32/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 33/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:13, 139.41s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 7\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [05:00, 111.68s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 7\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:46, 91.92s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 7\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:32, 78.23s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 7\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [07:18, 68.56s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 7\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [08:04, 69.20s/it]\u001b[A\n",
      " 80%|████████  | 8/10 [1:01:47<15:43, 471.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5806 - accuracy: 0.8953 - val_loss: 1.5228 - val_accuracy: 0.9426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 8\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5111 - accuracy: 0.9522 - val_loss: 1.5039 - val_accuracy: 0.9580\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4978 - accuracy: 0.9648 - val_loss: 1.4973 - val_accuracy: 0.9659\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4911 - accuracy: 0.9710 - val_loss: 1.4936 - val_accuracy: 0.9683\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4862 - accuracy: 0.9758 - val_loss: 1.4896 - val_accuracy: 0.9712\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4835 - accuracy: 0.9783 - val_loss: 1.4918 - val_accuracy: 0.9699\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4805 - accuracy: 0.9813 - val_loss: 1.4864 - val_accuracy: 0.9744\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4790 - accuracy: 0.9828 - val_loss: 1.4871 - val_accuracy: 0.9753\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4773 - accuracy: 0.9843 - val_loss: 1.4864 - val_accuracy: 0.9753\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4759 - accuracy: 0.9855 - val_loss: 1.4851 - val_accuracy: 0.9759\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4758 - accuracy: 0.9856 - val_loss: 1.4843 - val_accuracy: 0.9770\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4739 - accuracy: 0.9876 - val_loss: 1.4833 - val_accuracy: 0.9776\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4731 - accuracy: 0.9882 - val_loss: 1.4823 - val_accuracy: 0.9791\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4732 - accuracy: 0.9883 - val_loss: 1.4810 - val_accuracy: 0.9800\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4735 - accuracy: 0.9879 - val_loss: 1.4815 - val_accuracy: 0.9802\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4726 - accuracy: 0.9887 - val_loss: 1.4833 - val_accuracy: 0.9778\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4726 - accuracy: 0.9885 - val_loss: 1.4844 - val_accuracy: 0.9769\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [02:40, 160.49s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 8\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3026 - accuracy: 0.0979 - val_loss: 2.3022 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3020 - accuracy: 0.1124 - val_loss: 2.3017 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3017 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3015 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [04:06, 138.18s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 8\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:58, 112.17s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 8\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [05:43, 92.19s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 8\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [06:25, 77.15s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 8\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [07:07, 66.53s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 8\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [07:53, 67.71s/it]\u001b[A\n",
      " 90%|█████████ | 9/10 [1:09:43<07:53, 473.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5738 - accuracy: 0.9006 - val_loss: 1.5238 - val_accuracy: 0.9401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "0it [00:00, ?it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.0, current iteration is 9\n",
      "no of rows to prune total:  0\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.5121 - accuracy: 0.9513 - val_loss: 1.5037 - val_accuracy: 0.9594\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4982 - accuracy: 0.9645 - val_loss: 1.4947 - val_accuracy: 0.9674\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4913 - accuracy: 0.9709 - val_loss: 1.4906 - val_accuracy: 0.9718\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4862 - accuracy: 0.9761 - val_loss: 1.4881 - val_accuracy: 0.9739\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4828 - accuracy: 0.9790 - val_loss: 1.4875 - val_accuracy: 0.9740\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4803 - accuracy: 0.9814 - val_loss: 1.4876 - val_accuracy: 0.9740\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4792 - accuracy: 0.9826 - val_loss: 1.4881 - val_accuracy: 0.9736\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 1.4783 - accuracy: 0.9831 - val_loss: 1.4916 - val_accuracy: 0.9695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "1it [01:52, 112.78s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.5, current iteration is 9\n",
      "no of rows to prune total:  592\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3024 - accuracy: 0.1056 - val_loss: 2.3020 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3019 - accuracy: 0.1124 - val_loss: 2.3016 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3016 - accuracy: 0.1124 - val_loss: 2.3014 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3013 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3014 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3012 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3013 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 9/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 10/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 11/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 12/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 13/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 14/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 15/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 16/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 17/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 18/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 19/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 20/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 21/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 22/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 23/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 24/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 25/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 26/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3011 - val_accuracy: 0.1135\n",
      "Epoch 27/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 28/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 29/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 30/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 31/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "2it [03:21, 105.71s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.75, current iteration is 9\n",
      "no of rows to prune total:  888\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 8/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "3it [04:06, 87.30s/it] \u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.875, current iteration is 9\n",
      "no of rows to prune total:  1036\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 7/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "4it [04:50, 74.25s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.9375, current iteration is 9\n",
      "no of rows to prune total:  1110\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "5it [05:28, 63.43s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.96875, current iteration is 9\n",
      "no of rows to prune total:  1147\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "6it [06:06, 55.94s/it]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current pruning ratio is0.984375, current iteration is 9\n",
      "no of rows to prune total:  1165\n",
      "Epoch 1/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 2/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 3/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 4/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 5/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n",
      "Epoch 6/500\n",
      "938/938 [==============================] - 2s 2ms/step - loss: 2.3012 - accuracy: 0.1124 - val_loss: 2.3010 - val_accuracy: 0.1135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "7it [06:51, 58.74s/it]\u001b[A\n",
      "100%|██████████| 10/10 [1:16:36<00:00, 459.69s/it]\n"
     ]
    }
   ],
   "source": [
    "run_experiment(\n",
    "    structure='structured', \n",
    "    method='magnitude', \n",
    "    scope='global', \n",
    "    iterations=ITERATIONS\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prune, Train Attack Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(structure='unstructured', method='random', scope='global', iterations=10):\n",
    "    experiment_name = f'{EXPERIMENT_TYPE}-{ARCHITECTURE}-{method}-{scope}-{structure}'\n",
    "    cols = ['iteration','experiment_name','structure','method','scope','pruning_ratio','accuracy','loss','pgd_linf','cw_l2','bb_l0', 'total_params', 'params_left']\n",
    "    results = pd.DataFrame(columns=cols, dtype='object')\n",
    "    pgd_success_rates = []\n",
    "    cw_success_rates = []\n",
    "    bb0_success_rates = []\n",
    "    all_accuracies = []\n",
    "\n",
    "    \n",
    "    compression_rates = [tf.math.pow(2, x).numpy() for x in range(7)]\n",
    "    pruning_ratios = [1-1/x for x in compression_rates]\n",
    "\n",
    "    \n",
    "    for j in tqdm(range(iterations)):\n",
    "        accuracies = []\n",
    "        pgd_success_rate = []\n",
    "        cw_success_rate = []\n",
    "        bb0_success_rate = []\n",
    "        try: \n",
    "            del model\n",
    "        except:\n",
    "            ;\n",
    "        tf.compat.v1.reset_default_graph()\n",
    "        tf.keras.backend.clear_session()\n",
    "        \n",
    "        model = initialize_base_model(j, experiment_name=experiment_name, save_weights=True)\n",
    "        for index, pruning_ratio in tqdm(enumerate(pruning_ratios)):\n",
    "\n",
    "            #model.load_weights(f'./saved-weights/{experiment_name}-{j}')\n",
    "            #print(f'./saved-weights/{experiment_name}-{j}')\n",
    "\n",
    "            #for i in range(index + 1):\n",
    "                print(f'current pruning ratio is{pruning_ratio}, current iteration is {j}')\n",
    "                #if i != index:\n",
    "\n",
    "                if  method=='random' and scope=='global' and structure=='unstructured':\n",
    "                    model.prune_random_global_unstruct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='global' and structure=='structured':\n",
    "                    model.prune_random_global_struct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='local' and structure=='unstructured':\n",
    "                    model.prune_random_local_unstruct(pruning_ratio)\n",
    "                elif  method=='random' and scope=='local' and structure=='structured':\n",
    "                    model.prune_random_local_struct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='global' and structure=='unstructured':\n",
    "                    model.prune_magnitude_global_unstruct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='global' and structure=='structured':\n",
    "                    model.prune_magnitude_global_struct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='local' and structure=='unstructured':\n",
    "                    model.prune_magnitude_local_unstruct(pruning_ratio)\n",
    "                elif  method=='magnitude' and scope=='local' and structure=='structured':\n",
    "                    model.prune_magnitude_local_struct(pruning_ratio)\n",
    "                else:\n",
    "                    raise ValueError(\"pruning method invalid\")\n",
    "\n",
    "                zeros_ratio, non_zeros, param_count = get_zeros_ratio(model.get_weights())\n",
    "                model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                              metrics=['accuracy'],\n",
    "                              experimental_run_tf_function=False\n",
    "                             )\n",
    "                model = train_model(model, to_convergence=True)\n",
    "                zeros_ratio, non_zeros, param_count = get_zeros_ratio(model.get_weights())\n",
    "                res = model.evaluate(x_test, y_test, verbose=0)\n",
    "                #accuracies.append()\n",
    "                \n",
    "                #pgd_success_rate.append()\n",
    "                \n",
    "                if res[1] > .40:\n",
    "                    bb0_success = bb0_attack(model)\n",
    "                else: \n",
    "                    bb0_success = 'not successful'\n",
    "                vals = {\n",
    "                    'iteration':j,\n",
    "                    'experiment_name':experiment_name,\n",
    "                    'structure':structure,\n",
    "                    'method':method,\n",
    "                    'scope':scope,\n",
    "                    'pruning_ratio':pruning_ratio,\n",
    "                    'accuracy':res[1],\n",
    "                    'loss':res[0],\n",
    "                    'pgd_linf':pgd_attack(model),\n",
    "                    'cw_l2':cw2_attack(model),\n",
    "                    'bb_l0':bb0_success,\n",
    "                    'total_params':param_count,\n",
    "                    'params_left':non_zeros\n",
    "                }\n",
    "                results = results.append(pd.DataFrame([vals], index=[0], dtype='object'))\n",
    "                results.to_pickle(f'./final-results/{experiment_name}.pkl')\n",
    "                results.to_csv(f'./final-results/{experiment_name}.csv', index=False)\n",
    "        #all_accuracies.append(accuracies)\n",
    "        #pgd_success_rates.append(pgd_success_rate)\n",
    "        #cw_success_rates.append(cw_success_rate)\n",
    "        #bb0_success_rates.append(bb0_success_rate)\n",
    "    \n",
    "    \n",
    "    results.to_pickle(f'./final-results/{experiment_name}.pkl')\n",
    "    results.to_csv(f'./final-results/{experiment_name}.csv', index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_zeros_ratio(weights):\n",
    "    layers_to_examine = [0,3,6]\n",
    "    all_weights = np.array([])\n",
    "    for x in layers_to_examine:\n",
    "\n",
    "        all_weights = np.append(all_weights, weights[x].flatten())\n",
    "    return np.count_nonzero(all_weights)/len(all_weights), np.count_nonzero(all_weights), len(all_weights)\n",
    "\n",
    "def initialize_base_model(index, experiment_name, save_weights=False):\n",
    "\n",
    "    model = LeNet300_100()\n",
    "\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "                  metrics=['accuracy'],\n",
    "                  experimental_run_tf_function=False\n",
    "                 )\n",
    "\n",
    "    callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "    model.fit(x=x_train,\n",
    "              y=y_train,\n",
    "              batch_size=64,\n",
    "              epochs=1,\n",
    "              callbacks=[callback],\n",
    "              validation_data=(x_test, y_test),\n",
    "             )\n",
    "    if save_weights == True:\n",
    "        model.save_weights(f'./saved-weights/{experiment_name}-{index}')\n",
    "    return model\n",
    "\n",
    "\n",
    "\n",
    "def train_model(model, to_convergence=True, epochs = 5):\n",
    "    if to_convergence == True:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=500,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    if to_convergence == False:\n",
    "        callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "        model.fit(\n",
    "            x=x_train,\n",
    "            y=y_train,\n",
    "            batch_size=64,\n",
    "            epochs=epochs,\n",
    "            callbacks=[callback],\n",
    "            validation_data=(x_test, y_test),\n",
    "            )\n",
    "    return model\n",
    "\n",
    "def pgd_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.LinfProjectedGradientDescentAttack()\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        epsilons=[x/255 for x in [2,4,8,16,32]]\n",
    "    )\n",
    "    return [np.count_nonzero(eps_res)/len(y_to_attack) for eps_res in success]\n",
    "\n",
    "def cw2_attack(model_to_attack, eps=[10]):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    attack = fb.attacks.L2CarliniWagnerAttack(\n",
    "        binary_search_steps = 9,\n",
    "        steps= 5000,\n",
    "        stepsize = 1,\n",
    "        confidence = 0,\n",
    "        initial_const = 100,\n",
    "        abort_early = True,\n",
    "    )\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        y_to_attack,\n",
    "        epsilons=eps\n",
    "    )\n",
    "    dists = [tf.norm(x_to_attack[i]-adversarials[0][i]).numpy() for i in range(len(x_to_attack))]\n",
    "    return dists, success.numpy().tolist()\n",
    "\n",
    "def bb0_attack(model_to_attack):\n",
    "    fmodel = fb.models.TensorFlowModel(model_to_attack, bounds=(0,1))\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "    batches = [(x_to_attack[:500], y_to_attack[:500]), (x_to_attack[500:], y_to_attack[500:])]\n",
    "\n",
    "    # create attack that picks adversarials from given dataset of samples\n",
    "    #init_attack = fb.attacks.DatasetAttack()\n",
    "    init_attack = fb.attacks.DatasetAttack()\n",
    "\n",
    "    init_attack.feed(fmodel, batches[0][0])   # feed 1st batch of inputs\n",
    "    init_attack.feed(fmodel, batches[1][0])   # feed 2nd batch of inputs\n",
    "    attack = fb.attacks.L0BrendelBethgeAttack(binary_search_steps=30, steps=500,lr_num_decay=30, lr=1e7, init_attack=init_attack)\n",
    "    adversarials, _, success = attack(\n",
    "        fmodel,\n",
    "        x_to_attack,\n",
    "        criterion=fb.criteria.Misclassification(y_to_attack),\n",
    "        epsilons=[None]\n",
    "    )\n",
    "    dists = [np.count_nonzero(x_to_attack[i]-adversarials[0][i]) for i in range(len(x_to_attack))]\n",
    "    return dists, success.numpy().tolist()\n",
    "\n",
    "def prune_conv_layers(pruning_ratio):\n",
    "    layer_to_prune = [0, 3]\n",
    "    pruned_weights = model.get_weights()\n",
    "    \n",
    "    for layer in layer_to_prune:\n",
    "        converted_weights = convert_from_hwio_to_iohw(model.get_weights()[layer])\n",
    "        converted_mask = convert_from_hwio_to_iohw(model.get_weights()[layer + 2]).numpy()\n",
    "        for input_index, input_layer in enumerate(converted_weights):\n",
    "\n",
    "            for kernel_index, kernel in enumerate(input_layer):\n",
    "                dims = kernel.shape\n",
    "                flat_weights = kernel.numpy().flatten()\n",
    "                flat_masks = converted_mask[input_index][kernel_index].flatten()\n",
    "                flat_weights_df = pd.DataFrame(flat_weights)\n",
    "                flat_mask_df = pd.DataFrame(flat_masks)\n",
    "                no_of_weights_to_prune = int(len(flat_weights)*pruning_ratio)\n",
    "                #print(no_of_weights_to_prune)\n",
    "                indices_to_delete = flat_weights_df.abs().values.argsort(0)[:no_of_weights_to_prune]\n",
    "                for idx_to_delete in indices_to_delete:\n",
    "                    flat_masks[idx_to_delete] = 0\n",
    "\n",
    "                converted_mask[input_index][kernel_index] = flat_masks.reshape(dims)\n",
    "        back_converted_mask = convert_from_iohw_to_hwio(converted_mask)\n",
    "        pruned_weights[layer+2] = back_converted_mask\n",
    "    \n",
    "    return pruned_weights\n",
    "\n",
    "def convert_from_hwio_to_iohw(weights_nchw):\n",
    "    return tf.transpose(weights_nchw, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "\n",
    "def convert_from_iohw_to_hwio(weights_nhwc):\n",
    "    return tf.transpose(weights_nhwc, [2, 3, 0, 1])\n",
    "\n",
    "\n",
    "def get_average_accuracies(all_accuracies):\n",
    "    acc_per_pruning_rate=[]\n",
    "    for i in range(len(all_accuracies)):\n",
    "        for j in range(len(all_accuracies[i])):\n",
    "\n",
    "            try:\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "            except:\n",
    "                acc_per_pruning_rate.append([])\n",
    "                acc_per_pruning_rate[j].append(all_accuracies[i][j][1])\n",
    "    avg_acc_per_pruning_rate = [sum(x)/len(x) for x in acc_per_pruning_rate]; avg_acc_per_pruning_rate\n",
    "    return avg_acc_per_pruning_rate\n",
    "\n",
    "def get_average_success_rates(all_success_rates):\n",
    "    success_per_pruning_rate=[]\n",
    "    for i in range(len(all_success_rates)):\n",
    "        for j in range(len(all_success_rates[i])):\n",
    "\n",
    "            try:\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "            except:\n",
    "                success_per_pruning_rate.append([])\n",
    "                success_per_pruning_rate[j].append(all_success_rates[i][j])\n",
    "    avg_success_per_pruning_rate = [sum(x)/len(x) for x in success_per_pruning_rate];avg_success_per_pruning_rate\n",
    "    return avg_success_per_pruning_rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "x_train = x_train.reshape(60000, 784).astype('float32') / 255\n",
    "x_test = x_test.reshape(10000, 784).astype('float32') / 255\n",
    "\n",
    "x_to_attack = tf.convert_to_tensor(x_train[:1000].reshape(1000,28*28))\n",
    "y_to_attack = tf.convert_to_tensor([y_train[:1000]])[0];"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomLayer(layers.Layer):\n",
    "\n",
    "    def __init__(self,inputs, units=32, activation='relu'):\n",
    "        super(CustomLayer, self).__init__()\n",
    "        self.units = units\n",
    "        self.activation = activation\n",
    "        self.w = self.add_weight(shape=(inputs, self.units),\n",
    "                                 initializer='random_normal',\n",
    "                                 trainable=True,\n",
    "                                name='unpruned_weights')\n",
    "        self.b = self.add_weight(shape=(self.units), initializer='zeros', trainable=True, name='b')\n",
    "        self.mask = self.add_weight(shape=(self.w.shape),\n",
    "                                    initializer='ones',\n",
    "                                    trainable=False,\n",
    "                                   name='pruning_mask')\n",
    "\n",
    "    #def build(self, input_shape):\n",
    "        #print(input_shape)\n",
    "        \n",
    "\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        #self.mask_2 = tf.multiply(self.mask, self.mask_2)\n",
    "        x = tf.multiply(self.w, self.mask)\n",
    "        #print(self.pruned_w.eval())\n",
    "        x = tf.matmul(inputs, x)\n",
    "        x = tf.nn.bias_add(x, self.b)\n",
    "        \n",
    "        if self.activation == 'relu':\n",
    "            return tf.keras.activations.relu(x)\n",
    "        if self.activation == 'softmax':\n",
    "            return tf.keras.activations.softmax(x)\n",
    "        raise ValueError('Activation function not implemented')\n",
    "\n",
    "class LeNet300_100(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super(LeNet300_100, self).__init__()\n",
    "        self.dense1 = CustomLayer(28*28,300)\n",
    "        self.dense2 = CustomLayer(300,100)\n",
    "        self.dense3 = CustomLayer(100,10, activation='softmax')\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        x = layers.Flatten()(inputs)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dense2(x)\n",
    "        return self.dense3(x)\n",
    "    \n",
    "    def prune_random_local_unstruct(self, ratio):\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            dense_layer_to_prune = [0,3,6]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "                    no_of_weighs_to_prune = ratio * len(flat_weights)\n",
    "                    # find unpruned weights\n",
    "                    non_zero_weights = np.nonzero(flat_mask)[0]\n",
    "                    # calculate the amount of weights to be pruned this round\n",
    "                    no_of_weights_to_prune_left = int(no_of_weighs_to_prune - (len(flat_weights) - len(non_zero_weights)) )\n",
    "                    # shuffle all non-zero weights\n",
    "                    random.shuffle(non_zero_weights)\n",
    "                    # and take the indices of the first x weights where x is the number of weights to be pruned this round\n",
    "                    indices_to_delete = non_zero_weights[:no_of_weights_to_prune_left]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "    \n",
    "    def prune_magnitude_global_unstruct(self, ratio):\n",
    "        #flat out all weights:\n",
    "        conv_layer_to_prune = []\n",
    "        dense_layer_to_prune = [0, 3, 6]\n",
    "        weights = self.get_weights()\n",
    "        flat_weights = []\n",
    "        flat_mask = []\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            flat_weights = np.append(flat_weights, weights[x])\n",
    "            flat_mask = np.append(flat_mask, weights[x+2])\n",
    "            \n",
    "        no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "        indices_to_delete = np.abs(flat_weights).argsort(0)[:no_of_weights_to_prune]\n",
    "        \n",
    "        for idx_to_delete in indices_to_delete:\n",
    "            flat_mask[idx_to_delete] = 0\n",
    "            flat_weights[idx_to_delete] = 0\n",
    "        z = 0\n",
    "        for x in conv_layer_to_prune + dense_layer_to_prune:\n",
    "            weights[x] = flat_weights[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            weights[x + 2] = flat_mask[z:z + np.prod(weights[x].shape)].reshape(weights[x].shape)\n",
    "            z = z + np.prod(weights[x].shape)            \n",
    "        self.set_weights(weights)\n",
    "        \n",
    "        \n",
    "            \n",
    "        \n",
    "        \n",
    "    \n",
    "    def prune_magnitude_local_unstruct(self, ratio):\n",
    "        \n",
    "        def prune_dense_layers_locally(self, ratio):\n",
    "            \n",
    "            dense_layer_to_prune = [0,3,6]\n",
    "            weights = self.get_weights()\n",
    "            for index, weight in enumerate(weights):\n",
    "                if index in dense_layer_to_prune:\n",
    "                    shape = weight.shape\n",
    "                    flat_weights = weight.flatten()\n",
    "                    flat_mask = weights[index+2].flatten()\n",
    "\n",
    "                    no_of_weights_to_prune = int(len(flat_weights)*ratio)\n",
    "                    indices_to_delete = np.abs(flat_weights).argsort()[:no_of_weights_to_prune]\n",
    "                    for idx_to_delete in indices_to_delete:\n",
    "                        flat_mask[idx_to_delete] = 0\n",
    "                        flat_weights[idx_to_delete] = 0\n",
    "                    mask_reshaped = flat_mask.reshape(shape)\n",
    "                    weights_reshaped = flat_weights.reshape(shape)\n",
    "                    weights[index+2] = mask_reshaped\n",
    "                    weights[index] = weights_reshaped\n",
    "            self.set_weights(weights)\n",
    "            return True\n",
    "        prune_dense_layers_locally(self,ratio)\n",
    "        return\n",
    "    \n",
    "    def prune_random_local_struct(self, ratio):\n",
    "        \n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                rows = weights[layer_to_prune]\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                #print('rows to prune in layer', layer_to_prune,':',no_of_rows_to_prune)\n",
    "                non_zero_rows = np.nonzero([np.sum(row) for row in rows])[0]\n",
    "                no_of_rows_to_prune_left = no_of_rows_to_prune - (len(rows) - len(non_zero_rows))\n",
    "                #print(non_zero_rows)\n",
    "                random.shuffle(non_zero_rows)\n",
    "                #print(non_zero_rows)\n",
    "                rows_to_prune = non_zero_rows[:no_of_rows_to_prune_left]\n",
    "                \n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return True\n",
    "        weights = self.get_weights()\n",
    "        conv_layers_to_prune = []\n",
    "        dense_layers_to_prune = [0,3,6]\n",
    "        prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "\n",
    "    def prune_random_global_struct(self, ratio):\n",
    "        raise Warning('Not yet implemented')\n",
    "        return False\n",
    "    def prune_magnitude_local_struct(self, ratio):\n",
    "       \n",
    "        def prune_dense_layers(dense_layers_to_prune, weights):\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                no_of_rows_to_prune = int(ratio * len(weights[layer_to_prune]))\n",
    "                #print('rows to prune in layer', layer_to_prune,':',no_of_rows_to_prune)\n",
    "                vals = []\n",
    "                for row in weights[layer_to_prune]:\n",
    "                    vals.append(np.sum(np.abs(row)))\n",
    "                    #vals.append(np.sum(np.abs(row)) / len(row))\n",
    "                rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "                for row_to_prune in rows_to_prune:\n",
    "\n",
    "                    weights[layer_to_prune][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "                    weights[layer_to_prune+2][row_to_prune] = tf.zeros(len(weights[layer_to_prune][row_to_prune]))\n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "       \n",
    "        dense_layers_to_prune = [0,3,6]\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        return True\n",
    "        \n",
    "    def prune_magnitude_global_struct(self, ratio):\n",
    "\n",
    "        def prune_dense_layers(conv_layers_to_prune, weights):\n",
    "            vals = []\n",
    "            lengths = []\n",
    "            for layer_to_prune in dense_layers_to_prune:\n",
    "                lengths.append(weights[layer_to_prune].shape[0])\n",
    "                #vals = vals + [np.sum(np.abs(row)) / len(row) for row in weights[layer_to_prune]]\n",
    "                vals = vals + [np.sum(np.abs(row)) for row in weights[layer_to_prune]]\n",
    "            no_of_rows_to_prune = int(ratio * len(vals))\n",
    "            #print('no of rows to prune total: ',no_of_rows_to_prune)\n",
    "            rows_to_prune = np.argsort(vals)[:no_of_rows_to_prune]\n",
    "            for i, layer_to_prune in enumerate(dense_layers_to_prune):\n",
    "                for row_to_prune in rows_to_prune:\n",
    "                    if row_to_prune in range(int(np.sum(lengths[:i])), int(np.sum(lengths[:i+1]))):\n",
    "                        weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)\n",
    "                        weights[layer_to_prune + 2][row_to_prune - int(np.sum(lengths[:i]))] = tf.zeros(weights[layer_to_prune][row_to_prune - int(np.sum(lengths[:i]))].shape)                \n",
    "            return weights\n",
    "        weights = self.get_weights()\n",
    "        dense_layers_to_prune = [0,3,6]\n",
    "        weights = prune_dense_layers(dense_layers_to_prune, weights)\n",
    "        self.set_weights(weights)\n",
    "        #raise Warning('Not yet implemented')\n",
    "        return False\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compile and Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LeNet300_100()\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "              \n",
    "             )\n",
    "\n",
    "callback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )\n",
    "#model.save('./saved-models/mini-pipeline-CNN-baseline-model')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pre_pruning_weights = model.get_weights()\n",
    "model.prune_magnitude_local_unstruct(.8)\n",
    "post_pruning_weights= model.get_weights()\n",
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3),\n",
    "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) ,\n",
    "              metrics=['accuracy'],\n",
    "              experimental_run_tf_function=False\n",
    "             )\n",
    "model.fit(x=x_train,\n",
    "          y=y_train,\n",
    "          batch_size=128,\n",
    "          epochs=1,\n",
    "          callbacks=[callback],\n",
    "          validation_data=(x_test, y_test),\n",
    "         )\n",
    "post_fine_tuning_weights = model.get_weights()\n",
    "\n",
    "pre_pruning_masked_weights = get_masked_weights(pre_pruning_weights)\n",
    "post_pruning_masked_weights = get_masked_weights(post_pruning_weights)\n",
    "post_fine_tuning_masked_weights = get_masked_weights(post_fine_tuning_weights)\n",
    "\n",
    "def print_weight_hist(weights):\n",
    "    fig, axes = plt.subplots(ncols=1, nrows=3,sharex=True, sharey=True, figsize=(25,10) , gridspec_kw={'wspace': 0.05, 'hspace':.5})\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    #plt.figure(2, figsize=(25,10))\n",
    "    #plt.subplots_adjust(hspace=.5, sharey=True, sharex=True)\n",
    "    \n",
    "#    axes[0].subplot(3,1,1)    \n",
    "    \n",
    "    axes[0].hist(weights[0], bins=100)\n",
    "    axes[0].set_title(\"Pre pruning\")\n",
    "    axes[0].set_yscale('log')\n",
    " #   axes[1].subplot(3,1,2)    \n",
    "    \n",
    "    axes[1].hist(weights[1], bins=100)\n",
    "    axes[1].set_title(\"Post pruning\")\n",
    "  #  axes[2].subplot(3,1,3)    \n",
    "    \n",
    "    axes[2].hist(weights[2], bins=100)\n",
    "    axes[2].set_title(\"Post fine-tuning\")\n",
    "\n",
    "\n",
    "        \n",
    "def get_masked_weights(weights):\n",
    "\n",
    "        #print(np.array_equal(pre_pruning_weight_archive[0][1].flatten(), pre_pruning_weight_archive[1][1].flatten()))\n",
    "    masked_layers = []\n",
    "    masked_layers.append(tf.multiply(weights[0], weights[2]))\n",
    "    masked_layers.append(tf.multiply(weights[3], weights[5]))\n",
    "    masked_layers.append(tf.multiply(weights[6], weights[8]))\n",
    "    #masked_weights.append(masked_layers)\n",
    "    return masked_layers\n",
    "\n",
    "print_weight_hist([pre_pruning_masked_weights[0].numpy().flatten(), post_pruning_masked_weights[0].numpy().flatten(), post_fine_tuning_weights[0].flatten()])\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
